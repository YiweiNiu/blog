<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Understand Bioconductor Annotation Packages]]></title>
    <url>%2Fblog%2F2018%2F09%2FUnderstand-Bioconductor-Annotation-Packages%2F</url>
    <content type="text"><![CDATA[This note is to help me figure out the design schema of annotation packages in Bioconductor. And this note is mainly compiled from: Annotation Packages: the big picture. Fantastic slide! Introduction To Bioconductor Annotation Packages Package ‘AnnotationDbi’ Reference Manual Making and Utilizing TxDb Objects Introduction Packages in Bioconductor can be divided into three categories: software, annotation and experiment. All of the ‘.db’ (and most other Bioconductor annotation packages) are updated every 6 months corresponding to each release of Bioconductor. Exceptions are made for packages where the actual resources that the packages are based on have not themselves been updated. Schema of Annotation Packages Here is a very representative graph, but not informative-enough. There are three major types of annotation in Bioconductor: Gene centric AnnotationDb packages: Organism level: e.g. org.Mm.eg.db. Platform level: e.g. hgu133plus2.db, hgu133plus2.probes, hgu133plus2.cdf. Homology level: e.g. hom.Dm.inp.db. System-biology level: e.g. GO.db. Genome centric GenomicFeatures packages include: Transcriptome level: e.g. TxDb.Hsapiens.UCSC.hg19.knownGene, EnsDb.Hsapiens.v75. Generic genome feature: can generate via GenomicFeatures. One web-based resource accesss biomart, via biomaRt package: Query web-based ‘biomart’ resource for genes, sequence, SNPs, and etc. Working with AnotationDb objects AnnotationDb is the virtual base class for all annotation packages. It contain a database connection and is meant to be the parent for a set of classes in the Bioconductor annotation packages. These classes will provide a means of dispatch for a widely available set of select methods and thus allow the easy extraction of data from the annotation packages. All the annotation packages that base on AnnotationDb object expose an object named exactly the same as the package itself. 12library(org.Hs.eg.db)class(org.Hs.eg.db) 123[1] &quot;OrgDb&quot;attr(,&quot;package&quot;)[1] &quot;AnnotationDbi&quot; The more specific classes (the ones that you will actually see in the wild) have names like OrgDb, ChipDb or TxDb objects. These names correspond to the kind of package (and underlying schema) being represented. Like: org.Hs.eg.db - Genome wide annotation for Human. TxDb.Hsapiens.UCSC.hg19.knownGene - Annotation package for TxDb object(s). hgu95av2.db - annotations for the hgu95av2 Affymetrix platform. Methods select, columns and keys are used together to extract data from an AnnotationDb object (or any object derived from the parent class). Examples of classes derived from the AnnotationDb object include (but are not limited to): ChipDb, OrgDb, GODb, InparanoidDb and ReactomeDb. columns shows which kinds of data can be returned for the AnnotationDb object. keytypes allows the user to discover which keytypes can be passed in to select or keys and the keytype argument. keys returns keys for the database contained in the AnnotationDb object . This method is already documented in the keys manual page but is mentioned again here because it’s usage with select is so intimate. By default it will return the primary keys for the database, but if used with the keytype argument, it will return the keys from that keytype. select will retrieve the data as a data.frame based on parameters for selected keys columns and keytype arguments. Users should be warned that if you call select and request columns that have multiple matches for your keys, select will return a data.frame with one row for each possible match. This has the effect that if you request multiple columns and some of them have a many to one relationship to the keys, things will continue to multiply accordingly. So it’s not a good idea to request a large number of columns unless you know that what you are asking for should have a one to one relationship with the initial set of keys. In general, if you need to retrieve a column (like GO) that has a many to one relationship to the original keys, it is most useful to extract that separately. mapIds gets the mapped ids (column) for a set of keys that are of a particular keytype. Usually returned as a named character vector, a list or even a SimpleCharacterList. saveDb will take an AnnotationDb object and save the database to the file specified by the path passed in to the file argument. loadDb takes a .sqlite database file as an argument and uses data in the metadata table of that file to return an AnnotationDb style object of the appropriate type. species shows the genus and species label currently attached to the AnnotationDb objects database. dbfile gets the database file associated with an object. dbconn gets the datebase connection associated with an object. taxonomyId gets the taxonomy ID associated with an object (if available). ChipDb Platfom-based or chip-based annotation package are an extremely common kind of Annotation package. The following examples show how to use standard methods to interact with an object of this type. 1library("hgu95av2.db") Things loaded along with this package 1ls("package:hgu95av2.db") 12345678 [1] &quot;hgu95av2&quot; &quot;hgu95av2.db&quot; &quot;hgu95av2_dbconn&quot; &quot;hgu95av2_dbfile&quot; &quot;hgu95av2_dbInfo&quot; [6] &quot;hgu95av2_dbschema&quot; &quot;hgu95av2ACCNUM&quot; &quot;hgu95av2ALIAS2PROBE&quot; &quot;hgu95av2CHR&quot; &quot;hgu95av2CHRLENGTHS&quot; [11] &quot;hgu95av2CHRLOC&quot; &quot;hgu95av2CHRLOCEND&quot; &quot;hgu95av2ENSEMBL&quot; &quot;hgu95av2ENSEMBL2PROBE&quot; &quot;hgu95av2ENTREZID&quot; [16] &quot;hgu95av2ENZYME&quot; &quot;hgu95av2ENZYME2PROBE&quot; &quot;hgu95av2GENENAME&quot; &quot;hgu95av2GO&quot; &quot;hgu95av2GO2ALLPROBES&quot; [21] &quot;hgu95av2GO2PROBE&quot; &quot;hgu95av2MAP&quot; &quot;hgu95av2MAPCOUNTS&quot; &quot;hgu95av2OMIM&quot; &quot;hgu95av2ORGANISM&quot; [26] &quot;hgu95av2ORGPKG&quot; &quot;hgu95av2PATH&quot; &quot;hgu95av2PATH2PROBE&quot; &quot;hgu95av2PFAM&quot; &quot;hgu95av2PMID&quot; [31] &quot;hgu95av2PMID2PROBE&quot; &quot;hgu95av2PROSITE&quot; &quot;hgu95av2REFSEQ&quot; &quot;hgu95av2SYMBOL&quot; &quot;hgu95av2UNIGENE&quot; [36] &quot;hgu95av2UNIPROT&quot; These packages appear to contain a lot of data but it is an illusion. 12345&gt; library(hgu95av2.db)&gt; hgu95av2()&gt; hgu95av2_dbInfo()&gt; hgu95av2GENENAME&gt; hgu95av2_dbschema() Use columns() to see possible values for columns. 1columns(hgu95av2.db) 1234 [1] &quot;ACCNUM&quot; &quot;ALIAS&quot; &quot;ENSEMBL&quot; &quot;ENSEMBLPROT&quot; &quot;ENSEMBLTRANS&quot; &quot;ENTREZID&quot; &quot;ENZYME&quot; &quot;EVIDENCE&quot; [9] &quot;EVIDENCEALL&quot; &quot;GENENAME&quot; &quot;GO&quot; &quot;GOALL&quot; &quot;IPI&quot; &quot;MAP&quot; &quot;OMIM&quot; &quot;ONTOLOGY&quot; [17] &quot;ONTOLOGYALL&quot; &quot;PATH&quot; &quot;PFAM&quot; &quot;PMID&quot; &quot;PROBEID&quot; &quot;PROSITE&quot; &quot;REFSEQ&quot; &quot;SYMBOL&quot; [25] &quot;UCSCKG&quot; &quot;UNIGENE&quot; &quot;UNIPROT&quot; Use help(&quot;xxx&quot;)to see the description of columns. 1help('SYMBOL') Use keytypes() to see possible values for keytypes. In reality, some kinds of values make poor keys and so this list is shorter than that of above. 1keytypes(hgu95a.db) 1234 [1] &quot;ACCNUM&quot; &quot;ALIAS&quot; &quot;ENSEMBL&quot; &quot;ENSEMBLPROT&quot; &quot;ENSEMBLTRANS&quot; &quot;ENTREZID&quot; &quot;ENZYME&quot; &quot;EVIDENCE&quot; [9] &quot;EVIDENCEALL&quot; &quot;GENENAME&quot; &quot;GO&quot; &quot;GOALL&quot; &quot;IPI&quot; &quot;MAP&quot; &quot;OMIM&quot; &quot;ONTOLOGY&quot; [17] &quot;ONTOLOGYALL&quot; &quot;PATH&quot; &quot;PFAM&quot; &quot;PMID&quot; &quot;PROBEID&quot; &quot;PROSITE&quot; &quot;REFSEQ&quot; &quot;SYMBOL&quot; [25] &quot;UCSCKG&quot; &quot;UNIGENE&quot; &quot;UNIPROT&quot; Use keys() to extract some sample keys back. (default if the primary key.) 1head(keys(hgu95av2.db)) 1[1] &quot;1000_at&quot; &quot;1001_at&quot; &quot;1002_f_at&quot; &quot;1003_s_at&quot; &quot;1004_at&quot; &quot;1005_at&quot; Or for a particular keytype. 1head(keys(hgu95av2.db, keytype='SYMBOL')) 1[1] &quot;A1BG&quot; &quot;A2M&quot; &quot;A2MP1&quot; &quot;NAT1&quot; &quot;NAT2&quot; &quot;NATP&quot; Use select() to retrieve data. 1234#1st get some example keysk &lt;- head(keys(hgu95av2.db, keytype="PROBEID"))# then call selectselect(hgu95av2.db, keys=k, columns=c("SYMBOL","GENENAME"), keytype="PROBEID") 12345678&apos;select()&apos; returned 1:1 mapping between keys and columns PROBEID SYMBOL GENENAME1 1000_at MAPK3 mitogen-activated protein kinase 32 1001_at TIE1 tyrosine kinase with immunoglobulin like and EGF like domains 13 1002_f_at CYP2C19 cytochrome P450 family 2 subfamily C member 194 1003_s_at CXCR5 C-X-C motif chemokine receptor 55 1004_at CXCR5 C-X-C motif chemokine receptor 56 1005_at DUSP1 dual specificity phosphatase 1 If one wants to get only one column of data, mapIds can be used. 1mapIds(hgu95av2.db, keys=k, column=c("GENENAME"), keytype="PROBEID") 1234567&apos;select()&apos; returned 1:1 mapping between keys and columns 1000_at 1001_at &quot;mitogen-activated protein kinase 3&quot; &quot;tyrosine kinase with immunoglobulin like and EGF like domains 1&quot; 1002_f_at 1003_s_at &quot;cytochrome P450 family 2 subfamily C member 19&quot; &quot;C-X-C motif chemokine receptor 5&quot; 1004_at 1005_at &quot;C-X-C motif chemokine receptor 5&quot; &quot;dual specificity phosphatase 1&quot; OrgDb An organism level package (an ‘org’ package) uses a central gene identifier (e.g. Entrez Gene id) and contains mappings between this identifier and other kinds of identifiers (e.g. GenBank or Uniprot accession number, RefSeq id, etc.). The name of an org package is always of the form org.&lt;Ab&gt;.&lt;id&gt;.db (e.g. org.Sc.sgd.db) where &lt;Ab&gt; is a 2-letter abbreviation of the organism (e.g. Sc for Saccharomyces cerevisiae) and &lt;id&gt; is an abbreviation (in lower-case) describing the type of cen- tral identifier (e.g. sgd for gene identifiers assigned by the Saccharomyces Genome Database, or eg for Entrez Gene ids). Using OrgDb packages is just like using ChipDb packages. 12345678910111213141516library(org.Hs.eg.db)&gt; columns(org.Hs.eg.db) [1] "ACCNUM" "ALIAS" "ENSEMBL" "ENSEMBLPROT" "ENSEMBLTRANS" "ENTREZID" "ENZYME" "EVIDENCE" [9] "EVIDENCEALL" "GENENAME" "GO" "GOALL" "IPI" "MAP" "OMIM" "ONTOLOGY" [17] "ONTOLOGYALL" "PATH" "PFAM" "PMID" "PROSITE" "REFSEQ" "SYMBOL" "UCSCKG" [25] "UNIGENE" "UNIPROT"&gt; keytypes(org.Hs.eg.db) [1] "ACCNUM" "ALIAS" "ENSEMBL" "ENSEMBLPROT" "ENSEMBLTRANS" "ENTREZID" "ENZYME" "EVIDENCE" [9] "EVIDENCEALL" "GENENAME" "GO" "GOALL" "IPI" "MAP" "OMIM" "ONTOLOGY" [17] "ONTOLOGYALL" "PATH" "PFAM" "PMID" "PROSITE" "REFSEQ" "SYMBOL" "UCSCKG" [25] "UNIGENE" "UNIPROT"&gt; head(keys(org.Hs.eg.db))[1] "1" "2" "3" "9" "10" "11" GO.db 1234567891011121314151617181920212223242526272829303132333435363738394041424344library(GO.db)&gt; GO.dbGODb object:| GOSOURCENAME: Gene Ontology| GOSOURCEURL: ftp://ftp.geneontology.org/pub/go/godatabase/archive/latest-lite/| GOSOURCEDATE: 2018-Mar28| Db type: GODb| package: AnnotationDbi| DBSCHEMA: GO_DB| GOEGSOURCEDATE: 2018-Apr4| GOEGSOURCENAME: Entrez Gene| GOEGSOURCEURL: ftp://ftp.ncbi.nlm.nih.gov/gene/DATA| DBSCHEMAVERSION: 2.1Please see: help('select') for usage information&gt; columns(GO.db)[1] "DEFINITION" "GOID" "ONTOLOGY" "TERM" &gt; keytypes(GO.db)[1] "DEFINITION" "GOID" "ONTOLOGY" "TERM"&gt; head(keys(GO.db))[1] "GO:0000001" "GO:0000002" "GO:0000003" "GO:0000006" "GO:0000007" "GO:0000009"&gt; head(keys(GO.db, keytype='ONTOLOGY'))[1] "BP" "CC" "MF" "universal"&gt; head(keys(GO.db, keytype='TERM'))[1] "mitochondrion inheritance" "mitochondrial genome maintenance" [3] "reproduction" "ribosome biogenesis" [5] "protein binding involved in protein folding" "unfolded protein binding"&gt; keys &lt;- head(keys(GO.db))&gt; select(GO.db, keys=keys, columns=c("TERM","ONTOLOGY"), keytype="GOID")'select()' returned 1:1 mapping between keys and columns GOID TERM ONTOLOGY1 GO:0000001 mitochondrion inheritance BP2 GO:0000002 mitochondrial genome maintenance BP3 GO:0000003 reproduction BP4 GO:0000006 high-affinity zinc transmembrane transporter activity MF5 GO:0000007 low-affinity zinc ion transmembrane transporter activity MF6 GO:0000009 alpha-1,6-mannosyltransferase activity MF TxDb A TxDb package connects a set of genomic coordinates to various transcript oriented features. The package can also contain identifiers to features such as genes and transcripts, and the internal schema describes the relationships between these different elements. This class maps the 5’ and 3’ untranslated regions (UTRs), protein coding sequences (CDSs) and exons for a set of mRNA transcripts to their associated genome. TxDb objects have numerous accessors functions to allow such features to be retrieved individually or grouped together in a way that reflects the underlying biology. All TxDb containing packages follow a specific naming scheme that tells where the data came from as well as which build of the genome it comes from. Package GenomicFeatures contain a set of tools and methods to make and manipulate transcript centric annotation. 123library(TxDb.Hsapiens.UCSC.hg19.knownGene)txdb &lt;- TxDb.Hsapiens.UCSC.hg19.knownGene #shorthand (for conveniencetxdb 12345678910111213141516171819202122232425TxDb object:# Db type: TxDb# Supporting package: GenomicFeatures# Data source: UCSC# Genome: hg19# Organism: Homo sapiens# Taxonomy ID: 9606# UCSC Table: knownGene# Resource URL: http://genome.ucsc.edu/# Type of Gene ID: Entrez Gene ID# Full dataset: yes# miRBase build ID: GRCh37# transcript_nrow: 82960# exon_nrow: 289969# cds_nrow: 237533# Db created by: GenomicFeatures package from Bioconductor# Creation time: 2015-10-07 18:11:28 +0000 (Wed, 07 Oct 2015)# GenomicFeatures version at creation time: 1.21.30# RSQLite version at creation time: 1.0.0# DBSCHEMAVERSION: 1.1&gt; class(txdb)[1] &quot;TxDb&quot;attr(,&quot;package&quot;)[1] &quot;GenomicFeatures&quot; In addition to accessors via select, TxDb objects also provide access via the more familiar transcripts, exons, cds, transcriptsBy, exonsBy and cdsBy methods, and they will return GRanges objects. The ‘ungrouped’ functions transcripts, exons, cds, genes and promoters return the coordinate information as a GRanges object. 12345678910111213GR = transcripts(txdb)&gt; GR[1:3]GRanges object with 3 ranges and 2 metadata columns: seqnames ranges strand | tx_id tx_name &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; | &lt;integer&gt; &lt;character&gt; [1] chr1 11874-14409 + | 1 uc001aaa.3 [2] chr1 11874-14409 + | 2 uc010nxq.1 [3] chr1 11874-14409 + | 3 uc010nxr.1 ------- seqinfo: 93 sequences (1 circular) from hg19 genome&gt; length(GR)[1] 82960 The ‘grouped’ function transcriptsBy, exonsBy, cdsBy, intronsByTranscript, fiveUTRsByTranscript and threeUTRsByTranscript extract genomic features of a given type grouped based on another type of genomic feature. 123456789101112131415161718192021222324&gt; GRList &lt;- transcriptsBy(txdb, by = "gene")&gt; GRListGRangesList object of length 23459:$1 GRanges object with 2 ranges and 2 metadata columns: seqnames ranges strand | tx_id tx_name &lt;Rle&gt; &lt;IRanges&gt; &lt;Rle&gt; | &lt;integer&gt; &lt;character&gt; [1] chr19 58858172-58864865 - | 70455 uc002qsd.4 [2] chr19 58859832-58874214 - | 70456 uc002qsf.2$10 GRanges object with 1 range and 2 metadata columns: seqnames ranges strand | tx_id tx_name [1] chr8 18248755-18258723 + | 31944 uc003wyw.1$100 GRanges object with 1 range and 2 metadata columns: seqnames ranges strand | tx_id tx_name [1] chr20 43248163-43280376 - | 72132 uc002xmj.3...&lt;23456 more elements&gt;-------seqinfo: 93 sequences (1 circular) from hg19 genome The transcriptsBy function returns a GRangesList class object. The show method for a GRangesList object will display as a list of GRanges objects. And, at the bottom the seqinfo will be displayed once for the entire list. Then standard GRanges and GRangesList accessors can be used to deal with the returnings. And one can also leverage many nice IRanges methods. EnsDb Similar to the TxDb objects/packages, EnsDb objects/packages provide genomic coordinates of gene models along with additional annotations (e.g. gene names, biotypes etc) but are tailored to annotations provided by Ensembl. The central methods implemented for EnsDb objects allow also the use of the EnsDb specific filtering framework to retrieve only selected information from the database. 1234567library(EnsDb.Hsapiens.v86)edb = EnsDb.Hsapiens.v86&gt; class(edb)[1] "EnsDb"attr(,"package")[1] "ensembldb" key() function has an additional filter parameter, which accepts AnnotationFilter object. 123keys &lt;- head(keys(edb, keytype="GENEID"))keys(edb, filter=list(GeneBiotypeFilter("lincRNA"), SeqNameFilter("Y"))) 12345678 [1] &quot;ENSG00000129816&quot; &quot;ENSG00000129845&quot; &quot;ENSG00000131538&quot; &quot;ENSG00000147753&quot; &quot;ENSG00000147761&quot; &quot;ENSG00000176728&quot; &quot;ENSG00000180910&quot; [8] &quot;ENSG00000183385&quot; &quot;ENSG00000184991&quot; &quot;ENSG00000185700&quot; &quot;ENSG00000212855&quot; &quot;ENSG00000212856&quot; &quot;ENSG00000215560&quot; &quot;ENSG00000223517&quot;[15] &quot;ENSG00000223641&quot; &quot;ENSG00000224075&quot; &quot;ENSG00000224989&quot; &quot;ENSG00000225516&quot; &quot;ENSG00000225520&quot; &quot;ENSG00000226362&quot; &quot;ENSG00000226906&quot;[22] &quot;ENSG00000227439&quot; &quot;ENSG00000228240&quot; &quot;ENSG00000228296&quot; &quot;ENSG00000228379&quot; &quot;ENSG00000228786&quot; &quot;ENSG00000228890&quot; &quot;ENSG00000229236&quot;[29] &quot;ENSG00000229308&quot; &quot;ENSG00000229643&quot; &quot;ENSG00000230663&quot; &quot;ENSG00000231141&quot; &quot;ENSG00000231535&quot; &quot;ENSG00000232348&quot; &quot;ENSG00000232419&quot;[36] &quot;ENSG00000233522&quot; &quot;ENSG00000233699&quot; &quot;ENSG00000233864&quot; &quot;ENSG00000235059&quot; &quot;ENSG00000235412&quot; &quot;ENSG00000236951&quot; &quot;ENSG00000237048&quot;[43] &quot;ENSG00000237069&quot; &quot;ENSG00000237563&quot; &quot;ENSG00000239225&quot; &quot;ENSG00000240450&quot; &quot;ENSG00000251510&quot; &quot;ENSG00000254488&quot; &quot;ENSG00000260197&quot;[50] &quot;ENSG00000277930&quot; &quot;ENSG00000278847&quot; &quot;ENSG00000280961&quot; keys in mapIds and select also accepts AnnotationFilter object. 1234567txs &lt;- select(edb, keys=list(GeneBiotypeFilter("lincRNA"), SeqNameFilter("Y")), columns=c("TXID", "TXSEQSTART", "TXBIOTYPE"))&gt; head(txs, n=3) TXID TXSEQSTART TXBIOTYPE GENEBIOTYPE SEQNAME1 ENST00000250776 6390431 lincRNA lincRNA Y2 ENST00000250805 9753156 lincRNA lincRNA Y3 ENST00000253838 22439593 lincRNA lincRNA Y Other Questions Question: Difference between GO.db, biomaRt, and org.Hs.eg.db in GO annotations GO.db and org.Hs.eg.db are copies of the GO annotations. GO.db is updated every 6 months with each release of Bioconductor. org.Hs.eg.db is also updated at the same time and using GO.db. biomaRt connects to the server where the informations is stored, so it will be the most up to date. If you want a stable release you can use either GO.db or org.Hs.eg.db, if you want the most up to date (from yesterday) data every time you do an analysis you can use biomaRt. Question: org.Hs.eg.db - hg38 build? The orgDb packages don’t really contain any positional annotation. They used to, but these days you will be directed to a TxDb package if you try to get positional info. And the TxDb have the build in the package name. The orgDb packages mostly contain mappings between various databases and some functional annotation, none of which is based on any build. In fact, most of that stuff is updated weekly or monthly, so the orgDb packages get outdated to a certain extent rather quickly. Change log 20180918: create the note.]]></content>
      <categories>
        <category>R</category>
        <category>Bioconductor</category>
      </categories>
      <tags>
        <tag>R</tag>
        <tag>Bioconductor</tag>
        <tag>Bioconductor package</tag>
        <tag>annotation package</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Biological ID Conversion]]></title>
    <url>%2Fblog%2F2018%2F09%2FBiological-ID-Conversion%2F</url>
    <content type="text"><![CDATA[ID mapping is annoying but we have to face very often. This note is a collection of methods to deal with this trouble. R (Bioconductor) There are lots of annotation packages in Bioconductor and they contain various kinds of annotation we need and we don’t need. Different series of annotation packages may have different design purpose, and these differences should be considered when in practice. For ID conversion, two main resources can be used: biomaRt, the R interface of BioMart, and various specialized annotation packages. biomaRt Ref: The biomaRt users guide biomaRt is a R interface to BioMart databases. It’s very powerful and ID conversion is only one of many applications. The package enables retrieval of large amounts of data in a uniform way without the need to know the underlying database schemas or write complex SQL queries. Examples of BioMart databases are Ensembl, COSMIC, Uniprot, HGNC, Gramene, Wormbase and dbSNP mapped to Ensembl. These major databases give biomaRt users direct access to a diverse set of data and enable a wide range of powerful online queries from gene annotation to database mining. via: http://www.bioconductor.org/packages/release/bioc/html/biomaRt.html 1library("biomaRt") Display all available BioMart web services 1listMarts() 12345 biomart version1 ENSEMBL_MART_ENSEMBL Ensembl Genes 932 ENSEMBL_MART_MOUSE Mouse strains 933 ENSEMBL_MART_SNP Ensembl Variation 934 ENSEMBL_MART_FUNCGEN Ensembl Regulation 93 Choose to query the Ensembl BioMart database. 1ensembl=useMart("ensembl") Look at which datasets are available in the selected BioMart by using the function 1listDatasets(ensembl)[1:5, ] 123456 dataset description version1 acarolinensis_gene_ensembl Anole lizard genes (AnoCar2.0) AnoCar2.02 amelanoleuca_gene_ensembl Panda genes (ailMel1) ailMel13 amexicanus_gene_ensembl Cave fish genes (AstMex102) AstMex1024 anancymaae_gene_ensembl Ma&apos;s night monkey genes (Anan_2.0) Anan_2.05 aplatyrhynchos_gene_ensembl Duck genes (BGI_duck_1.0) BGI_duck_1.0 Update the Mart object using the function useDataset() 1ensembl = useDataset("hsapiens_gene_ensembl", mart=ensembl) Or alternatively if the dataset one wants to use is known in advance, we can select a BioMart database and dataset in one step by: 1ensembl = useMart("ensembl", dataset="hsapiens_gene_ensembl") Shows all available filters in the selected dataset 12filters = listFilters(ensembl)filters[1:5,] 123456## name description## 1 chromosome_name Chromosome/scaffold name## 2 start Start## 3 end End## 4 band_start Band Start## 5 band_end Band End Displays all available attributes in the selected dataset 12attributes = listAttributes(ensembl)attributes[1:5,] 123456 name description page1 ensembl_gene_id Gene stable ID feature_page2 ensembl_gene_id_version Gene stable ID version feature_page3 ensembl_transcript_id Transcript stable ID feature_page4 ensembl_transcript_id_version Transcript stable ID version feature_page5 ensembl_peptide_id Protein stable ID feature_page Annotate a set of Affymetrix identifiers with HUGO symbol and chromosomal locations of corresponding genes 12345affyids=c(&quot;202763_at&quot;,&quot;209310_s_at&quot;,&quot;207500_at&quot;)getBM(attributes=c(&apos;affy_hg_u133_plus_2&apos;, &apos;entrezgene&apos;), filters = &apos;affy_hg_u133_plus_2&apos;, values = affyids, mart = ensembl) 1234 affy_hg_u133_plus_2 entrezgene1 209310_s_at 8372 207500_at 8383 202763_at 836 Retrieve all HUGO gene symbols of genes that are located on chromosomes 17,20 or Y, and are associated with specific GO terms. 12345go=c("GO:0051330","GO:0000080","GO:0000114","GO:0000082")chrom=c(17,20,"Y")getBM(attributes= "hgnc_symbol", filters=c("go","chromosome_name"), values=list(go, chrom), mart=ensembl) 1234567 hgnc_symbol1 RPS6KB12 CDC63 RPA14 CDK35 MCM86 CRLF3 Annotate a set of EntrezGene identifiers with GO annotation. 123456entrez=c("673","837")goids = getBM(attributes = c('entrezgene', 'go_id'), filters = 'entrezgene', values = entrez, mart = ensembl)head(goids) 1234567 entrezgene go_id1 673 GO:00001662 673 GO:00046723 673 GO:00046744 673 GO:00055245 673 GO:00064686 673 Ensembl id to gene symbol and entrez id. 123456789library(biomaRt)ensembl = useMart("ensembl", dataset = "hsapiens_gene_ensembl")ensg = c('ENSG00000242268.2', 'ENSG00000158486.13')# get stable idensg.no_version = sapply(strsplit(as.character(ensg),"\\."),"[[",1)getBM(attributes = c('ensembl_gene_id', 'entrezgene', 'hgnc_symbol'), filters = 'ensembl_gene_id', values=ensg.no_version, mart=ensembl) 123 ensembl_gene_id entrezgene hgnc_symbol1 ENSG00000158486 55567 DNAH32 ENSG00000242268 NA LINC02082 If you do not want to NA, use na.omit to remove those genes that can’t be transformed. OrgDb packages + bitr Ref: clusterProfiler - bitr orgDb packages are gene-centric annotation packages at organism level, such as org.Hs.eg.db, org.Mmu.eg.db. 1library(org.Hs.eg.db) Here is org.Hs.eg package. We can see all the resources used to build this package. 123456789101112131415161718192021222324252627282930313233&gt; org.Hs.eg.dbOrgDb object:| DBSCHEMAVERSION: 2.1| Db type: OrgDb| Supporting package: AnnotationDbi| DBSCHEMA: HUMAN_DB| ORGANISM: Homo sapiens| SPECIES: Human| EGSOURCEDATE: 2018-Apr4| EGSOURCENAME: Entrez Gene| EGSOURCEURL: ftp://ftp.ncbi.nlm.nih.gov/gene/DATA| CENTRALID: EG| TAXID: 9606| GOSOURCENAME: Gene Ontology| GOSOURCEURL: ftp://ftp.geneontology.org/pub/go/godatabase/archive/latest-lite/| GOSOURCEDATE: 2018-Mar28| GOEGSOURCEDATE: 2018-Apr4| GOEGSOURCENAME: Entrez Gene| GOEGSOURCEURL: ftp://ftp.ncbi.nlm.nih.gov/gene/DATA| KEGGSOURCENAME: KEGG GENOME| KEGGSOURCEURL: ftp://ftp.genome.jp/pub/kegg/genomes| KEGGSOURCEDATE: 2011-Mar15| GPSOURCENAME: UCSC Genome Bioinformatics (Homo sapiens)| GPSOURCEURL: | GPSOURCEDATE: 2018-Mar26| ENSOURCEDATE: 2017-Dec04| ENSOURCENAME: Ensembl| ENSOURCEURL: ftp://ftp.ensembl.org/pub/current_fasta| UPSOURCENAME: Uniprot| UPSOURCEURL: http://www.UniProt.org/| UPSOURCEDATE: Mon Apr 9 20:58:54 2018Please see: help('select') for usage information Use keytypes() to list all supporting types. 1keytypes(org.Hs.eg.db) 1234 [1] &quot;ACCNUM&quot; &quot;ALIAS&quot; &quot;ENSEMBL&quot; &quot;ENSEMBLPROT&quot; &quot;ENSEMBLTRANS&quot; &quot;ENTREZID&quot; &quot;ENZYME&quot; &quot;EVIDENCE&quot; [9] &quot;EVIDENCEALL&quot; &quot;GENENAME&quot; &quot;GO&quot; &quot;GOALL&quot; &quot;IPI&quot; &quot;MAP&quot; &quot;OMIM&quot; &quot;ONTOLOGY&quot; [17] &quot;ONTOLOGYALL&quot; &quot;PATH&quot; &quot;PFAM&quot; &quot;PMID&quot; &quot;PROSITE&quot; &quot;REFSEQ&quot; &quot;SYMBOL&quot; &quot;UCSCKG&quot; [25] &quot;UNIGENE&quot; &quot;UNIPROT&quot; Key types supported by differenct packages can be different. 1keytypes(org.Ss.eg.db) 12 [1] &quot;ACCNUM&quot; &quot;ALIAS&quot; &quot;ENTREZID&quot; &quot;ENZYME&quot; &quot;EVIDENCE&quot; &quot;EVIDENCEALL&quot; &quot;GENENAME&quot; &quot;GO&quot; &quot;GOALL&quot; [10] &quot;ONTOLOGY&quot; &quot;ONTOLOGYALL&quot; &quot;PATH&quot; &quot;PMID&quot; &quot;REFSEQ&quot; &quot;SYMBOL&quot; &quot;UNIGENE&quot; &quot;UNIPROT&quot; Convert Ensembl ids to entrez id and gene symbol. 123456789library(clusterProfiler)library(org.Hs.eg.db)ensg = c('ENSG00000242268.2', 'ENSG00000158486.13')# remove version numberensg.no_version = sapply(strsplit(as.character(ensg),"\\."),"[[",1)bitr(ensg.no_version, fromType="ENSEMBL", toType=c("ENTREZID", "SYMBOL"), OrgDb="org.Hs.eg.db") 1234&apos;select()&apos; returned 1:1 mapping between keys and columns ENSEMBL ENTREZID SYMBOL1 ENSG00000242268 100507661 LINC020822 ENSG00000158486 55567 DNAH3 Other Annotation Packages Apart from the OrgDb packages, there are also many other annotation packages like TxDbpackages and EnsDb packages, which provide various kinds of information. And most of them are based on AnnotationDb object, and one can use standard select function to retrieve information needed. NCBI gene DATA Sometimes we want to have all information on local disks and use in-house scripts to do the conversion. ftp://ftp.ncbi.nih.gov/gene/DATA provide most up-to-date and comprehensive collections of gene-centric information. By incorporating the data from LocusLink in an Entrez database with gene-specific data from other species, you now have a single point of lookup for gene-specific information for the taxa within the scope of the RefSeq project. You also have more immediate access to related data that was cumbersome to maintain independent of Entrez, and can harness the power of Entrez-based tools such as Entrez Programming Utilities (E-Utilities) and MyNCBI. via: https://www.ncbi.nlm.nih.gov/entrez/query/static/help/LL2G.html#files This README discribes all the files included. Here is a short summary. Entrez Gene file name Comments DATA/ASN_BINARY Files in this directory contain comprehensive extractions from Entrez Gene in ASN.1 format. DATA/GENE_INFO extractions from Entrez Gene in the same format as the gene_info file. Each file contains a subset of data for the species or taxonomic group indicated by the file name. DATA/expression reports of normalized RNA expression levels computed from RNA-seq data for human, mouse, and rat genes. gene2accession a comprehensive report of the accessions that are related to a GeneID. It includes sequences from the international sequence collaboration, Swiss-Prot, and RefSeq. The RefSeq subset of this file is also available as gene2refseq… If you want to convert any accessions into GeneIDs, this one file should suffice. gene2ensembl This file reports matches between NCBI and Ensembl annotation based on comparison of rna and protein features. gene2vega This file reports matches between NCBI and Vega annotation. gene2go GeneID/GO ID/Evidence Code. Consolidated summary based on gene_association files from the GO Consortium and Entrez Gene’s gene_info file. gene2pubmed gene2pubmed includes the identifier for the species of the GeneID (i.e. the Taxonomy ID). gene2refseq This file is the RefSeq subset of gene2accession. The file in Entrez Gene does not include information about secondary accessions. This function is now provided from the RefSeq ftp site, as documented in the current release notes: ftp://ftp.ncbi.nlm.nih.gov/refseq/release/release-notes/RefSeq-release#.txt, where # is the value of the current release number. gene2sts GeneID/UniSTS marker ID relationship gene2unigene GeneID/UniGene cluster relationship gene_group report of genes and their relationships to other genes gene_orthologs report of orthologous genes gene_history comprehensive information about GeneIDs that are no longer current gene_info GeneID, names, map locations, and database cross-reference. gene_neighbors reports neighboring genes for all genes placed on a given genomic sequence. gene_refseq_uniprotkb_collab report of the relationship between NCBI Reference Sequence protein accessions and UniProtKB protein accessions mim2gene_medgen report of the relationship between MIM numbers (OMIM), GeneIDs, and Records in MedGen API Many databases provide APIs to help access their data and some of them can be used for id conversion. But I do not recommend to use these APIs directly if one dose not want to spend much time on this job, as they can be changed over time and users have to be familiar with the data structure provided. Many commonly used APIs have external software or packages to access, and you may use Google to find them before using the APIs. Ensembl REST API Ensembl REST API provides many user-friendly interfaces to retrive information. And there are three APIs for cross biological id mapping. GET xrefs/symbol/:species/:symbol looks up an external symbol and returns all Ensembl objects linked to it. GET xrefs/id/:id performs lookups of Ensembl Identifiers and retrieve their external references in other databases. GET xrefs/name/:species/:name performs a lookup based upon the primary accession or display label of an external reference and returning the information we hold about the entry. I guess biomaRt aforementioned is actually a well-capsulated software that communicates with databases through APIs. KEGG API KEGG API is a REST-stype Application Programming Interface to the KEGG database resource. We can use this API by bitr_kegg in clusterProfiler package or KEGGREST package. bitr_kegg Ref: clusterProfiler - bitr_kegg 12345library(clusterProfiler)hg = c("4597", "7111", "5266", "2175", "755", "23046")bitr_kegg(hg, fromType='kegg', toType='ncbi-proteinid', organism='hsa') 1234567 kegg ncbi-proteinid1 2175 NP_0001262 23046 NP_0012390293 4597 NP_0024524 5266 NP_0026295 7111 NP_0011595886 755 NP_004919 The ID type (both fromType &amp; toType) should be one of ‘kegg’, ‘ncbi-geneid’, ‘ncbi-proteinid’ or ‘uniprot’. The ‘kegg’ is the primary ID used in KEGG database. The data source of KEGG was from NCBI. A rule of thumb for the ‘kegg’ ID is entrezgene ID for eukaryote species and Locus ID for prokaryotes. Many prokaryote species don’t have entrezgene ID available. For example we can check the gene information of ece:Z5100 in http://www.genome.jp/dbget-bin/www_bget?ece:Z5100, which have NCBI-ProteinID and UnitProt links in the Other DBs Entry, but not NCBI-GeneID. The full list of KEGG supported organisms can be accessed via http://www.genome.jp/kegg/catalog/org_list.html. KEGGREST KEGGREST provides a client interface to the KEGG REST server. And keggConv() can be used for converting identifiers. 1library(KEGGREEST) Convert between KEGG identifiers and outside identifiers. 1keggConv("ncbi-proteinid", c("hsa:10458", "ece:Z5100")) 12 hsa:10458 ece:Z5100 &quot;ncbi-proteinid:NP_059345&quot; &quot;ncbi-proteinid:AAG58814&quot; …or get the mapping for an entire species: 1head(keggConv("eco", "ncbi-geneid")) 12ncbi-geneid:944742 ncbi-geneid:945803 ncbi-geneid:947498 ncbi-geneid:945198 ncbi-geneid:944747 ncbi-geneid:944749 &quot;eco:b0001&quot; &quot;eco:b0002&quot; &quot;eco:b0003&quot; &quot;eco:b0004&quot; &quot;eco:b0005&quot; &quot;eco:b0006&quot; Reversing the arguments does the opposite mapping: 1head(keggConv("ncbi-geneid", "eco")) 12 eco:b0001 eco:b0002 eco:b0003 eco:b0004 eco:b0005 eco:b0006 &quot;ncbi-geneid:944742&quot; &quot;ncbi-geneid:945803&quot; &quot;ncbi-geneid:947498&quot; &quot;ncbi-geneid:945198&quot; &quot;ncbi-geneid:944747&quot; &quot;ncbi-geneid:944749&quot; Web Server DAVID - Gene ID Conversion Tool. easy to use, but a bit old. BioMart - Ensembl. up-to-date and powerful. Change log 20180918: create the note.]]></content>
      <categories>
        <category>bioinformatics</category>
        <category>basic</category>
      </categories>
      <tags>
        <tag>ID conversion</tag>
        <tag>bioconductor</tag>
        <tag>biomaRt</tag>
        <tag>OrgDb</tag>
        <tag>API</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Remove Microbial Contamination in Reads]]></title>
    <url>%2Fblog%2F2018%2F07%2FRemove-Contamination-of-Pokaryotic-Organisms-in-Reads%2F</url>
    <content type="text"><![CDATA[Purpose in short: I’ve got both illumina (PE and MPE) and PacBio reads of an insect for de novo geome assembly. Since whole bodies were used for extracted DNA, I thought there was some contamination in the raw sequencing reads, so I want to remove them before assembling. The main tool for illumina reads I used was BBDuk, which belongs to the BBTools suite. Seal and BBSplit can also do this, but BBDuk is the most suitable for my purpose. See discussions here: Question: How to remove contamination from the transcriptome assembly. BBTools is quite versatile, fast, and convenient! Though now I don’t fully understand every kit in it, the experience was very good from the tools I’ve tried. The author Brian Bushnell is very active and responsive. Here is a handy summary of BBMap: Yes … BBMap can do that!. But for PacBio long reads, I didn’t find any tools specialized for that. I tried several tools including mapPacBio.sh (based on bbmap, and is tuned for the error profile of long reads.), blasr, MashMap, and minimap2. At last, I found minimap2 best met my needs. Prepare the contamination library Download all the sequences of bacteria, viral, fungi, protozoa, and archaea using refseq2kraken. See note Detect Microbial Contamination in Contigs by Kraken for details. We can also use ncbi-genome-download to download them. 123# folder structures$ ls genomes/refseq/archaea bacteria fungi protozoa viral Merge all the sequences into one single Fasta. Since I’ve got the mitochondrion DNA sequences of the target organism, I added them in the contaminant library to remove reads from mtDNA too. This library is refered as to $contaminants below. After discussing with the author of MashMap, I included several insects’ genomes of the same order of my target insect to improve the specificity of aligners. This library is refered as to $contaminants2. BBDuk for short reads BBDuk is a member of the BBTools package. “Duk” stands for Decontamination Using Kmers. BBDuk is extremely fast, scalable, and memory-efficient, while maintaining greater sensitivity and specificity than other tools. 1234567891011121314151617WORKDIR=/parastor300/niuyw/Project/beetle_genome_171231/data/secondANNODIR=/home/niuyw/RefDataTOOLDIR=/home/niuyw/softwarepath2java=$TOOLDIR/jre1.8.0_111/bin/javaDATADIR=/home/niuyw/Project/beetle_genome_171231/data/secondPPN=24path2bbduk=$TOOLDIR/bbmap/bbduk.shcontaminants=$ANNODIR/NCBI_contaminants/contaminants_4_bettle_genome.fafor sample in 270B 500B 800B 3k_1 5k-1 5k-2 10kdo$path2bbduk ref=$&#123;contaminants&#125; threads=$&#123;PPN&#125; ordered=t k=31 in=$&#123;DATADIR&#125;/TrimGalore/$&#123;sample&#125;_R1_val_1.fq.gz in2=$&#123;DATADIR&#125;/TrimGalore/$&#123;sample&#125;_R2_val_2.fq.gz out=$&#123;DATADIR&#125;/bbduk/$&#123;sample&#125;.R1.fq out2=$&#123;DATADIR&#125;/bbduk/$&#123;sample&#125;.R2.fq outm=$&#123;DATADIR&#125;/bbduk/$&#123;sample&#125;.bad.R1.fq outm2=$&#123;DATADIR&#125;/bbduk/$&#123;sample&#125;.bad.R2.fqdone The ‘good’ reads will be in ${sample}.R*.fq, and the ‘bad’ ones will be in ${sample}.bad.R*.fq. Long reads mapPacBio mapPacBio.sh is based on bbmap, and is tuned for the error profile of long reads. 12345678910111213141516WORKDIR=/parastor300/niuyw/Project/beetle_genome_171231ANNODIR=/home/niuyw/RefDataTOOLDIR=/home/niuyw/softwarepath2java=$TOOLDIR/jre1.8.0_111/bin/javaDATADIR=/home/niuyw/Project/beetle_genome_171231/data/thirdPPN=24path2bbmap=$TOOLDIR/bbmapcontaminants=$ANNODIR/NCBI_contaminants/contaminants_4_bettle_genome.fa$path2bbmap/mapPacBio.sh threads=$&#123;PPN&#125; ref=$&#123;contaminants&#125; in=third_all.fasta maxlen=6000 out=bbmap.sam$path2bbmap/reformat.sh unmappedonly in=bbmap.sam out=good.fa The output of bbmap or mapPacBio.sh is SAM, and both mapped and unmapped reads are saved in one file. reformat.sh was used to extract mapped reads and transfrom it to FASTA. See this: Question: bbmap command to extract mapped and unmapped pair end reads. But mapPacBio was very very very slow, even I used 24 threads. After about 25 days, the ouput was about 4.6G (my input was about 60G!), though it was still increasing, I killed the job. blasr blasr may be the one of the first long-read aligner [1]. So I gave it a shot. 1$TOOLDIR/anaconda2/bin/blasr third_all.fasta $contaminants --nproc $PPN --out blasr.bad --unaligned blasr.good Related: output unmapped reads. But it went wrong: 12[INFO] 2018-07-02T18:56:18 [blasr] started.ERROR! Reading fasta files greater than 4Gbytes is not supported. What?! My fasta was about 60G. If I wanted use blasr, I had to split the input into small ones, but I didn’t want to. MashMap MashMap is a fast approximate aligner for long DNA sequences [2]. It’s very fast and is easy to use. 12PPN=8$TOOLDIR/mashmap-Linux64-v2.0/mashmap -r $contaminants -q third_all.fasta -o mashmap.out -s is the minimum query length (default is 5000), and --pi is the minimum identity to be reported (default is 85). The output of MashMap is like this. Separated by space, it is query name, length, 0-based start, end, strand, target name, length, start, end and mapping nucleotide identity in turn. 123m161109_080520_42256_c101052872550000001823247601061737_s1_p0/104/24935_31494 6559 0 4999 - NC_037282.1 2038340 596479 601478 82.1711m161109_080520_42256_c101052872550000001823247601061737_s1_p0/357/31541_41029 9488 0 4999 - NC_004326.2 1343557 117176 122175 81.9933m161109_080520_42256_c101052872550000001823247601061737_s1_p0/562/0_12626 12626 7626 12625 + NC_001224.1 85779 50909 55908 82.2314 Here is also a thread talking about contamination: Decontamination of bacterial sequences in an assembly. The author suggested that “One more thing that may be helpful for you is to include the representative genome (corresponding to your assembly) in the database as well. This would help improve the specificity of the method for correct portions of your assembly.” But, the default parameters MashMap use are a bit loose, I wanted to use more strict parameters. When I lowered the -s and --pi, the program needed huge RAM and couldn’t run on our machine. I’ve reported this issue to the author: Decontamination of bacterial sequences in an assembly. And, there were also some questions about its alignment: Questions about the alignment of MashMap. I first ran MashMap with different parameters and got several outputs: 12345678# run1, with default parameters$path2mashmap -r $contaminants -q third_all.fasta -o mashmap.out# run2, with -s 2500 --pi 80$path2mashmap -t 8 -r $contaminants -q third_all.fasta -s 2500 --pi 80 -o mashmap2.out# run3, with -s 500 --pi 85$path2mashmap -t 8 -r $contaminants -q third_all.fasta -s 500 --pi 85 -o mashmap3.out And the outputs from three runs varied: 123456789101112131415# there are 6633142 sequences of input$ grep -c '&gt;' third_all.fasta6633142# run1cut -f 1 -d ' ' mashmap.out |sort|uniq|wc -l463569# run2cut -f 1 -d ' ' mashmap2.out |sort|uniq|wc -l2821004# run3cut -f 1 -d ' ' mashmap3.out |sort|uniq|wc -l6189307 As can be seen, nearly all the sequences were aligned to contaminant library. That really shocked me! Then I checked the top 10 sequences with highest identity and top 10 ones with loweset identity from the first run using blastn. The highest ones were fine. There were some differences between hits reported by blastn and MashMap, but maybe it’s because they used different databases. But the loweset ones were problematic. Most of them were ‘No significant similarity found’ when default parameters of blastn were used. And when I unselected ‘Low complexity regions’, the alignments were unreliable. There maybe something with ‘low complexity regions’ or ‘repeat’ things. And the author explained: Mashmap identity is an estimate based on Jaccard similarity- not the precise identity; unfortunately the Jaccard-similarity based metric delivers poor specificity in cases when the source of reads is absent from database. See if you can include an insect reference genome (s) in the reference list to avoid this. Then I added several insects’ genomes of the same order of the target insect into the contaminant library and ran MapshMap again: 1$path2mashmap -t 20 -r $contaminants2 -q third_all.fasta -s 500 --pi 80 -o mashmap4.out For each read, it will be one of two states: mapped or unmapped, and for the mapped, it will be one of three states: mapped to insects (good), mapped to contaminants (bad) and mapped to both (ambivalent). So I counted reads in each categories: 123456No. of total reads: 6633142 No. of reads in the mashmap.output: 6214500 No. of good: 853515 No. of bad: 138799 No. of ambivalent: 5222186 No. of reads not in the mashmap.output: 418642 As can be seen, majority of reads had been mapped ambivalently, and here is a example of such reads: 1234567m161123_064622_42256_c101049952550000001823247601061783_s1_p0/52396/4594_8610 4016 3500 4015 + NW_017852934.1 2683736 1681820 1682319 79.4204m161123_064622_42256_c101049952550000001823247601061783_s1_p0/52396/4594_8610 4016 1000 1999 + NW_019280650.1 1003565 813077 813577 78.0766m161123_064622_42256_c101049952550000001823247601061783_s1_p0/52396/4594_8610 4016 2500 2999 - LJIG01019880.1 38067 34865 35364 79.3626m161123_064622_42256_c101049952550000001823247601061783_s1_p0/52396/4594_8610 4016 500 999 + NC_007418.3 31381287 24981081 24981580 79.5573m161123_064622_42256_c101049952550000001823247601061783_s1_p0/52396/4594_8610 4016 3000 3499 - kraken:taxid|76857|NZ_CP022123.1 2521394 1537365 1537864 81.3159m161123_064622_42256_c101049952550000001823247601061783_s1_p0/52396/4594_8610 4016 0 499 - kraken:taxid|1202539|NC_018417.1 157543 40864 41363 79.4397m161123_064622_42256_c101049952550000001823247601061783_s1_p0/52396/4594_8610 4016 1500 2499 + kraken:taxid|1936081|NZ_CP019389.1 3752836 1813582 1814081 76.7726 So it’s very hard to extract good ones. After all this, I got two points: I should add some insects’ genomes into the library to reduce the false positive hits. Alignment from MashMap maybe not so reliable (). minimap2 minimap2 is a versatile pairwise aligner for genomic and spliced nucleotide sequences created by Heng Li [3]. It’s easy to use and runs very fast. mimimap2 1234PPN=24$TOOLDIR/minimap2-2.8_x64-linux/minimap2 -x map-pb $contaminants third_all.fasta -t $PPN -a -Q &gt; minimap.sam# -a: output the SAM format# -Q:not output base quality in SAM I had about 60G input, and minimap2 was so fast, finished after 77 CPU hours (4 real hours, 24 threads). The output was very huge (~685G!), and it’s like this (truncated, and the sequences were replaced by ‘seq’.): 12345678m54174_171023_074758/9962478/22561_26454 4 * 0 0 * * 0 0 seq *m54174_171023_074758/9962478/22561_26454 4 * 0 0 * * 0 0 seq *m54174_171023_074758/9962478/22561_26454 16 kraken:taxid|1094466|NC_017025.1 1304571 1 3661S5M2D6M4I8M1D4M1D13M1D11M1I16M1D13M1D13M1D27M1D4M1D4M2I4M2D12M2D9M1D2M1D13M1D12M1D14M1D13M1D6M2I7M1D7M * 0 0 seq * NM:i:43 ms:i:220 AS:i:220 nn:i:0 tp:A:P cm:i:3 s1:i:40 s2:i:0 dv:f:0.0200m54174_171023_074758/9962478/22561_26454 4 * 0 0 * * 0 0 seq *m54174_171023_074758/9962478/22561_26454 16 kraken:taxid|1323664|NZ_CP012748.1 2198444 1 903S12M1I12M1I12M1I4M1I6M1I2M1I12M1I39M1I9M1I6M5I4M4I7M1I2M1I5M1D6M1I12M1I12M1I12M1I13M1I11M2I5M1D8M2I22M1I13M1I23M1I12M1I12M1I10M1I3M2I10M3I5M1I6M1I3M1I16M1D6M1I12M2I6M1D3M1I2M1I7M1D4M1I5M1I7M1I12M1I6M7I3M1I3M1D6M1I11M1I12M1I12M2464S * 0 0 seq * NM:i:95 ms:i:432 AS:i:432 nn:i:0 tp:A:P cm:i:4 s1:i:49 s2:i:77 dv:f:0.0701 SA:Z:kraken:taxid|1323664|NZ_CP012748.1,2198449,-,1461S464M47I1921S,15,95;kraken:taxid|1323664|NZ_CP012748.1,2198443,-,1853S470M47I1523S,1,95;kraken:taxid|1323664|NZ_CP012748.1,2198444,-,683S469M51I2690S,4,106;kraken:taxid|1323664|NZ_CP012748.1,2198449,-,2305S464M17I1107S,16,105;kraken:taxid|1323664|NZ_CP012748.1,2198456,-,2607S450M11I825S,22,93;kraken:taxid|1323664|NZ_CP012748.1,2198451,-,430S460M63I2940S,3,105;kraken:taxid|1323664|NZ_CP012748.1,2198449,-,13S464M94I3322S,11,133;m54174_171023_074758/9962478/22561_26454 2064 kraken:taxid|1323664|NZ_CP012748.1 2198449 15 1461H7M1I12M1I7M1D4M1I12M1I4M1D10M1I10M1D24M1I18M2I9M1I8M1I12M2I9M1I2M6I7M1I13M5I5M1I7M1D4M1I10M1I2M1I12M1I12M1I12M2I13M1I11M1I11M1I5M1I7M1I5M1I8M1I12M1I5M1D4M1I2M1I4M2I2M1I7M5D5M1D7M2I21M1I24M4I8M1I12M1I12M1I4M1I20M1I12M1921H * 0 0 seq * NM:i:95 ms:i:420 AS:i:420 nn:i:0 tp:A:P cm:i:6 s1:i:80 s2:i:0 dv:f:0.0526 SA:Z:kraken:taxid|1323664|NZ_CP012748.1,2198444,-,903S469M57I2464S,1,95;kraken:taxid|1323664|NZ_CP012748.1,2198443,-,1853S470M47I1523S,1,95;kraken:taxid|1323664|NZ_CP012748.1,2198444,-,683S469M51I2690S,4,106;kraken:taxid|1323664|NZ_CP012748.1,2198449,-,2305S464M17I1107S,16,105;kraken:taxid|1323664|NZ_CP012748.1,2198456,-,2607S450M11I825S,22,93;kraken:taxid|1323664|NZ_CP012748.1,2198451,-,430S460M63I2940S,3,105;kraken:taxid|1323664|NZ_CP012748.1,2198449,-,13S464M94I3322S,11,133;m54174_171023_074758/9962478/22561_26454 272 kraken:taxid|1323664|NZ_CP012748.1 2198445 0 1010S9M1I2M1I4M1I8M1I12M1I5M1D6M1I12M1I39M1I11M2I3M1I10M1I22M1I13M1I23M1I12M1I12M1I10M1I3M2I11M1I1M2I3M1I6M1I3M1I16M1D6M1I13M4D7M1D5M2I7M1D4M1I5M1I6M1I13M1I6M1I9M1I2M1D7M1I11M1I10M1I15M1I10M1I13M1I12M1I12M1I7M1D4M1I13M2386S * 0 0 * * NM:i:86 ms:i:418 AS:i:418 nn:i:0 tp:A:S cm:i:6 s1:i:77 dv:f:0.0589m54174_171023_074758/9962478/22561_26454 2064 kraken:taxid|1323664|NZ_CP012748.1 2198443 1 1853H19M1I22M4I8M1I12M1I38M1I18M2I9M1I3M1I5M1I12M1I24M1I12M2I24M2I3M1I3M1D5M3I12M1I13M2I3M1I8M1I11M2I8M1D3M1I2M1I11M1I12M1I12M1I5M1D3M1D2M1I12M1I12M1I13M2I1M3I4M1I10M1D14M2I2M1I4M1D4M1I4M1I8M1I5M1D4M1I2M1I12M2I12M1I7M1D10M1523H * 0 0 seq * NM:i:95 ms:i:414 AS:i:414 nn:i:0 tp:A:P cm:i:7 s1:i:60 s2:i:97 dv:f:0.0466 SA:Z:kraken:taxid|1323664|NZ_CP012748.1,2198444,-,903S469M57I2464S,1,95;kraken:taxid|1323664|NZ_CP012748.1,2198449,-,1461S464M47I1921S,15,95;kraken:taxid|1323664|NZ_CP012748.1,2198444,-,683S469M51I2690S,4,106;kraken:taxid|1323664|NZ_CP012748.1,2198449,-,2305S464M17I1107S,16,105;kraken:taxid|1323664|NZ_CP012748.1,2198456,-,2607S450M11I825S,22,93;kraken:taxid|1323664|NZ_CP012748.1,2198451,-,430S460M63I2940S,3,105;kraken:taxid|1323664|NZ_CP012748.1,2198449,-,13S464M94I3322S,11,133; We can clearly found that the same sequence had been mapped to different reference sequences, and it’s been reported several times, with the exactly same line content. This is a limitation of minimap2, as reported: Multiple empty hits? and How does using a multi part index affect the accuracy?. Specifically, when a huge reference is used, minimap2 will split it into multiple parts and align all queries against each part independently. For most parts, minimap2 will print unmapped records. The good news is that Minimap2-2.12 (r827) had addressed this bug. minimap2-arm minimap2-arm is a solution provieded by Hasindu Gamaarachchi, and it merges the results from a multi-part index to achieve a considerably similar output from a single-part index. So I cloned this modified minimap2 to run the job anain. 1234567# installgit clone https://github.com/hasindu2008/minimap2 minimap2-arm &amp;&amp; cd minimap2-arm &amp;&amp; git checkout multipart-merge-tmp &amp;&amp; make# run$TOOLDIR/minimap2-arm/minimap2 -x map-pb -I 500G -t $PPN -a -Q --multi-prefix tmp $contaminants2 third_all.fasta &gt; minimap2.sam# --multi-prefix: enable mergine# -I: split index for every ~500G input bases, this number is far more than the reference. I counted reads in each categories like I did in MashMap part: 12345678No. of total reads: 6633142 No. of reads in the SAM: 6633142 No. of mapped: 661646 No. of good: 490150 No. of bad: 125390 No. of ambivalent: 46106 No. of Unmapped: 5971496 No. of reads not in the SAM: 0 Then I kept the ‘unmapped’, ‘good’ and ‘ambivalent’ reads for downstream analysis. To parse the result and get the clean sequences, I used a simple python script to extract clean fasta and bad fasta ids. (appendix 1) To validate the effciency, I checked several sequences by blastn manually. Belows are two examples. This is the alignment of ‘m161109_080520_42256_c101052872550000001823247601061737_s1_p0/28/0_4873’ by minimap2. 12345678$ grep 'm161109_080520_42256_c101052872550000001823247601061737_s1_p0/28/0_4873' minimap2.samm161109_080520_42256_c101052872550000001823247601061737_s1_p0/28/0_4873 16 kraken:taxid|66084|NC_012416.1 1207118 5 2514S9M1D7M1D9M1I8M1D15M1I6M1I7M1D5M1D15M1D8M1I4M1D11M1D12M1I17M1D5M1D4M1D5M1I3M2D18M1I1M1I33M1I13M2I13M1D6M1I29M1I4M1D26M1D8M1I27M1D3M1D12M1I4M1I8M1I8M1I12M1D3M1D8M1I16M1I12M1I11M1I2M2D6M2D3M1D31M1D9M2I33M1D16M1I15M1D11M2D9M1D25M2I5M1D3M2I11M1D3M1D14M1I8M1D4M1D19M4I29M1D11M1D8M1D2M1D5M1I2M4I9M1D15M1I19M2I7M1I6M1I28M1D3M1D3M1I16M1D2M1D3M1I6M1I14M2I12M1D28M1D3M1D6M2I5M1I32M1D8M1D25M1D8M1D30M1I11M1I1M1I16M1D14M1D17M1D19M1I32M1D14M1D6M1D12M1I50M1D26M1I8M1D3M1D12M1D13M1I8M1D18M1D12M1I24M1D4M2D6M3I10M1D3M2D18M1D3M1I5M1D5M1I3M2D15M1I5M1I7M1I9M1D2M3I31M1I13M1D6M1I22M2D2I3M1I6M1I10M1I5M1D6M2D10M1I16M1I13M1I3M1I13M1I3M3I2M1I11M1I7M1D4M2I8M1D9M1I17M1I3M1I7M2I3M1I20M1D2M3I5M2I5M1I13M5I3M2I6M1I17M2I3M1I5M1D10M2I11M1D13M1D2M1I8M1D9M1D8M1I3M1I12M1I3M1D3M1D5M1I6M1I8M1I5M1I5M1I10M1I4M1I8M1I3M1I7M1I5M1D8M1D8M1I6M1D10M2I9M1I1M1I9M1D10M1D3M1D4M1D6M1D7M2D5M2I5M2I2M2D2M1I16M1I5M14I4M2D3M1I11M1D6M1I1M1I9M1I14M2I6M2I14M1D2M1D6M1D9M2D1M1D22M1D12M1D7M1I13M2D8M1I3M1I15M1I18M1I17M9S * 0 0 seq * NM:i:424 ms:i:2078 AS:i:2078 nn:i:0 tp:A:P cm:i:28 s1:i:477 s2:i:474 dv:f:0.0866 SA:Z:kraken:taxid|66084|NC_012416.1,1360016,-,616S1173M32I3052S,9,195;m161109_080520_42256_c101052872550000001823247601061737_s1_p0/28/0_4873 272 kraken:taxid|1236909|NC_021089.1 1067958 0 2514S9M1D7M1D9M1I8M1D15M1I6M1I7M1D5M1D15M1D8M1I4M1D11M1D12M1I17M1D5M1D4M1D8M7D4M3D15M1I1M1I33M1I13M2I13M1D6M1I29M1I4M1D26M1D8M1I27M1D3M1D12M1I4M1I8M1I8M1I12M1D3M1D8M1I16M1I12M1I11M1I2M2D6M2D3M1D31M1D9M2I33M1D16M1I15M1D11M2D9M1D25M2I5M1D3M2I11M1D3M1D14M1I8M1D4M1D19M4I29M1D11M1D8M1D2M1D5M1I2M4I9M1D15M1I19M2I7M1I6M1I28M1D3M1D3M1I16M1D2M1D3M1I6M1I18M1I3M1I5M1D28M1D3M1D6M2I5M1I32M1D3M1D30M1D8M1D30M1I11M1I1M1I16M1D14M1D22M1D14M1I32M1D14M1D6M1D13M1I49M1D26M1I8M1D3M1D10M1D15M1I8M1D18M1D12M1I25M1D3M2D6M3I10M1D3M2D17M1D4M1I5M1D5M1I3M2D15M1I5M1I7M1I9M1D2M3I35M1I9M1D6M1I22M1I11M1I10M1I5M1D6M2D10M1I16M1I13M1I3M1I13M1I3M3I2M1I11M1I7M1D4M2I8M1D9M1I17M1I3M1I7M2I3M1I20M1D2M3I5M2I5M1I13M5I3M2I6M1I17M2I3M1I5M1D10M2I11M1D13M1D2M1I8M1D9M1D8M1I3M1I12M1I4M1D2M1D5M1I6M1I8M1I5M1I5M1I10M1I4M1I8M1I3M1I7M1I5M1D8M1D8M1I6M1D10M2I9M1I1M1I9M1D13M2D4M1D6M1D7M2D5M2I11M1I16M1I5M14I4M2D4M1I10M1D6M1I1M1I9M1I14M2I6M2I14M1D2M1D6M1D8M2D2M1D22M1D12M1D7M1I12M1D1M1D8M1D8M1I3M1I1M1I5M1I18M1I17M9S * 0 0 * NM:i:435 ms:i:2028 AS:i:2028 nn:i:0 tp:A:S cm:i:28 s1:i:474 dv:f:0.0866m161109_080520_42256_c101052872550000001823247601061737_s1_p0/28/0_4873 272 kraken:taxid|225364|NZ_LK055284.1 991010 0 2514S9M1D7M1D9M1I8M1D15M1I6M1I7M1D5M1D15M1D8M1I4M1D11M1D12M1I17M1D5M1D4M1D8M7D4M3D15M1I1M1I33M1I13M2I13M1D6M1I29M1I4M1D26M1D8M1I27M1D3M1D12M1I4M1I8M1I8M1I12M1D3M1D8M1I16M1I12M1I3M1I3M1D4M1I2M2D6M3D34M1D9M2I33M1D16M1I15M1D11M2D9M1D25M2I6M1D2M2I11M1D3M1D14M1I8M1D4M1D19M4I29M1D11M1D8M1D2M1D5M1I2M4I9M1D15M1I19M2I7M1I6M1I24M1D7M1D4M1I15M1D2M1D3M1I6M1I18M1I3M1I5M1D28M1D3M1D6M2I5M1I32M1D3M1D30M1D8M1D30M1I11M1I1M1I16M1D14M1D22M1D14M1I32M1D12M1D8M1D13M1I49M1D26M1I8M1D3M1D10M1D15M1I8M1D18M1D12M1I25M1D3M2D6M3I10M1D3M2D17M1D4M1I5M1D5M1I3M2D15M1I5M1I7M1I9M1D2M3I31M1I13M1D6M1I22M2D2I3M1I6M1I10M1I5M1D6M2D10M1I16M1I13M1I3M1I13M1I3M3I2M1I11M1I7M1D4M2I8M1D10M1I16M1I3M1I7M2I3M1I20M1D2M3I5M2I5M1I13M5I3M2I6M1I17M2I3M1I5M1D10M2I11M1D13M1D2M1I8M1D9M1D8M1I3M1I12M1I3M1D3M1D5M1I6M1I8M1I5M1I5M1I10M1I4M1I8M1I3M1I7M1I5M1D8M1D8M1I6M1D10M2I9M1I1M1I9M1D10M1D3M1D4M1D6M1D7M2D5M2I5M2I2M2D2M1I16M1I5M14I4M2D4M1I10M1D6M1I1M1I3M1I14M2I3M1I4M1D4M2I14M1D2M1D6M1D8M2D2M1D22M1D12M1D7M1I13M2D8M1I3M1I15M1I18M1I17M9S * NM:i:444 ms:i:1984 AS:i:1984 nn:i:0 tp:A:S cm:i:24 s1:i:437 dv:f:0.0923m161109_080520_42256_c101052872550000001823247601061737_s1_p0/28/0_4873 272 kraken:taxid|163164|NC_002978.6 1039834 0 2514S9M1D7M1D9M1I8M1D15M1I6M1I7M1D5M1D15M1D8M1I4M1D11M1D12M1I17M1D5M1D4M1D8M7D4M3D15M1I1M1I33M1I13M2I13M1D6M1I29M1I4M1D26M1D8M1I27M1D3M1D12M1I4M1I8M1I8M1I12M1D3M1D8M1I16M1I12M1I3M1I3M1D4M1I2M2D6M3D34M1D9M2I33M1D16M1I15M1D11M2D9M1D25M2I6M1D2M2I11M1D3M1D14M1I8M1D4M1D19M4I29M1D11M1D8M1D2M1D5M1I2M4I9M1D15M1I19M2I7M1I6M1I24M1D7M1D4M1I15M1D2M1D3M1I6M1I18M1I3M1I5M1D28M1D3M1D6M2I5M1I32M1D3M1D30M1D8M1D30M1I11M1I1M1I16M1D14M1D22M1D14M1I32M1D12M1D8M1D13M1I49M1D26M1I8M1D3M1D10M1D15M1I8M1D18M1D12M1I25M1D3M2D6M3I10M1D3M2D17M1D4M1I5M1D5M1I3M2D15M1I5M1I7M1I9M1D2M3I31M1I13M1D6M1I22M2D2I3M1I6M1I10M1I5M1D6M2D10M1I16M1I13M1I3M1I13M1I3M3I2M1I11M1I7M1D4M2I8M1D10M1I16M1I3M1I7M2I3M1I20M1D2M3I5M2I5M1I13M5I3M2I6M1I17M2I3M1I5M1D10M2I11M1D13M1D2M1I8M1D9M1D8M1I3M1I12M1I3M1D3M1D5M1I6M1I8M1I5M1I5M1I10M1I4M1I8M1I3M1I7M1I5M1D8M1D8M1I6M1D10M2I9M1I1M1I9M1D10M1D3M1D4M1D6M1D7M2D5M2I5M2I2M2D2M1I16M1I5M14I4M2D4M1I10M1D6M1I1M1I3M1I14M2I3M1I4M1D4M2I14M1D2M1D6M1D8M2D2M1D22M1D12M1D7M1I13M2D8M1I3M1I15M1I18M1I17M9S * 0 NM:i:444 ms:i:1984 AS:i:1984 nn:i:0 tp:A:S cm:i:25 s1:i:452 dv:f:0.0908m161109_080520_42256_c101052872550000001823247601061737_s1_p0/28/0_4873 272 kraken:taxid|1633785|NZ_CP011148.1 1039878 0 2514S9M1D7M1D9M1I8M1D15M1I6M1I7M1D5M1D15M1D8M1I4M1D11M1D12M1I17M1D5M1D4M1D8M7D4M3D15M1I1M1I33M1I13M2I13M1D6M1I29M1I4M1D26M1D8M1I27M1D3M1D12M1I4M1I8M1I8M1I12M1D3M1D8M1I16M1I12M1I3M1I3M1D4M1I2M2D6M3D34M1D9M2I33M1D16M1I15M1D11M2D9M1D25M2I6M1D2M2I11M1D3M1D14M1I8M1D4M1D19M4I29M1D11M1D8M1D2M1D5M1I2M4I9M1D15M1I19M2I7M1I6M1I24M1D7M1D4M1I15M1D2M1D3M1I6M1I18M1I3M1I5M1D28M1D3M1D6M2I5M1I32M1D3M1D30M1D8M1D30M1I11M1I1M1I16M1D14M1D22M1D14M1I32M1D12M1D8M1D13M1I49M1D26M1I8M1D3M1D10M1D15M1I8M1D18M1D12M1I25M1D3M2D6M3I10M1D3M2D17M1D4M1I5M1D5M1I3M2D15M1I5M1I7M1I9M1D2M3I31M1I13M1D6M1I22M2D2I3M1I6M1I10M1I5M1D6M2D10M1I16M1I13M1I3M1I13M1I3M3I2M1I11M1I7M1D4M2I8M1D10M1I16M1I3M1I7M2I3M1I20M1D2M3I5M2I5M1I13M5I3M2I6M1I17M2I3M1I5M1D10M2I11M1D13M1D2M1I8M1D9M1D8M1I3M1I12M1I3M1D3M1D5M1I6M1I8M1I5M1I5M1I10M1I4M1I8M1I3M1I7M1I5M1D8M1D8M1I6M1D10M2I9M1I1M1I9M1D10M1D3M1D4M1D6M1D7M2D5M2I5M2I2M2D2M1I16M1I5M14I4M2D4M1I10M1D6M1I1M1I3M1I14M2I3M1I4M1D4M2I14M1D2M1D6M1D8M2D2M1D22M1D12M1D7M1I13M2D8M1I3M1I15M1I18M1I17M9S * NM:i:446 ms:i:1972 AS:i:1972 nn:i:0 tp:A:S cm:i:25 s1:i:452 dv:f:0.0908m161109_080520_42256_c101052872550000001823247601061737_s1_p0/28/0_4873 2064 kraken:taxid|66084|NC_012416.1 1360016 9 616H6M1I3M1I9M1D12M1I3M1I4M1I1M1I6M3I5M1D10M1D2M1D4M1D4M1I28M2I7M4I3M2I10M1I1M1I4M1D4M6I7M1I6M1I9M1I2M1I13M1I1M1I8M1D4M1I8M1I15M1D3M1I15M1D5M1I1M3D4M1I10M1I8M2I7M1I9M1D4M1I7M1I12M2D11M1I27M1D21M1D5M2I23M5I3M1I9M1I6M3I14M1I6M1D3M1D43M1I2M1D4M1D8M1D21M2I16M1I7M1D5M1D5M1I11M1I21M1D6M1D2M1D7M2I3M1I15M3I2M1I1M2I6M2I4M1I6M1D9M1D62M1I10M1D3M1I11M2D7M1D13M1I4M1I10M1D4M1I8M1D15M1D12M1D10M1D10M1D10M1I12M1D2M2D1M1D14M1I8M1D6M1D11M2D15M1I13M1I14M1I8M1I32M1D4M1I18M1D30M2D5M1D10M1D5M1D11M1D13M1D6M1I7M1I4M1D13M1I1M2D18M2D8M1I8M1I3M1I7M3052H * 0 0 seq * NM:i:195 ms:i:1194 AS:i:1194 nn:i:0 tp:A:P cm:i:21 s1:i:287 s2:i:284 dv:f:0.0502 SA:Z:kraken:taxid|66084|NC_012416.1,1207118,-,2514S2299M51I9S,5,424;m161109_080520_42256_c101052872550000001823247601061737_s1_p0/28/0_4873 272 kraken:taxid|1633785|NZ_CP011148.1 1051218 0 616S6M1I3M1I9M1D12M1I3M1I4M1I1M1I6M3I5M1D10M1D2M1D4M1D4M1I28M2I7M4I3M2I10M1I1M1I4M1D4M6I7M1I6M1I9M1I2M1I13M1I1M1I8M1D4M1I8M1I15M1D3M1I15M1D7M2D4M1I10M1I8M2I7M1I9M1D4M1I7M1I12M2D11M1I27M1D29M1I21M5I8M1I4M1I6M3I14M1I6M1D3M1D43M1I2M1D4M1D8M1D21M2I16M1I7M1D5M1D5M1I11M1I21M1D6M1D2M1D7M2I3M1I15M3I2M1I2M4I9M1I6M1D9M1D62M1I10M1D3M1I11M2D5M1D15M1I4M1I10M1D4M1I8M1D15M1D12M1D10M1D10M1D10M1I12M1D2M2D1M1D14M1I8M1D6M1D11M2D15M1I13M1I14M1I8M1I32M1D4M1I18M1D30M2D5M1D10M1D5M1D11M1D13M1D6M1I7M1I4M1D13M1I1M2D18M2D8M1I8M1I3M1I7M3052S * 0 0 * * NM:i:200 ms:i:1164 AS:i:1164 nn:i:0 tp:A:S cm:i:22 s1:i:284 dv:f:0.0655 This is the results of blastn, and I marked them with taxonomy ID. The results agreed well with that of minimap2. Here is another example. 123$ grep 'm161109_080520_42256_c101052872550000001823247601061737_s1_p0/796/0_3902' minimap2.samm161109_080520_42256_c101052872550000001823247601061737_s1_p0/796/0_3902 16 kraken:taxid|573570|NZ_CP016796.1 218142 29 390S6M1D15M1D10M3I6M1D15M1I3M2I11M3I4M1I14M3I4M1I9M1D7M1D3M2I9M3I20M2I6M1D15M1I3M2I17M3I9M2I4M1I5M1D11M3I9M3I4M4I5M1I10M1I12M1I6M3D10M2D5M2D12M2I11M3177S * 0 0 seq * NM:i:85 ms:i:178 AS:i:178 nn:i:0 tp:A:P cm:i:5 s1:i:93 s2:i:52 dv:f:0.0291 SA:Z:kraken:taxid|573570|NZ_CP016796.1,218159,-,296S287M9I3310S,22,82;m161109_080520_42256_c101052872550000001823247601061737_s1_p0/796/0_3902 2064 kraken:taxid|573570|NZ_CP016796.1 218159 22 296H9M3I6M4I7M1D11M3I3M1I3M1D11M3I8M1I10M3I6M1D11M3I13M1I5M3I6M1D15M1I3M2I11M4D4M5D7M5D11M3I4M1I9M1D7M1D3M2I6M3I23M2I6M1D17M5D4M2D5M2D13M3310H * 0 0 seq * NM:i:82 ms:i:178 AS:i:178 nn:i:0 tp:A:P cm:i:7 s1:i:166 s2:i:120 dv:f:0.0235 SA:Z:kraken:taxid|573570|NZ_CP016796.1,218142,-,390S304M31I3177S,29,85; And, I got different results using blastn. And the detailed alignment of the top 1. But when I checked the raw sequence, there are lots of repeats. I don’t know what it is. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&gt;m161109_080520_42256_c101052872550000001823247601061737_s1_p0/796/0_3902 RQ=0.860CAGAAGATGAGATTTAAATTGCATCCCAATTGTCATATATAATTAATCATATAAATTATATATGTGTAAGTTTTTTTTTTTTGTGAACATTATAGAAATAATAATAATATTCCATGGCCCTGACAAAGGGCCGAGTATTTACTCAATTTGGACAAAGAATAAAAAATTTAAGCCTTCCTTTCAGTTTCTTGGGCAACAGAACATATTTGGTAAAACACCACCTTGTGCAATCGTACACCAAACAGAAGTTTAATTTATTCTTCGTCCACCATCATACGAATTGCAATTGCAAATGTCTATGGAATAATACGAGTTTTCTTGTTGTCCACAACTCTTATGAGCAGCATTACCGCGCCAATTCTAGTACTATCAAGCAGCTAAATATTACCCATGACAGCAGCCAAATCATAAACTGGTGCACCAGCTCCAACACGTTCAGCATAATTTCCTTTGAAAAAATTTAAAAATTTTGAAAAATTCAAAATTTTGAAAAATTCAAAAATTTTGAAAAATTACAACAAATTTGAAAAAAATTCAAAAATTTGAAAAAATTCAATAAAATTTTGAAAAAATTCAAAAAAATTTTAAAAAATCTCAAAAATTTTGAAAATTCATAAATTTTGAAAAAATTCAAAACCTTTTGAAAACAAAATCAAAATTTGAAAAAAATTCAAAAATTTTGAAAAATTCAAAAATTTGAAAAACCATTCAAAAATTTTGAAAAAAAATTCAAAAATTTTGAAAAAATTCAAAAATTTTGAAATAAAAAATTTCAAAAATTTTGAAAAAAATTCAAAAATTTTGAAAAATCTCAAAAAATTTTGAAAAAGTTCAATGAAATTTTGAAAAAGTTTAAAAGATTTTGAAAAAAATACCAAAAATTTTGAAAAAATCAAGAATTTTGAAAAAAATTCCAAAATTTTGAAAAAATTCAATTAAATTTTGAAAAATTCAAAATCTTTGAAAAAAATTCAACAAATTTTGAAAAAATTCACAAAAATTTTGAAAAAATTCAAAAATTTTGAAAAATTCAAAAATTTTTGAAAAAACAAACTCAAAATTTTGAAAAAATTCAAAAATTTGAAAAAATTCAAAAAATTTGAAAAAATTCAAAAACTTTTGAAAAAAATTCAAAAATCTTTGAAAAAATTCAAAAATTTTTGAAAAAATTCAAAAATTTTGAAAAAAATTCAAAAATTTTGAAAAAATTCAAAAATTTTTGAAAAAATCAAAAATTTGAAAAAATTCCAAAAATTTTTGAAAAAATTCAAAAAATTTTCGAAAAATTCAATTTTGAAAAATTCAAAAATTTACTGAAAAAATTTCAAAAATTTTGAAAAAATTCAATAAATTTTGAAAAAATTCAAACATTTTGAAAAATTCAAAAATTTTGAAAAAATCAACAAATTTTTGAAAAAATTTCAAAAATTTTTGAAAAATCAAAAAATTTTTTGAAAAAAATTCACGCCAAATTTTGAAAAAATTTAAAAATTTTGAAAAAATTAAATAAATTTTAAAATTTGTATGATTTTTCAAATTTTTGATAAGTTTTCATTTTGAAAAATTTAAATTTTTTCAAAATTTTTTGAATTTTTTCAAAATTTTGAATTTTTTCACCCAATTTTTGAATTTTCTTTCAAAAAATTTTTAGAAATTAACAAATAACCTATTTCAAATTTTTGAATTTTTCAAATTTTTGATTTTTTCAAAATTTTGAATTTTTTTTCAAAATTTTGAATTTTTAAATACAAATTTTTGAATTTTTCAAAATTGTTTGAATTTTTATCAAAATTTTTGAATTTTTTTAAAAATTTTTGATTTTTTCAAAGGAAATTATTGCTGAAGCCGTGTTGGAGCTGTGCACCAGTTTTATTTGGCTGCCTGTTCATGGAACTTTTAGCTGCTGAGTACTAAGAATTGGCGGGTAATGCTGTCAGTGACAGAACAGAAAACTCGTATTATTTCCTAGACATTTGCAATTGGCAATTCAGTAAATGACGAACCAATTAAATAATACTTCTGTCTGGTGTTTACGAATGCACAAGGTGAGTGTTTTACCAAATAACAAGCTGTTCTGATGCCTCAACGAAAACTGAAAGAAGGCTTAAATTTTTTTATTCCTTTGTCCAAATTAGTAAACTACTTCGGCCCTTTTCAGGGCCATAATATTCATTATCTATTTTCACTCAGATGTTCGCAAAAAAAAAACTTACACATATAATAATTTTATATATTAATTTGCAAATTTGGATGCAATTTAAAATTGAATGATAAAGTGCAATGGTGTTGTCTAGTCTACAAAAATTCTATAAACGTACACAAAAAAAATATTCGCTAAATTGAATTGTTGATAAAAAAAATATTTTTACTTAGAAAATCTTAAAAAAAAAAAACACAATAGATATGATTGTATATAATCAAAAAATGCTATTGAATGTAAATAATTTTTTAACAATTTCAAATTTTTAAAAACGTTCTCGCTTAGTTATATCAAATTATTCGGATTTTGGTTTTTTTATTATTAATTATTATTATATTAATAATAAATTATTATTCTACTCAATTATTAAAATTTGAATAATTTTGTGGCCTCAAAGGGCCATTTGTTTTATAATGACAATTTTATTGAAGGAATACCAAAAGAATTGAAATAAACATACGATTGATATATTAAAGTATTAGCGTGTTTTATTTCTTTCTTTTAGGTGAAAATGCTGCCTTCCTGGGCTTTAGGTGAAGTATGGATGTTTTTGGCCAGTTTTATGGTTTATTTGGTGCCTTTGGTTTTTAGTTTGGACCTTTTAGCTGATTTTTTGGCCTTTAATCGGTGAATTTTGCAGTTTGTAGTCGTACGAATGTAGTTTTAGGCAGCAGATGGTGGTTTTTTCTTCTCGTCTTTTTTTCGGTTTAGCAGCTTTAATTGGTGATTTTTTTTGCTTTGATTGATTTCTTACAGCCGGTTTTATGTTTTAATTGTCCAGTTTGCTAAAGTAGAATTCGTTTTACAGTTGCAAGCTTTTTTGGGTTTTGTACCAGAAGCCCTTCTTTCTTTTTCTTTTAGAACTTTTTTTTCTTAGAACTTTTTTTTTTTAGAACATTTTTTTTTAGGAACTTTTTTTTTAGAACTTTTTTTAGAAACTTTTGTTATCTTTTCTGTTTGTTCTTGTTATCTTTTTGTTTGTTTTGTTATCATTTTTTTGTTTCTTTTGGTTCTCTTTTTGTTTGTTTTTGTTATCATTTTTTGTTTGTTTTTGGGTATCTCTTTTTTTGTTTGTTTTGTTATCTTTTTGTTGTTTGGTTTATCTTTTCCTTTGTTTTGTTTATTTTTTTGTTTGTTTTGGTTATATTTTTGTTTGTTTGTTATCTTTTTTGTTTTTTTTGTTATCTTTTTGTTTGTTTGTATCTTTTTTTGTTTGTTTTGTTATCTTTTTTTGTTTGTTTTGTTATCTTTTTGTTTGTTTTTGTTATCTTTTTGTTTGTTTTGTTTATCTTTATTGTTTGTTTTGTTATCTTTTTGTTTGTTTTGTTATCTTCTTTTGTTTGTTTTGTTATCTTTTTGCTTTGTTTTGTTATCTTTTTGTTTTGTTTTGTTATCTTTTTGTTGTTCTTTATTCTGGTTAATCATTTTTTTGGTTGTTTTGTTCATCTTTTTGTGTTTGTTTTTGTTATCTTTTTGTTTGTTTTGTTAATCTTTTTGTTTGCTTTGTTATCATTTTTTGTTTGCTTTGTTATTTTTTGTTTGCTTTGTTATATTTTTGTTGCTTTGTTATCATTTTTGTTTGCTTTGGTTATCTTTGTTTGCTTTGTTATCTTTTTTTGTTGGCTTTGTTTATCGTTTTTGTATTTGCTTTGTTATCGTTTTTGTTTGCTTTGTTATCTTTTTTGTTTTGCGTTTTTTTAGC The differences between minimap2 and blastn are explainable, they used different algorithm and different databases. And generally, I think minimap2 is reliable. In summary In summary, I removed contaminants in reads by the following steps: Prepare contamination library (bacteria, viral, fungi, protozoa, and archaea from Refseq and mtDNAs). Use BBDuk to remove contaminants from illumina short reads. Use minimap2 to remove contaminants from PacBio long reads. But there are some concerns existing: Some real sequences may also be removed along with the contaminants. Many repeat sequences were removed, and I don’t know where they were from. For the ‘ambivalent’ reads, I kept them for downstream analysis, but I didn’t know whether they should be removed, say, throw away reads which the primary aligment were contaminant. There are some other long-reads mapper, and people also try to tune the parameters of short-read aligners to work with long-reads. There are some threads/posts talking about this: Question: Long read alignment Question: Long read-alignment + variant calling Question: Alternative to BLASR ? Question: BWA-MEM using long PacBio reads Mapping long reads with Bowtie STAR: segmentation fault when using long reads I don’t really understand the “mapping” things now, but I expect that there will be several dominant tools for long-reads mapping, just as short-reads mapping. Useful links Question: Removing contaminations from PacBio reads How can I download RefSeq data for all complete bacterial genomes? Introducing BBSplit: Read Binning Tool for Metagenomes and Contaminated Libraries Yes … BBMap can do that! From raw reads to assembly STEP by STEP Removing contamination with BBDUK Question: Tool to separate human and mouse rna seq reads Change log 20180628: create the note. 20180725: complete the note. 20180807: update the ‘minimap-arm’ part, add the results of kraken. 20180815: add the part of ‘Minimap2-2.12 (r827)’ Chaisson MJ, Tesler G. 2012. Mapping single molecule sequencing reads using basic local alignment with successive refinement (BLASR): application and theory. BMC Bioinformatics. 13:238. doi:10.1186/1471-2105-13-238. ↩︎ Jain C, Dilthey A, Koren S, Aluru S, Phillippy AM. 2018 Apr 30. A Fast Approximate Algorithm for Mapping Long Reads to Large Reference Databases. Journal of Computational Biology. doi:10.1089/cmb.2018.0036. [accessed 2018 Jul 2]. https://www.liebertpub.com/doi/10.1089/cmb.2018.0036. ↩︎ Li H. 2017 Aug 4. Minimap2: versatile pairwise alignment for nucleotide sequences. arXiv:170801492 [q-bio]. [accessed 2018 Jan 10]. http://arxiv.org/abs/1708.01492. ↩︎]]></content>
      <categories>
        <category>reads</category>
        <category>contamination</category>
      </categories>
      <tags>
        <tag>bio-tools</tag>
        <tag>contamination</tag>
        <tag>reads</tag>
        <tag>BBDuk</tag>
        <tag>MashMap</tag>
        <tag>long-reads</tag>
        <tag>long-reads alignment</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Genome Assembly Pipeline: wtdbg]]></title>
    <url>%2Fblog%2F2018%2F06%2FGenome-Assembly-Pipeline-wtdbg%2F</url>
    <content type="text"><![CDATA[Introduction wtdbg has two git repos: wtdbg and wtdbg-1.2.8, and the author Jue Ruan (who also developed SMARTdenovo) introduces them as: wtdbg: A fuzzy Bruijn graph approach to long noisy reads assembly. wtdbg is desiged to assemble huge genomes in very limited time, it requires a PowerPC with multiple-cores and very big RAM (1Tb+). wtdbg can assemble a 100 X human pacbio dataset within one day. wtdbg-1.2.8: Important update of wtdbg Jue Ruan preferred wtdbg-1.2.8. In personal feeling, I like wtdbg-1.2.8 more than SMARTdenovo and wtdbg-1.1.006. This tool hasn’t been published now (20180307), and I found it in an evaluation paper from BIB: Jayakumar V, Sakakibara Y. Comprehensive evaluation of non-hybrid genome assembly tools for third-generation PacBio long-read sequence data. Briefings in Bioinformatics. 2017 Nov 3:bbx147-bbx147. doi:10.1093/bib/bbx147 My feelings: very fast easy to install easy to use docs and discussions about this tool is limited. aggressive good N50 (at least in our two genome projects, an insect and a plant) relatively bad completeness General usage Because wtdbg has two different versions and I didn’t know which one is more suitable for me, I just tried both. wtdbg v1.1.006 Install I got a problem when compile the software. The issue is caused by the CPATH of our OS, and eventually solved with the help of Jue Ruan. 12git clone https://github.com/ruanjue/wtdbg.git &amp;&amp; cd wtdbgmake Examples in the doc 1234567891011121314151617181920# assembly of contigswtdbg-1.1.006 -t 96 -i pb-reads.fa -o dbg -H -k 21 -S 1.02 -e 3 2&gt;&amp;1 | tee log.wtdbg# -t: number of threads, please type 'wtdbg-1.1.006 -h' to get a document# -i: you can set more than one sequences files, such as -i 1.fa. -i 2.fq -i 3.fa.gz -i 4.fq.gz# -o: the prefix of results# -S: 1.01 will use all kmers, 1.02 will use half by sumsampling, 1.04 will use 1/4, and so on# 2.01 will use half by picking minimizers, but not fully tested# -e: if too low coverage(&lt; 30 X), try to set -e 2# please note that dbg.ctg.fa is full of errors from raw reads# first round of polishmentwtdbg-cns -t 96 -i dbg.ctg.lay -o dbg.ctg.lay.fa -k 15 2&gt;&amp;1 | tee log.cns.1# dbg.ctg.lay.fa is the polished contigs# if possible, further polishmentminimap -t 96 -L 100 dbg.ctg.lay.fa pb-reads.fa 2&gt; &gt;(tee log.minimap) | best_minimap_hit.pl | awk '&#123;print $6"\t"$8"\t"$9"\t"$1"\t"$5"\t"$3"\t"$4&#125;' &gt;dbg.mapmap2dbgcns dbg.ctg.lay.fa pb-reads.fa dbg.map &gt;dbg.map.laywtdbg-cns -t 96 -i dbg.map.lay -o dbg.map.lay.fa -k 13 2&gt;&amp;1 | tee log.cns.2# you need to concat all reads into one file for minimap and map2dbgcns# dbg.map.lay.fa is the final contigs wtdbg v1.2.8 Install 12git https://github.com/ruanjue/wtdbg-1.2.8.git &amp;&amp; cd wtdbg-1.2.8make For higher error rate long sequences Decrease -p. Try -p 19 or -p 17 Decrease -S. Try -S 2 or -S 1 Both will increase computing time. For very high coverage Increase --edge-min. Try --edge-min 4, or higher. For low coverage Decrease --edge-min. Try --edge-min 2 --rescue-low-cov-edges. Filter reads --tidy-reads 5000. Will filtered shorter sequences. If names in format of \/\d+_\d+$, will selected the longest subread. Consensus 1wtdbg-cns -t 64 -i dbg.ctg.lay -o dbg.ctg.lay.fa The output file dbg.ctg.lay.fa is ready for further polished by PILON or QUIVER. In practice I’ve tried two versions of wtdbg and diferent parameter combinations in two genome assembly projects. The parameters and the logs/stats received are as follows: An insect The species: high heterogeneity, high AT, high repetition. Genome size: male 790M, female 830M. Data used：about 70X PacBio long-reads. OS environment: CentOS6.6 86_64 glibc-2.12. QSUB grid system. 15 Fat nodes (2TB RAM, 40 CPU) and 10 Blade nodes (156G RAM, 24 CPU). wtdbg v1.1.006 run1, with -H -k 21 -S 1.02 -e 3: stats： 12345678total base: 607971510%GC: 32.21num: 4655min: 2700max: 6594103avg: 130606N50: 573208N90: 46228 wtdbg v1.2.8 run1, with defalult -k 0 -p 21 -S 4: stats: 12345678total base: 757804309%GC: 32.37num: 20960min: 2247max: 3846453avg: 36154N50: 103681N90: 12128 run2, with --edge-min 2 --rescue-low-cov-edges --tidy-reads 5000 (Because median node depth = 6, less than 20) stats: 12345678total base: 845834770%GC: 32.51num: 19555min: 2030max: 2025061avg: 43254N50: 158013N90: 14248 run3, with -k 15 -p 0 -S 1 --rescue-low-cov-edges --tidy-reads 5000 stats: 12345678910Size_includeN 795503989Size_withoutN 795503989Seq_Num 12557Mean_Size 63351Median_Size 15690Longest_Seq 7257493Shortest_Seq 2277GC_Content 32.44N50 308340N90 21383 run4, with -k 0 -p 19 -S 2 --rescue-low-cov-edges --tidy-reads 5000 stats: 12345678910Size_includeN 780618272Size_withoutN 780618272Seq_Num 11722Mean_Size 66594Median_Size 16335Longest_Seq 8184393Shortest_Seq 2547GC_Content 32.4N50 294217N90 23008 run5, with --tidy-reads 5000 -k 21 -p 0 -S 2 --rescue-low-cov-edges stats: 12345678910Size_includeN 843085698Size_withoutN 843085698Seq_Num 26341Mean_Size 32006Median_Size 18982Longest_Seq 491063Shortest_Seq 2992GC_Content 32.51N50 54544N90 13737 run6, with -k 0 -p 21 -S 4 --aln-noskip After discussion with the author, he suggested me to use --aln-noskip. stats: 12345678910Size_includeN 726925732Size_withoutN 726925732Seq_Num 15983Mean_Size 45481Median_Size 12714Longest_Seq 2523944Shortest_Seq 2290GC_Content 32.21N50 164635N90 14464 run7, with -k 15 -p 0 -S 1 --rescue-low-cov-edges --tidy-reads 5000 --aln-noskip stats: 12345678910Size_includeN 762713695Size_withoutN 762713695Seq_Num 9803Mean_Size 77804Median_Size 15366Longest_Seq 11163143Shortest_Seq 2449GC_Content 32.22N50 488952N90 25913 After all the experiments, I’m not sure what to do next (try more or move on). As suggeested by Jue Ruan, N50 contig of ~500kb is good enough for scaffolding and genomic analysis. So I should try to evaluate the assembly and improve it while trying other tools. A plant The species: high heterogeneity, high repetition. Genome size: 2.1G. Data used：more than 100X PacBio long reads. OS environment: CentOS6.6 86_64 glibc-2.12. QSUB grid system. 15 Fat nodes (2TB RAM, 40 CPU) and 10 Blade nodes (156G RAM, 24 CPU). wtdbg v1.1.006 commands: 123# run1, version 1.1.006$TOOLDIR/wtdbg/wtdbg -t $PPN -i $WORKDIR/data/Pacbio/all.fq.gz -o run1 -H -k 21 -S 1.02 -e 3 2&gt;&amp;1 | tee log.run1$TOOLDIR/wtdbg/wtdbg-cns -t $PPN -i run1.ctg.lay -o run1.ctg.lay.fa -k 15 2&gt;&amp;1 | tee log.run1.cns.1 stats: 123456789101112Size_includeN 2105945650Size_withoutN 2105945650Seq_Num 21871Mean_Size 96289Median_Size 48435Longest_Seq 2968570Shortest_Seq 2531GC_Content 38.27N50 194480L50 2523N90 40454Gap 0.0 wtdbg v1.2.8 commands: 123# run2, version 1.2.8$TOOLDIR/wtdbg-1.2.8/wtdbg-1.2.8 -t $PPN -i $WORKDIR/data/Pacbio/all.fq.gz -o run2$TOOLDIR/wtdbg-1.2.8/wtdbg-cns -t $PPN -i run2.ctg.lay -o run2.ctg.lay.fa stats: 123456789101112Size_includeN 1924031835Size_withoutN 1924031835Seq_Num 37933Mean_Size 50721Median_Size 14836Longest_Seq 2424157Shortest_Seq 2006GC_Content 38.75N50 184177L50 2391N90 17404Gap 0.0 Where to go next? I asked Jue Ruan that if it is necessary to run consensus tools on the results of wtdbg or smartdenovo, he said: The inside consensus tool wtdbg-cns aims to provide a quick way to reduce sequencing errors. It is suggested to use Quiver and/or Pilon to polish the consensus sequences after you feel happy with the assembly. Usually, wtdbg-cns can reduce error rate down to less than 1%, which can be well-aligned by short reads. Useful links Discussions about “Optimisation of parameters” if it is necessary to run consensus tools on the results of wtdbg or smartdenovo Change log 20180307: create the note. 20180630: add the ‘A plant’ part.]]></content>
      <categories>
        <category>genome assembly</category>
        <category>TGS pipeline</category>
      </categories>
      <tags>
        <tag>bio-tools</tag>
        <tag>genome assembly</tag>
        <tag>TGS genome assembly</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Detect Microbial Contamination in Contigs by Kraken]]></title>
    <url>%2Fblog%2F2018%2F06%2FDetect-Microbial-Contamination-in-Contigs-by-Kraken%2F</url>
    <content type="text"><![CDATA[Purpose in short: I want to detect (and remove) potential contaminants in the genome assembly, and Kraken is a tool designed for that. Introduction From its webpage: Kraken is a system for assigning taxonomic labels to short DNA sequences, usually obtained through metagenomic studies. Previous attempts by other bioinformatics software to accomplish this task have often used sequence alignment or machine learning techniques that were quite slow, leading to the development of less sensitive but much faster abundance estimation programs. Kraken aims to achieve high sensitivity and high speed by utilizing exact alignments of k-mers and a novel classification algorithm. In its fastest mode of operation, for a simulated metagenome of 100 bp reads, Kraken processed over 4 million reads per minute on a single core, over 900 times faster than Megablast and over 11 times faster than the abundance estimation program MetaPhlAn. Kraken’s accuracy is comparable with Megablast, with slightly lower sensitivity and very high precision. Kraken is written in C++ and Perl, and is designed for use with the Linux operating system. We have also successfully compiled and run it under the Mac OS. In practice See Kraken manual for full instructions. Install Download the latest release. 123$ unzip kraken-1.1.zip &amp;&amp; cd kraken-1.1$ ./install_kraken.sh $TOOLDIR/kraken.1.1 If you want to build your own dabase, jellyfish version 1 should be in your PATH. Build standard Kraken database The standard Kraken database includes bacterial, archeal, and viral genomes in Refseq at the time of the build. 1234567#!/bin/shpath2kraken=$TOOLDIR/kraken.1.1DBNAME=Kraken_DB$path2kraken/kraken-build --standard --threads 16 --db $DBNAME If any step (including the initial downloads) fails, the build process will abort. However, kraken-build will produce checkpoints throughout the installation process, and will restart the build at the last incomplete step if you attempt to run the same command again on a partially-built database. Build custom Kraken database Because the standard Kraken database doesen’t include fungi and protozoa, which I want to include in my analysis. I found refseq2kraken, which facilitates the downloading and preparation of Kraken database. 123456789101112131415161718192021222324252627282930path2kraken=$TOOLDIR/kraken.1.1DBNAME=Kraken_DB_201806# install refseq2krakengit clone https://github.com/sschmeier/refseq2kraken.git refseq2krakencd refseq2kraken# Download refseq =&gt; here only "Complete Genome"# assemblies, e.g. the defaultpython getRefseqGenomic.py -p 8# convert to kraken format =&gt; again only "Complete Genome"# assemblies here, e.g. the defaultpython getKrakenFna.py -p 8 $DBNAME# build a new minikraken database# download taxonomykraken-build --download-taxonomy --db $DBNAME# for each branch, add all fna in the directory to the databasefor dir in bacteria viral archaea fungi protozoa; do find $DBNAME/$dir/ -name '*.fna' -print0 | xargs -0 -I&#123;&#125; -n1 -P8 kraken-build --add-to-library &#123;&#125; --db $DBNAME;done# build the actual databasekraken-build --build --db $DBNAME# remove intermediate fileskraken-build --clean --db $DBNAME In the following part of this note, this library will be refered as to ‘Non-Masked library’. This post Download refseq-genomic data and prepare it for Kraken is also helpful. Mask low-complexity regions I noticed a pull request of kraken: Fixed human genome downloading and added auto-masking feature using dustmasker, and realized that I should mask the library! So I re-generated the kraken library (I don’t know why the author of kraken didn’t mention that in their docs.) Here is the new script: 1234567891011121314151617181920212223242526272829303132#!/bin/shpath2kraken=$TOOLDIR/kraken.1.1path2refseq2kraken=$TOOLDIR/refseq2krakenpath2dustmasker=$TOOLDIR/ncbi-blast-2.7.1+/bin/dustmaskerDBNAME=Kraken_DB_abfpv_1806# Download refseq =&gt; here only "Complete Genome"python $path2refseq2kraken/getRefseqGenomic.py -p 8# convert to kraken format =&gt; again only "Complete Genome"python $path2refseq2kraken/getKrakenFna.py -p 8 $DBNAME# build a new minikraken database$path2kraken/kraken-build --download-taxonomy --db $DBNAME# for each branch# filter with Dustmasker and convert low complexity regions to N's with Sed (skipping headers)# and add all fna to the databasefor i in `find $DBNAME \( -name '*.fna' -o -name '*.ffn' \)`do $path2dustmasker -in $i -infmt fasta -outfmt fasta | sed -e '/&gt;/!s/a\|c\|g\|t/N/g' &gt; tempfile $path2kraken/kraken-build --add-to-library tempfile --db $DBNAMEdone# qsub jobs below to the computer nodes# build the actual database$path2kraken/kraken-build --build --threads 20 --db $DBNAME# remove intermediate files$path2kraken/kraken-build --clean --db $DBNAME In the following part, this library will be refered as to ‘Masked library’. Classify contigs Non-Masked library 12345# classify$path2kraken/kraken --db $Kraken_DB_201806 --threads $PPN --fasta-input $WROKDIR/flye/run2/contigs.fasta --classified-out $WORKDIR/kraken/run2/flye.run2.classified --unclassified-out $WORKDIR/kraken/run2/flye.run2.unclassified &gt; $WORKDIR/kraken/run2/flye.run2.kraken# report$path2kraken/kraken-report --db $Kraken_DB_201806 $WORKDIR/kraken/run2/flye.run2.kraken &gt; $WORKDIR/kraken/run2/flye.run2.report The ‘classified’ sequences will be saved in ‘classified.fa’, and the ‘unclassified.fa’ will be the ‘clean’ one, which can be used for downstream analysis. But when I looked over the ‘report’, the result quite disappointed me. 123456$ head -5 flye.run2.report 31.30 4647 4647 U 0 unclassified 68.70 10199 435 - 1 root 51.55 7653 166 - 131567 cellular organisms 47.44 7043 471 D 2 Bacteria 24.67 3662 19 P 1224 Proteobacteria This means about 70% contigs were contaminated! I was curious about the percentage of contaminant lengths of each read. Then I used a simple Python script to count that. Then I used R to visualize the relationship between contig lengths and the contaminated lengths. There were 14846 contigs, among which 12004 (80.86%) contigs, 2788 (18.78%) contigs and 54 (0.36%) contigs has a contaminated rate of blow 1%, between 1% and 10%, and more than 10% respectively. Masked library 12345# classify$path2kraken/kraken --db Kraken_DB_abfpv_1806 --threads $PPN --fasta-input $WROKDIR/flye/flye/run2/contigs.fasta --classified-out $WROKDIR/flye/run3/flye.run2.classified --unclassified-out $WROKDIR/flye/run3/flye.run2.unclassified &gt; $WROKDIR/flye/run3/flye.run2.kraken# report$path2kraken/kraken-report --db Kraken_DB_abfpv_1806 $WROKDIR/flye/run3/flye.run2.kraken &gt; $WROKDIR/flye/run3/flye.run2.report And about 17% of contigs were classified as ‘contaminant’. 123456$ head -5 flye.run2.report 83.49 12395 12395 U 0 unclassified 16.51 2451 15 - 1 root 13.53 2009 0 D 10239 Viruses 13.46 1999 0 - 35237 dsDNA viruses, no RNA stage 13.44 1996 0 F 10482 Polydnaviridae There were 14846 contigs, among which 14766 (99.46%) contigs, 61 (0.41%) contigs and 19 (0.13%) contigs has a contaminated rate of blow 1%, between 1% and 10%, and more than 10% respectively. Replace contaminant regions with N My collaborator asked me to give him an assembly draft for meta-genomic study, which a ‘clean’ one was needed. Removing all the contaminated sequences was not feasible, since there would be few contigs left. Then I checked the output of Kraken, which contained the detailed information of each contig: 1234$ head -3 flye.run2.krakenC contig_1102 118110 395511 0:71251 28874:1 1:73 35237:1 1241371:1 1:1 694430:4 131567:16 1:15 0:376 158:2 0:1 1:33 2:1 1783272:1 1769:1 0:5975 633697:1 1783272:1 273035:1 1:19 1307:1 0:18402 2:1 1:39 0:14 85655:2 0:152 78219:2 1:18 0:8591 679926:1 131567:2 1:12 1118964:3 0:2539 523841:1 1:31 0:3050 1:27 0:3370 1:26 1267001:1 0:1341 523841:1 1:5 131567:1 1783272:1 2100:2 0:75636 1:38 0:2 864702:2 0:6975 1428:4 131567:1 1:114 1783272:1 203124:1 0:10293 1353243:1 0:1 1:32 2:1 1313292:1 0:48537 1783272:1 273035:1 1:11 2:1 1313292:1 0:10283 1161:1 1507806:1 1:91 131567:1 0:611 1:13 131567:16 694430:4 1:1 1241371:1 35237:1 1:49 35237:3 10401:3 0:628 1605721:1 0:8 1:6 0:20 1605721:8 1:25 0:15 1:83 0:16 1605721:1 0:3 1605721:8 1:47 10239:2 12315:8 0:9 1605721:8 1:100 0:15 1:47 0:15 1:53 0:7 1:22 0:1 1:53 0:7 1:36 35237:1 0:3604 523841:1 1:51 2:2 444612:1 0:667 1654582:4 1:3 118110:9 0:3597 46170:1 1783272:1 1:7 0:25607 118110:7 131567:2 2:1 1898474:2 1783272:2 1:22 2:3 0:732 273035:2 1:35 0:16896 1:23 273035:1 0:5012 28890:2 2207:2 1:30 28890:1 2209:3 0:11916 118110:6 1:3 118110:1 1:13 320432:1 0:13148 39152:3 1:15 0:669 1105113:1 320432:1 1:29 1654582:1 0:6006 10371:3 2:1 1:41 0:5940 85655:2 0:6 864702:2 0:2 1:34 0:14 85655:2 0:301 1307:2 1783272:1 1:2 35237:1 1:2 1654582:1 0:5603 1428:1 1:23 0:4788 456320:1 1:23 0:169 2:2 131567:1 1:10 131567:1 1783272:1 0:1735 10335:1 1:28 118110:1 1:3 118110:4 0:11831 118110:3 131567:2 2:1 1898474:2 1783272:2 1:19 0:1542 118110:3 1:3 118110:1 1:19 216946:1 0:452 444612:1 2:2 1:15 0:1 66266:1 0:1790 118110:8 131567:2 2:1 1898474:2 1783272:2 1:11 0:1 66266:1 0:319 375175:2 0:328 1:17 131567:1 1769:1 0:2798U contig_11021 0 4721 0:4691C contig_11022 118110 43806 0:12538 1:19 131567:1 444612:1 0:5305 118110:9 131567:2 2:1 1898474:1 0:149 1:17 39640:1 0:25732 Because Kraken uses a k-mer mapping strategy to locate potential contaminants, there would be many contigs within which only a small part could be aligned to bacteria/virul sequences. So I retrieved the precise k-mers that mapped to contamination, and masked them with ‘N’. I discussed this with another guy: Questions about de novo genome assembly from mixed DNA samples, and he also thought it’s a vivid approach. Finally But I began to think how the contaminanted sequences would affect the assembly process, and maybe removing contaminants from raw data is a good way. See discussions here: Filtering for contamination when assembling a genome, before or after assemby? and Question: How to remove contamination from the transcriptome assembly. Useful links refseq2kraken: download refseq-genomic data and prepare it for Kraken Download refseq-genomic data and prepare it for Kraken Building a low-complexity masked database with dustmasker Questions about de novo genome assembly from mixed DNA samples Handling microbial contamination in NGS data Question: Contamination in assembly Filtering for contamination when assembling a genome, before or after assemby? Question: How to remove contamination from the transcriptome assembly Change log 20180424: create the note. 20180620: add the “refseq2kraken” part. 20180808: add “Mask low-complexity regions” and respective parts.]]></content>
      <categories>
        <category>genome assembly</category>
        <category>contamination</category>
      </categories>
      <tags>
        <tag>bio-tools</tag>
        <tag>genome assembly</tag>
        <tag>contamination</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Viral Expression in RNA-seq data]]></title>
    <url>%2Fblog%2F2018%2F05%2FViral-Expression-in-RNA-seq-data%2F</url>
    <content type="text"><![CDATA[Recently I wanted to check viral expression from RNA-seq data. I found two good examples: Cao S, Strong MJ, Wang X, Moss WN, Concha M, Lin Z, O’Grady T, Baddoo M, Fewell C, Renne R, et al. 2015. High-Throughput RNA Sequencing-Based Virome Analysis of 50 Lymphoma Cell Lines from the Cancer Cell Line Encyclopedia Project. J. Virol. 89:713–729. doi:10.1128/JVI.02570-14. Wang Zheng, Hao Y, Zhang C, Wang Zhiliang, Liu X, Li G, Sun L, Liang J, Luo J, Zhou D, et al. 2017. The Landscape of Viral Expression Reveals Clinically Relevant Viruses with Potential Capability of Promoting Malignancy in Lower-Grade Glioma. Clinical Cancer Research 23:2177–2185. Also some useful discussions: using STAR to map against 100 viral species slow mapping to a small genome Alex (the author of STAR) suggested to combine human genome and viruses. But I already mapped the FASTQ to human genome (hg38), and saved unmapped reads to seperated FASTQ files. Step 1, download all virul genomes from NCBI Refseq Viral Release. 123456$ wget ftp://ftp.ncbi.nih.gov/refseq/release/viral/viral.1.1.genomic.fna.gz$ wget ftp://ftp.ncbi.nih.gov/refseq/release/viral/viral.2.1.genomic.fna.gz$ gzip -d *$ cat viral.1.1.genomic.fna viral.2.1.genomic.fna &gt; viral.refseq.180424.fa Step 2, build STAR index. 123$ mkdir STARgenomes$ /software/STAR-2.5.3a/bin/Linux_x86_64_static/STAR --runThreadN 10 --genomeDir ./STARgenomes --runMode genomeGenerate --genomeFastaFiles viral.refseq.180424.fa Step 3, align unmapped reads to viral genomes. 123456for sample in x1 x2 ...do $TOOLDIR/STAR-2.5.3a/bin/Linux_x86_64_static/STAR --runMode alignReads --runThreadN 24 --genomeDir $STARindex --outSAMtype BAM SortedByCoordinate --outSAMattributes All --readFilesIn $WORKDIR/STAR_out/$&#123;sample&#125;_Unmapped.out.mate1.gz $WORKDIR/STAR_out/$&#123;sample&#125;_Unmapped.out.mate2.gz --readFilesCommand zcat --outFileNamePrefix $WORKDIR/Viral_expression/$&#123;sample&#125;_ $TOOLDIR/samtools.1.3.1/bin/samtools index $WORKDIR/Viral_expression/$&#123;sample&#125;_Aligned.sortedByCoord.out.bamdone Step 4, compute viral expression. I wanted to use existing read-counting software to quantify the viruses, so I had to create a fake annotation (a fake GTF file). The tiny script to create GTF from FASTA file was like this: 123456789101112131415161718192021222324252627#!/usr/bin/evn python'''purpose: to calculate read count of virus, I want to make a fake GTF of virus fasta.usage: python xxx.py virus.fa &gt; virus.gtf'''import sys# step 1: read the fasta and put it into a dictseq_dict = &#123;&#125;with open(sys.argv[1], 'r') as fin: for line in fin: line = line.strip() if line.startswith('&gt;'): seqID = line.split()[0].replace('&gt;', '') seqName = ' '.join(line.split(',')[0].split()[1:]) seq_dict[seqID] = [seqName, ''] else: seq_dict[seqID][1] += line# step2: traverse the dict and generate GTF. Use the whole virus as a exon.for key in seq_dict: seqLen = len(seq_dict[key][1]) tmp_list = [key, 'Virus', 'exon', '1', str(seqLen), '.', '+', '.'] print '\t'.join(tmp_list) + '\t' + 'gene_id "' + key + '"; gene_name "' + seq_dict[key][0] + '";' And the output looked like this: 1234$ head -3 viral.refseq.180424.fake.gtfNC_003747.2 Virus exon 1 4212 . + . gene_id "NC_003747.2"; gene_name "Ryegrass mottle virus isolate MAFF. No. 307043 from Japan";NC_011500.2 Virus exon 1 1614 . + . gene_id "NC_011500.2"; gene_name "Rotavirus A segment 5";NC_007737.1 Virus exon 1 3055 . + . gene_id "NC_007737.1"; gene_name "Liao ning virus segment 2"; Then I used featureCounts function from Rsubread R package to count the reads of viruses (non-strand specific, 'cause not knowing the transcription direction), and used rpkm function of edgeR to normalize the raw count to viral “FPKM”. Note: The expression is a estimation. There maybe lots of errors. Be careful to interpret the results. Change notes 20180424: create the note.]]></content>
      <categories>
        <category>RNA-seq</category>
        <category>viral expression</category>
      </categories>
      <tags>
        <tag>RNA-seq</tag>
        <tag>virus</tag>
        <tag>viral expression</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Genome Assembly Pipeline: SMARTdenovo]]></title>
    <url>%2Fblog%2F2018%2F04%2FGenome-Assembly-Pipeline-SMARTdenovo%2F</url>
    <content type="text"><![CDATA[Introduction From its Git Repo: SMARTdenovo is a de novo assembler for PacBio and Oxford Nanopore (ONT) data. It produces an assembly from all-vs-all raw read alignments without an error correction stage. It also provides tools to generate accurate consensus sequences, though a platform dependent consensus polish tools (e.g. Quiver for PacBio or Nanopolish for ONT) are still required for higher accuracy. SMARTdenovo consists of several separate command line tools: wtzmo for read overlapping, wtgbo to rescue missing overlaps, wtclp for identifying low-quality regions and chimaera, and wtcns or wtmsa to produce better unitig consensus. The smartdenovo.pl script provides a convenient interface to call these programs in one go. This tool has not been published yet. (20180313) My feelings: easy to install/use not as fast as wtdbg, but fast comparatively good results (at least in my case) docs and discussions about this tool is limited. General usage 12345678# Download sample PacBio from the PBcR websitewget -O- http://www.cbcb.umd.edu/software/PBcR/data/selfSampleData.tar.gz | tar zxf -awk &apos;NR%4==1||NR%4==2&apos; selfSampleData/pacbio_filtered.fastq | sed &apos;s/^@/&gt;/g&apos; &gt; reads.fa# Install SMARTdenovogit clone https://github.com/ruanjue/smartdenovo.git &amp;&amp; (cd smartdenovo; make)# Assemble (raw unitigs in wtasm.lay.utg; consensus unitigs: wtasm.cns)smartdenovo/smartdenovo.pl -c 1 reads.fa &gt; wtasm.makmake -f wtasm.mak In practice An insect The species: high heterogeneity, high AT, high repetition. Genome size: male 790M, female 830M. commands: 123# run1, default$path2perl $TOOLDIR/smartdenovo/smartdenovo.pl -t $PPN -c 1 -p run1 $DATADIR/third/third_all.fasta &gt; run1.makmake -f run1.mak stats: 1234567891011Size_includeN 756816708Size_withoutN 756816708Seq_Num 6135Mean_Size 123360Median_Size 55901Longest_Seq 5704487Shortest_Seq 10769GC_Content 31.72N50 240010N90 44546Gap 0.0 SMARTdenovo can also use zmo overlapper. I also test this option, but it generated about 17G genome! (The estimated genome size is about 850M.) A plant The species: high heterogeneity, high repetition. Genome size: 2.1G. run1, with about 100X data commands: 123# run1, default$path2perl $TOOLDIR/smartdenovo/smartdenovo.pl -t 24 -c 1 -p run1 $WORKDIR/data/Pacbio/all.fq.gz &gt; run1.makmake -f run1.mak And the stats I got: 123456789101112Size_includeN 2103140368Size_withoutN 2103140368Seq_Num 6164Mean_Size 341197Median_Size 163362Longest_Seq 9288681Shortest_Seq 12171GC_Content 38.16N50 703465L50 809N90 151138Gap 0.0 run2, with about 50X data commands: 123# run2, 50X$path2perl $TOOLDIR/smartdenovo/smartdenovo.pl -t $PPN -c 1 -p run2 $WORKDIR/data/Pacbio/Pacbio_50x.fasta &gt; run2.makmake -f run2.mak And the stats I got: 123456789101112Size_includeN 2028605527Size_withoutN 2028605527Seq_Num 5811Mean_Size 349097Median_Size 170070Longest_Seq 10046321Shortest_Seq 24367GC_Content 38.18N50 708215L50 758N90 147345Gap 0.0 This was a very good N50 size! And the assembled size was close to the expected one. Change notes 20180423: create the note.]]></content>
      <categories>
        <category>genome assembly</category>
        <category>TGS pipeline</category>
      </categories>
      <tags>
        <tag>bio-tools</tag>
        <tag>genome assembly</tag>
        <tag>long-read genome assembly</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Identify circRNAs and Fusions from RNA-seq Using STARChip]]></title>
    <url>%2Fblog%2F2018%2F04%2FIdentify-circRNAs-and-Fusions-from-RNA-seq-Using-STARChip%2F</url>
    <content type="text"><![CDATA[I want to find out some circRNAs from RNA-seq data (total RNA-seq, not poly-A enriched). There are many tools for this mission. Here is a good review paper [1] talking about computational methods for analyzing circRNAs, both identification and downstream analysis. Also another review paper about identifying circRNAs [2]. There are also two evaluation papers for the identification tools [3][4]. From all the tools I know, CIRCexplorer2 [5] and CIRI [6] are well matained. But I want to try something new: STARChip [7]. STARChip is short for Star Chimeric Post, written by Dr. Nicholas Kipp Akers as part of his work in Bojan Losic’s group at the Icahn Institute of Genomics and Multiscale Biology at Mount Sinai School of Medicine This software is designed to take the chimeric output from the STAR alignment tool and discover high confidence fusions and circular RNA in the data. Before running, you must have used a recent version of STAR with chimeric output turned on, to align your RNA-Seq data. So, it can identify fusions and circRNAs at the same time. According to its paper, for circRNA detection, “STARChip achieves the best precision of all tools tested and nearly the best sensitivity. This does not appear to come at an increased resource cost. Both CIRI and CIRCexplorer had competitive precision and sensitivity values; STARChip required 43 and 179% of the runtimes of these packages, respectively, and ∼72% of the memory requirements.”; for fusions, “With STARChip, we have attempted to emphasize precision at the expense of sensitivity in these particular gold-standard studies, reasoning that such hyper-tuning inflates type I error in mining novel datasets.” I’ve discussed with the author Kipp Akers about the precision: https://github.com/LosicLab/starchip/issues/9#issuecomment-381181507. He said: To your final question, my goal with STARChip was to develop a tool that focused on precision. There are a dozen fusion finders out there that sacrifice everything to get the highest sensitivity. For my projects, this was not too helpful. However, STARChip’s read requirement settings can be set manually and because it runs so quickly, it’s easy to play with the settings to turn up sensitivity and turn down precision and see what you get. Feel free to do so, and let me know what you find! I agree with the designing purpose of STARChip, so I decide to give it a shot. There are two main modules in STARChip: starchip-fusions is for fusion detection. It runs on individual samples. /path/to/starchip/starchip-fusions.pl output_seed Chimeric.out.junction Paramters.txt starchip-circles is for circRNA detection. It runs on groups of samples. /path/to/starchip/starchip-circles.pl STARdirs.txt Parameters.txt /path/to/starchip/starchip-circles.pl fastq_files.txt parameters.txt Notes below are more for my own convenience. See its git repo for full usage. prepare STARChip is written to be an extension of the STAR read aligner. It is optional for STARChip to run STAR on your samples. In most instances to run STARChip you must first run star on each of your samples. See the STAR documentation for installation, as well as building or downloading a STAR genome index. It is absolutely critical however, that you follow the STAR manual’s instructions and build a genome using all chromosomes plus unplaced contigs. Not doing so will strongly inflate your false positives rate, because reads that map perfectly to an unplaced contig will instead find the next best alignment, often a chimeric alignment. Run STAR with the following parameters required for chimeric output: –chimSegmentMin X –chimJunctionOverhangMin X (where X is an integer). Your project will have it’s own requirements, but a good starting point for your star alignments might look like: STAR --genomeDir /path/to/starIndex/ --readFilesIn file1_1.fastq.gz file1_2.fastq.gz --runThreadN 11 --outReadsUnmapped Fastx --quantMode GeneCounts --chimSegmentMin 15 --chimJunctionOverhangMin 15 --outSAMstrandField intronMotif --readFilesCommand zcat --outSAMtype BAM Unsorted reference/BED files STARChip makes use of gtf files for annotating fusions and circRNA with gene names. First, download the package and prepare annotation files: 123$ git clone https://github.com/LosicLab/starchip.git &amp;&amp; cd starchip$ mkdir starchip_ref &amp;&amp; ./setup.sh ~/RefData/Homo_sapiens/GENCODE_v27/gencode.v27.annotation.gtf ~/RefData/Homo_sapiens/GRCh38_no_alt/genome.fa ./starchip_ref additional files for Fusions starchip-fusions filters using the location of known repeats in bed format as well. Following the instructions in the picture to download repeats from UCSC genome browser. Go to http://genome.ucsc.edu/cgi-bin/hgTables Change ‘genome’ to your desired genome Change the following settings: group: Repeat track: RepeatMasker region: genome output format: BED output file: some reasonable name.bed Click ‘get output’ to download your bed file. On your local machine sort the bed file: sort -k1,1 -k2,2n repeats.bed &gt; repeats.sorted.bed If you’re working on hg19 or hg38, you don’t have to do the following things. The files needed are already included in the directory of STARChip. starchip-fusions can also make use of known antibody parts, and copy number variants. These files come with starchip for human hg19 and hg38 in the reference directory. For other species you can create your own in the simple format: Chromosome StartPosition EndPosition Finally, starchip-fusions uses known gene families and known/common false-positive pairs to filter out fusions which are likely mapping errors or PCR artifacts. Family data can be downloaded from ensembl biomart: Go to http://www.ensembl.org/biomart/martview Database: Ensembl Genes Dataset: Your species Click Attributes on the left hand side. Under GENE dropdown, select only “Gene Name” Under PROTEIN FAMILIES AND DOMAINS dropdown select Ensembl Protein Family ID. Click Results at the top. Export the file. It should have two columns, Family ID and Gene ID. Known false positives are stored within data/pseudogenes.txt. In practice, we’ve found that pseudogenes and tissue specific highly expressed genes are commonly “fused” via PCR template switching errors. Feel free to put add any additional lines that result from your data to this file in the format: Gene1Name Gene2Name run STARChip Since my previous run of STAR didn’t use parameters --chimSegmentMin and --chimJunctionOverhangMin, I have to start with Fastq files. starchip-circles can run from Fastq files, but starchip-fusions starts from Chimeric.out.junction. I’ll first run starchip-circles then run starchip-fusions. First of all, I prepare dirs for STARChip under my WORKDIR like this: 123456STARChip/├── STARChip-circRNA│ ├── starchip-circles.fastqfiles # the fastq files│ └── starchip-circles.params # starchip-circles parameters└── STARChip-fusions └── starchip-fusions.param # starchip-fusions parameters run starchip-circles The parameter file and Fastq file: 12345678910111213141516171819202122232425262728293031$ cat starchip-circles.params ##Parameters for starchimp-circlesreadsCutoff = 5minSubjectLimit = 10cpus = 20do_splice = TruecpmCutoff = 0subjectCPMcutoff = 0 annotate = true#Reference Filesrefbed = /software/starchip/starchip_ref/gencode.v27.annotation.gtf.bed #use setup.sh to create this from a gtf. refFasta = /RefData/Homo_sapiens/GRCh38_no_alt/genome.fa#STAR Parameters## Do you use a prefix for your STAR output?starprefix = ## Are you starting from fastq and need to run STAR alignment? runSTAR = TrueSTARgenome = /RefData/Homo_sapiens/GRCh38_no_alt/STARgenomes #not necassary if runSTAR != TrueSTARreadcommand = zcat #cat for fastq, zcat for fastq.gz etc. not necassary if runSTAR != TrueIDstepsback = 1 ## this is the position from the right of your path of the name of your files. ##for example: /path/to/sample1/star/2.4.2/output/Chimeric.out.junction ##sample1 is 4 steps back. ##or /path/to/star/2.4.2/sample1/Chimeric.out.junction #sample1 is 1 step back.$ head -3 starchip-circles.fastqfiles XJ-3-1-25_R1.fastq.gz XJ-3-1-25_R2.fastq.gzXJ-2-1-25_R1.fastq.gz XJ-2-1-25_R2.fastq.gzXJ-7-1-25_R1.fastq.gz XJ-7-1-25_R2.fastq.gz Then go into the $WORKDIR/STARChip/STARChip-circRNA and run starchip-circles to generate scripts: 123456$ $path2circles starchip-circles.fastqfiles starchip-circles.paramsUsing the following parameters: Circular RNA must have at least 5 reads in at least 10 subjects/output files. Using 20 CPUs.Rscript must be callable. Requiring 0 subjects/outputs with 0 Counts per million circular reads to count a given circular RNAOther requirements are bedtools (&gt;= 2.24.0)You have indicated you would like STARChip to perform STAR alignments. starchip-circles.fastqfiles should contain a list of fastq files; 1 sample per line, multiple files separated by a comma, and paired end files separated by a space.STARChip run scripts generated, please run ./Step1.sh through Step4.sh to detect and quantify circRNA There will be four scripts: Step1.sh: align Step2.sh: discover circRNA Step3.sh: re-align Step4.sh: quantify/annotate Step2.sh and Step3.sh use STAR in the system PATH, but I want to use another one: 12sed -i '3,$s|^|/software/STAR-2.5.3a/bin/Linux_x86_64_static/|g' Step1.shsed -i '3,$s|^|/software/STAR-2.5.3a/bin/Linux_x86_64_static/|g' Step3.sh I’m working on a PBS grid system, then I create a script to submit these scripts: 12345678910111213141516171819#!/bin/bash#PBS -V#PBS -j eo#PBS -N STARChip-circles#PBS -q Blade#PBS -l nodes=1:ppn=20echo Start time is `date +%Y/%m/%d--%H:%M`# work dirWORKDIR=/STARChip/STARChip-circRNA# starchip-circlessh Step1.shsh Step2.shsh Step3.shsh Step4.shecho Finish time is `date +%Y/%m/%d--%H:%M` In my samples, only four circRNAs were identified by STARChip-circles. 123456$ cat circRNA.5reads.10ind.countmatrix8_4-10_R1.fastq.gz 8_4-11_R1.fastq.gz 8_4-3_R1.fastq.gz 8_4-4_R1.fastq.gz 8_4-5_R1.fastq.gz 8_4-6_R1.fastq.gz 8_4-7_R1.fastq.gz 8_4-8_R1.fastq.gz 8_4-9_R1.fastq.gz XJ-10-1-25_R1.fastq.gz XJ-11-1-25_R1.fastq.gz XJ-1-1-25_R1.fastq.gz XJ-12-1-25_R1.fastq.gz XJ-13-1-25_R1.fastq.gz XJ-2-1-25_R1.fastq.gz XJ-3-1-25_R1.fastq.gXJ-4-1-25_R1.fastq.gz XJ-5-1-25_R1.fastq.gz XJ-6-1-25_R1.fastq.gz XJ-7-1-25_R1.fastq.gz XJ-8-1-25_R1.fastq.gz XJ-9-1-25_R1.fastq.gzchr1:117402186-117442325 0 0 0 2 0 0 4 0 6 3 8 0 0 4 3 2 2 2 0 0 3chr15:101213315-101216678 0 0 38 20 8 0 0 0 0 0 0 26 0 0 58 60 18 12 25 23 0chr15:90217439-90219891 0 4 0 1 0 0 0 0 1 0 1 0 5 2 0 1 0 2 0 1 9 4chr9:111786793-111787947 0 0 33 186 23 0 0 0 0 1 0 10 0 0 5 29 42 14 15 15 0 run starchip-fusions The parameter file: 12345678910111213141516171819202122232425262728293031323334353637### Parameters for fusions-from-star.pl## Describing Your Data:pairedend = TRUE #TRUE means paired end data. any other value means single end. $spancutoff should be 0 if data is single end.consensus = TRUE # anything but TRUE will make this skip the consensus sequence generation for each fusion.## Filters, Cutoffs splitReads = auto # number of minimum read support at jxn. Minimum 2. Greatly impacts running time. Other options are: "auto" , "highsensitivity" , and "highprecision" uniqueReads = 2 # number of unique read support values (higher indicates more likely to be real. lower is more likely amplification artifact).spancutoff = 1 #minimum number of non-split reads support. If single end data, this must be 0 or auto. Other options are: "auto" , "highsensitivity" , and "highprecision"wiggle = 500 #number of base-pairs of 'wiggle-room' when determining the location of a fusion (for spanning read counts)overlapLimit = 5 #wiggle room for joining very closely called fusion sites.samechrom_wiggle = 20000 #this is the distance that fusions have to be from each other if on the same chromosome. Set to 0 if you want no filtering of same-chromosome prlopsidedupper = 10 # (topsidereads + 0.1) / (bottomsidereads + 0.1) must be below this value. set very high to disable. Reccomended setting 5lopsidedlower = 0.1 # (topsidereads + 0.1) / (bottomsidereads + 0.1) must be above this value. set to 0 to disable. Reccomended setting 0.2cnvwiggle = 1000 #we skip fusions that can be explained by known cnvs. how close to the edges of the cnv must our fusion be?circlesize = 100000 #we skip fusions that look more like circular rna/backsplices. how big (bp) could a circle be? ## Local Reference Files: #refbed is a bed format version of a gtf. This should probably be derived from the same GTF that STAR aligned with using setup.sh. refbed=/software/starchip/starchip_ref/gencode.v27.annotation.gtf.bed # a bed format list of known repeatsrepeatbed=/RefData/RepeatMasker/hg38.ucsc.180414.rmsk.sorted.bed # fasta reference. should be indexed (run 'samtools faidx file.fa')refFasta = /RefData/Homo_sapiens/GRCh38_no_alt/genome.faabparts = reference/hg38.abpartscnvs = reference/conrad_hg38.cnvsfamilyfile = reference/ensfams.txtfalsepositives = reference/knownFP.txt #Scoring Parameters (feel free to tweak).splitscoremod = 10spanscoremod = 20skewpenalty = 4repeatpenalty = 0.5 # score = score*(repeatpenalty^repeats) --&gt; a fusion can have 0,1,or 2 sites fall into repeat regions. Based on the output of previous STAR running for starchip-circles, the script to run starchip-fusions contains: 12345678910111213141516171819202122#!/bin/bash#PBS -V#PBS -j eo#PBS -N STARChip-fusions#PBS -q Blade#PBS -l nodes=1:ppn=2echo Start time is `date +%Y/%m/%d--%H:%M`# work dirWORKDIR=/STARChip# starchip-fusionsTOOLDIR=/softwarepath2fusions=$TOOLDIR/starchip/starchip-fusions.plfor sample in for sample in XJ-3-1-25 XJ-2-1-25 XJ-7-1-25 XJ-4-1-25 XJ-5-1-25 XJ-6-1-25 XJ-1-1-25 XJ-10-1-25 XJ-9-1-25 XJ-13-1-25 XJ-11-1-25 XJ-12-1-25 XJ-8-1-25 8_4-3 8_4-4 8_4-5 8_4-6 8_4-7 8_4-8 8_4-9 8_4-10 8_4-11do /usr/bin/perl $path2fusions $sample $WORKDIR/STARChip-circRNA/STARout/$&#123;sample&#125;_R1.fastq.gz/Chimeric.out.junction starchip-fusions.paramdoneecho Finish time is `date +%Y/%m/%d--%H:%M` In my samples, no fusions were found by STARChip-fusions, and I don’t want to tweak parameters to improve sensitivity. Change notes 20180413: create the note. Gao Y, Zhao F. 2018 Jan 12. Computational Strategies for Exploring Circular RNAs. Trends in Genetics. doi:10.1016/j.tig.2017.12.016. [accessed 2018 Jan 15]. https://www.sciencedirect.com/science/article/pii/S0168952517302366. ↩︎ Szabo L, Salzman J. 2016. Detecting circular RNAs: bioinformatic and experimental challenges. Nat Rev Genet 17:679–692. doi:10.1038/nrg.2016.114. ↩︎ Hansen TB, Ven? MT, Damgaard CK, Kjems J. 2016. Comparison of circular RNA prediction tools. Nucleic Acids Research 44:e58–e58. doi:10.1093/nar/gkv1458. ↩︎ Zeng X, Lin W, Guo M, Zou Q. 2017. A comprehensive overview and evaluation of circular RNA detection tools. PLOS Computational Biology 13:e1005420. doi:10.1371/journal.pcbi.1005420. ↩︎ Zhang X-O, Dong R, Zhang Y, Zhang J-L, Luo Z, Zhang J, Chen L-L, Yang L. 2016. Diverse alternative back-splicing and alternative splicing landscape of circular RNAs. Genome Res. 26:1277–1287. doi:10.1101/gr.202895.115. ↩︎ Gao Y, Wang J, Zhao F. 2015. CIRI: an efficient and unbiased algorithm for de novo circular RNA identification. Genome Biology 16:4. doi:10.1186/s13059-014-0571-3. ↩︎ Akers NK, Schadt EE, Losic B. 2018 Feb 20. STAR Chimeric Post for rapid detection of circular RNA and fusion transcripts. Bioinformatics:bty091–bty091. doi:10.1093/bioinformatics/bty091. ↩︎]]></content>
      <categories>
        <category>RNA-seq</category>
        <category>circRNA</category>
      </categories>
      <tags>
        <tag>bio-tools</tag>
        <tag>RNA-seq</tag>
        <tag>circRNA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[How to Install Perl Modules]]></title>
    <url>%2Fblog%2F2018%2F04%2FHow-to-install-perl-modules%2F</url>
    <content type="text"><![CDATA[Installing perl modules can be a troublesome work, especially when you are not a ROOT user. After a lot of “pain”, I decide to document the two ways to install perl modules (it’s not my creation, just for a memo). Check installed First, check if the module has been installed: 1234567891011121314# system perl$ which perl/usr/bin/perl$ perl -e 'use DBD::Oracle; print $DBD::Oracle::VERSION;'Can't locate DBD/Oracle.pm in @INC (@INC contains: /home/niuyw/software/perl.5.24.0/lib/site_perl/5.24.0/x86_64-linux /home/software/lib64/perl5 /home/software/share/perl5/ /home/software/vcftools-0.1.15/src/perl /home/niuyw/bin/perl_lib/share/perl5 /home/software/lib64/perl5 /home/software/share/perl5/ /home/software/vcftools-0.1.15/src/perl /home/niuyw/bin/perl_lib/share/perl5 /usr/local/lib64/perl5 /usr/local/share/perl5 /usr/lib64/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib64/perl5 /usr/share/perl5 .) at -e line 1.# perl installed under my own dirctory$ ~/software/perl.5.24.0/bin/perl -e 'use DBD::Oracle; print $DBD::Oracle::VERSION;'Can't locate DBD/Oracle.pm in @INC (you may need to install the DBD::Oracle module) (@INC contains: /home/niuyw/software/perl.5.24.0/lib/site_perl/5.24.0/x86_64-linux /home/software/lib64/perl5 /home/software/share/perl5/ /home/software/vcftools-0.1.15/src/perl /home/niuyw/bin/perl_lib/share/perl5 /home/software/lib64/perl5 /home/software/share/perl5/ /home/software/vcftools-0.1.15/src/perl /home/niuyw/bin/perl_lib/share/perl5 /home/niuyw/software/perl.5.24.0/lib/site_perl/5.24.0/x86_64-linux /home/niuyw/software/perl.5.24.0/lib/site_perl/5.24.0 /home/niuyw/software/perl.5.24.0/lib/5.24.0/x86_64-linux /home/niuyw/software/perl.5.24.0/lib/5.24.0 .) at -e line 1.BEGIN failed--compilation aborted at -e line 1.# if the module has been installed$ ~/software/perl.5.24.0/bin/perl -e 'use URI::Escape; print $URI::Escape::VERSION;\n'3.31 Scenario 1: you are a ROOT user OR use your own perl In this case, the installation is simple. Use cpan -i module_name 1$ ~/software/perl.5.24.0/bin/cpan -i Net::Server Use perl -MCPAN -e shell 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748$ ~/software/perl.5.24.0/bin/perl -MCPAN -e shellTerminal does not support AddHistory.cpan shell -- CPAN exploration and modules installation (v2.16)Enter 'h' for help.cpan[1]&gt; hDisplay Information (ver 2.16) command argument description a,b,d,m WORD or /REGEXP/ about authors, bundles, distributions, modules i WORD or /REGEXP/ about any of the above ls AUTHOR or GLOB about files in the author's directory (with WORD being a module, bundle or author name or a distribution name of the form AUTHOR/DISTRIBUTION)Download, Test, Make, Install... get download clean make clean make make (implies get) look open subshell in dist directory test make test (implies make) readme display these README files install make install (implies test) perldoc display POD documentationUpgrade installed modules r WORDs or /REGEXP/ or NONE report updates for some/matching/all upgrade WORDs or /REGEXP/ or NONE upgrade some/matching/all modulesPragmas force CMD try hard to do command fforce CMD try harder notest CMD skip testingOther h,? display this menu ! perl-code eval a perl command o conf [opt] set and query options q quit the cpan shell reload cpan load CPAN.pm again reload index load newer indices autobundle Snapshot recent latest CPAN uploads# search modules using keywordcpan[2]&gt; i /scws/Distribution XUERON/Text-Scws-0.01.tar.gzModule &lt; Text::Scws (XUERON/Text-Scws-0.01.tar.gz)2 items found# install modulescpan[3]&gt; install Net::ServerNet::Server is up to date (2.009).# quitcpan[5]&gt; q/quit/exit Scenario 2: you are a common user but want to use the system perl This process can be tedious, especially if the modules you want to install depend on other modules. Sometimes, even we have installed own perl, but still need to use the perl under /usr/bin/perl. For example, many scripts told me This Perl not built to support threads. First, create a directory for perl modules. 1mkdir -p /home/niuyw/bin/perl_lib Second, download a module and install it from source code locally. Go CPAN to search and download the modules you want to install. 1234567891011121314151617181920$ tar zxf Capture-Tiny-0.46.tar.gz &amp;&amp; cd Capture-Tiny-0.46$ which perl/usr/bin/perl# specify the path to install$ perl Makefile.PL PREFIX=/home/niuyw/bin/perl_libChecking if your kit is complete...Looks goodGenerating a Unix-style MakefileWriting Makefile for Capture::TinyWriting MYMETA.yml and MYMETA.json$ make &amp;&amp; make installcp lib/Capture/Tiny.pm blib/lib/Capture/Tiny.pmManifying 1 pod documentManifying 1 pod documentAppending installation info to /home/niuyw/bin/perl_lib/lib64/perl5/perllocal.pod$ perl -e 'use Capture::Tiny; print $Capture::Tiny::VERSION;'0.46 Third, add the path above to your .bashrc. Notice the format. 1export PERL5LIB=$PERL5LIB:/home/niuyw/bin/perl_lib/share/perl5:/home/niuyw/bin/perl_lib/lib64/perl5 Thanks Quan Kang for teaching me how to install perl modules from source code. Change notes 20180413: create the note. 20180414: change the setting of PERL5LIB.]]></content>
      <categories>
        <category>mixture</category>
      </categories>
      <tags>
        <tag>perl</tag>
        <tag>install</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Genome Assembly Pipeline: Flye]]></title>
    <url>%2Fblog%2F2018%2F03%2FGenome-assembly-pipeline-Flye%2F</url>
    <content type="text"><![CDATA[Intro From its git repo: Flye is a de novo assembler for long and noisy reads, such as those produced by PacBio and Oxford Nanopore Technologies. The algorithm uses an A-Bruijn graph to find the overlaps between reads and does not require them to be error-corrected. After the initial assembly, Flye performs an extra repeat classification and analysis step to improve the structural accuracy of the resulting sequence. The package also includes a polisher module, which produces the final assembly of high nucleotide-level quality. This tool is now on biRxiv: Kolmogorov M, Yuan J, Lin Y, Pevzner P. Assembly of Long Error-Prone Reads Using Repeat Graphs. bioRxiv. 2018 Jan 12:247148. doi:10.1101/247148 My feelings: easy to use comparatively good results, good N50, good completeness not too many parameters to be tested General usage 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# installgit clone https://github.com/fenderglass/Flye Flye-2.3.3cd Flye-2.3.3python setup.py build# usage$ ./bin/flye -husage: flye (--pacbio-raw | --pacbio-corr | --nano-raw | --nano-corr | --subassemblies) file1 [file_2 ...] --genome-size size --out-dir dir_path [--threads int] [--iterations int] [--min-overlap int] [--resume] [--debug] [--version] [--help]Assembly of long and error-prone readsoptional arguments: -h, --help show this help message and exit --pacbio-raw path [path ...] PacBio raw reads --pacbio-corr path [path ...] PacBio corrected reads --nano-raw path [path ...] ONT raw reads --nano-corr path [path ...] ONT corrected reads --subassemblies path [path ...] high-quality contig-like input -g size, --genome-size size estimated genome size (for example, 5m or 2.6g) -o path, --out-dir path Output directory -t int, --threads int number of parallel threads (default: 1) -i int, --iterations int number of polishing iterations (default: 1) -m int, --min-overlap int minimum overlap between reads (default: auto) --resume resume from the last completed stage --resume-from stage_name resume from a custom stage --debug enable debug output -v, --version show program&apos;s version number and exitInput reads could be in FASTA or FASTQ format, uncompressedor compressed with gz. Currenlty, raw and corrected readsfrom PacBio and ONT are supported. Additionally, --subassembliesoption does a consensus assembly of high-quality input contigs.You may specify multiple fles with reads (separated by spaces).Mixing different read types is not yet supported.You must provide an estimate of the genome size as input,which is used for solid k-mers selection. The estimate couldbe rough (e.g. withing 0.5x-2x range) and does not affectthe other assembly stages. Standard size modificators aresupported (e.g. 5m or 2.6g)# E. coli P6-C4 PacBio datawget https://zenodo.org/record/1172816/files/E.coli_PacBio_40x.fastaflye --pacbio-raw E.coli_PacBio_40x.fasta --out-dir out_pacbio --genome-size 5m --threads 4# E. coli Oxford Nanopore Technologies datawget https://zenodo.org/record/1172816/files/Loman_E.coli_MAP006-1_2D_50x.fastaflye --nano-raw Loman_E.coli_MAP006-1_2D_50x.fasta --out-dir out_nano --genome-size 5m --threads 4 See Flye manual for full usage. In practice An insect The species: high heterogeneity, high AT, high repetition. Genome size: male 790M, female 830M. Data used：about 70X PacBio long-reads. OS environment: CentOS6.6 86_64 glibc-2.12. QSUB grid system. 15 Fat nodes (2TB RAM, 40 CPU) and 10 Blade nodes (156G RAM, 24 CPU). version 2.3.2-gd46edb7 Flye version: 2.3.2-gd46edb7 I didn’t test all the parameters. Below is the results based on default settings. command: 1flye --pacbio-raw $DATADIR/third/third_all.fasta --out-dir run1 --genome-size 850m --threads 24 stats: 12345678910111213141516171819202122232425# contigSize_includeN 724744485Size_withoutN 724744485Seq_Num 17602Mean_Size 41173Median_Size 17572Longest_Seq 1648999Shortest_Seq 55GC_Content 31.55N50 91066N90 17456Gap 0.0# scaffoldSize_includeN 724753785Size_withoutN 724744485Seq_Num 17509Mean_Size 41393Median_Size 17532Longest_Seq 1648999Shortest_Seq 55GC_Content 31.55N50 92367N90 17530Gap 0.0 Version 2.3.3-g47cdd0b Flye 2.3.3 have two updates appealing to me: Automatic selection of minimum overlap parameter based on read length Minimap2 updated Because I’ve run Canu before, and Flye can start from raw data and corrected data, I’ll test Flye for both. From raw data Commands: 1$TOOLDIR/Flye-2.3.3/bin/flye --pacbio-raw third_all.fasta --out-dir run2 --genome-size 830m --threads 40 Stats: 123456789101112131415161718192021222324252627# contigsSize_includeN 787629166Size_withoutN 787629166Seq_Num 14846Mean_Size 53053Median_Size 20542Longest_Seq 1636300Shortest_Seq 12GC_Content 31.6N50 121564L50 1699N90 25419Gap 0.0# scaffoldsSize_includeN 787632766Size_withoutN 787629166Seq_Num 14810Mean_Size 53182Median_Size 20465Longest_Seq 1692734Shortest_Seq 12GC_Content 31.6N50 122313L50 1680N90 25437Gap 0.0 From corrected data from Canu (about 33X) Commands: 1$TOOLDIR/Flye-2.3.3/bin/flye --pacbio-corr canu.correctedReads.fasta.gz --out-dir run3 --genome-size 830m --threads 40 Stats: 123456789101112131415161718192021222324252627# contigsSize_includeN 833065987Size_withoutN 833065987Seq_Num 17536Mean_Size 47506Median_Size 26593Longest_Seq 1145994Shortest_Seq 518GC_Content 31.47N50 88680L50 2594N90 22129Gap 0.0# scaffoldsSize_includeN 833070387Size_withoutN 833065987Seq_Num 17492Mean_Size 47625Median_Size 26602Longest_Seq 1145994Shortest_Seq 518GC_Content 31.47N50 89165L50 2581N90 22242Gap 0.0 A plant The species: high heterogeneity, high repetition. Genome size: 2.1G. Data used：more than 100X PacBio long reads. OS environment: CentOS6.6 86_64 glibc-2.12. QSUB grid system. 15 Fat nodes (2TB RAM, 40 CPU) and 10 Blade nodes (156G RAM, 24 CPU). run1, more than 100X data commands: 1$path2flye --pacbio-raw $WORKDIR/data/Pacbio/all.fasta --out-dir run1 --genome-size 2g --threads 30 But I came across a memory issue: ERROR: Caught unhandled exception: std::bad_alloc in both 2.3.2 and 2.3.3. And the author suggested me to downsample the data. And I asked him that what’s the difference: using all raw data (say 100X) and using downsampling data (say longest 50X)? He said “You might have extra connectivity information in these 100x reads (you can resolve more repeats, for example). But some studies suggest (Canu paper, for example) that you don’t really need more than 40x in general (but it, of course, also depends on the genome complexity, ploidy etc…). Plus, extra coverage helps to get a good final consensus.” run2, with about 50X data I used SelectLongestReads to downsample about 50X data and ran Flye again. 1234567# run1.1, extract 50X dataSelectLongestReads sum 100000000000 longest 1 o Pacbio_50x.fasta f all.fasta# remove @ from the fastareplace_@_in_fasta_header.py Pacbio_50x.fasta &gt; Pacbio_50x_no_@.fasta$path2flye --pacbio-raw Pacbio_50x_no_@.fasta --out-dir run1.1 --genome-size 2g --threads 30 --resume The reason why I removed the @ from the headers was because I encountered another problem: ERROR: parse error in 1-consensus/consensus.fasta on line 1: empty sequence. It seemed that Flye would ignore these headers. And the stats I got: 123456789101112131415161718192021222324252627# contigsSize_includeN 1640872256Size_withoutN 1640872256Seq_Num 9843Mean_Size 166704Median_Size 72841Longest_Seq 8808184Shortest_Seq 139GC_Content 37.78N50 398108L50 1119N90 83615Gap 0.0# scaffoldsSize_includeN 1640874656Size_withoutN 1640872256Seq_Num 9819Mean_Size 167112Median_Size 72812Longest_Seq 8808184Shortest_Seq 139GC_Content 37.78N50 399898L50 1114N90 83809Gap 0.0 Not bad. run3, with corrected data from Canu (about 37X) The Canu version was 1.7. commands: 12# run2, pacbio corrected by canu, defalut$path2flye --pacbio-corr canu.correctedReads.fasta.gz --out-dir run2 --genome-size 2g --threads 30 stats: 123456789101112131415161718192021222324252627# contigsSize_includeN 1886541389Size_withoutN 1886541389Seq_Num 13177Mean_Size 143169Median_Size 70083Longest_Seq 2690265Shortest_Seq 110GC_Content 38.01N50 310885L50 1669N90 70697Gap 0.0# scaffoldsSize_includeN 1886554189Size_withoutN 1886541389Seq_Num 13049Mean_Size 144574Median_Size 70202Longest_Seq 2690265Shortest_Seq 110GC_Content 38.01N50 315687L50 1648N90 71378Gap 0.0 Useful links Flye manual Change log 20180314: create the note. 20180428: test version 2.3.3, and run from corrected reads of Canu 20180630: add the part of ‘A plant’.]]></content>
      <categories>
        <category>genome assembly</category>
        <category>TGS pipeline</category>
      </categories>
      <tags>
        <tag>bio-tools</tag>
        <tag>genome assembly pipeline</tag>
        <tag>TGS genome assembly</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Genome Assembly Pipeline: miniasm & Racon]]></title>
    <url>%2Fblog%2F2018%2F03%2FGenome-assembly-pipeline-miniasm-Racon%2F</url>
    <content type="text"><![CDATA[miniasm + Racon is a long-read de novo genome assembly pipeline. miniasm + Racon assembly pipeline There are two good examples: Assembly using miniasm+racon Genome Assembly – minimap/miniasm/racon Overview and a paper based on miniasm, actually, it is a consensus tool called Racon [1]. The miniasm + Racon pipeline consists of the following steps: using minimap/minimap2 [2] for fast all-vs-all overlap of raw reads (Minimap for overlap detection, Overlap) using miniasm, this “simply concatenates pieces of read sequences to generate the final sequences. Thus the per-base error rate is similar to the raw input reads.” (Miniasm layout for generating raw contigs, Layout) mapping the raw reads back to the assembly using minimap again (Minimap for mapping of raw reads to raw contigs, Consensus) using racon (‘rapid consensus’) for consensus calling (Racon for generating high-quality consensus sequences, Consensus) Compared with general pipelines, it achieves ‘similar or better quliaty’ while ‘being an order of magnitude faster’. As described in the miniasm paper [3], published long-read assembly pipelines all include four stages: all-vs-all raw read mapping raw read error correction assembly of error corrected reads (may involve all-vs-all read mapping again, but as the error rate is much reduced at this step, it is easier and faster than stage 1) contig consensus polish Intro minimap Minimap is an experimental tool to efficiently find multiple approximate mapping positions between two sets of long sequences, such as between reads and reference genomes, between genomes and between long noisy reads. By default, it is tuned to have high sensitivity to 2kb matches around 20% divergence but with low specificity. Minimap does not generate alignments as of now and because of this, it is usually tens of times faster than mainstream aligners. With four CPU cores, minimap can map 1.6Gbp PacBio reads to human in 2.5 minutes, 1Gbp PacBio E. coli reads to pre-indexed 9.6Gbp bacterial genomes in 3 minutes, to pre-indexed &gt;100Gbp nt database in ~1 hour (of which ~20 minutes are spent on loading index from the network filesystem; peak RAM: 10GB), map 2800 bacteria to themselves in 1 hour, and map 1Gbp E. coli reads against themselves in a couple of minutes. Minimap does not replace mainstream aligners, but it can be useful when you want to quickly identify long approximate matches at moderate divergence among a huge collection of sequences. For this task, it is much faster than most existing tools. minimap2 Minimap2 is a versatile sequence alignment program that aligns DNA or mRNA sequences against a large reference database. Typical use cases include: (1) mapping PacBio or Oxford Nanopore genomic reads to the human genome; (2) finding overlaps between long reads with error rate up to ~15%; (3) splice-aware alignment of PacBio Iso-Seq or Nanopore cDNA or Direct RNA reads against a reference genome; (4) aligning Illumina single- or paired-end reads; (5) assembly-to-assembly alignment; (6) full-genome alignment between two closely related species with divergence below ~15%. For ~10kb noisy reads sequences, minimap2 is tens of times faster than mainstream long-read mappers such as BLASR, BWA-MEM, NGMLR and GMAP. It is more accurate on simulated long reads and produces biologically meaningful alignment ready for downstream analyses. For &gt;100bp Illumina short reads, minimap2 is three times as fast as BWA-MEM and Bowtie2, and as accurate on simulated data. Detailed evaluations are available from the minimap2 preprint. miniasm miniasm was developed by Heng Li. From its git repo: Miniasm is a very fast OLC-based de novo assembler for noisy long reads. It takes all-vs-all read self-mappings (typically by minimap) as input and outputs an assembly graph in the GFA format. Different from mainstream assemblers, miniasm does not have a consensus step. It simply concatenates pieces of read sequences to generate the final unitig sequences. Thus the per-base error rate is similar to the raw input reads. So far miniasm is in early development stage. It has only been tested on a dozen of PacBio and Oxford Nanopore (ONT) bacterial data sets. Including the mapping step, it takes about 3 minutes to assemble a bacterial genome. Under the default setting, miniasm assembles 9 out of 12 PacBio datasets and 3 out of 4 ONT datasets into a single contig. The 12 PacBio data sets are PacBio E. coli sample, ERS473430, ERS544009, ERS554120, ERS605484, ERS617393, ERS646601, ERS659581, ERS670327, ERS685285, ERS743109 and a deprecated PacBio E. coli data set. ONT data are acquired from the Loman Lab. Miniasm confirms that at least for high-coverage bacterial genomes, it is possible to generate long contigs from raw PacBio or ONT reads without error correction. It also shows that minimap can be used as a read overlapper, even though it is probably not as sensitive as the more sophisticated overlapers such as MHAP and DALIGNER. Coupled with long-read error correctors and consensus tools, miniasm may also be useful to produce high-quality assemblies. Algorithm Overview Crude read selection. For each read, find the longest contiguous region covered by three good mappings. Get an approximate estimate of read coverage. Fine read selection. Use the coverage information to find the good regions again but with more stringent thresholds. Discard contained reads. Generate a string graph. Prune tips, drop weak overlaps and collapse short bubbles. These procedures are similar to those implemented in short-read assemblers. Merge unambiguous overlaps to produce unitig sequences. Limitations Consensus base quality is similar to input reads (may be fixed with a consensus tool). Only tested on a dozen of high-coverage PacBio/ONT data sets (more testing needed). Prone to collapse repeats or segmental duplications longer than input reads (hard to fix without error correction). Since miniasm is not a stand-alone genome assembly tool, it depends on minimap or minimap2. minimap had been archived by the author, and minimap2 now is the successor. But minimap is also worth a try. In this note I only used minimap or minimap2 as a read overlapper for assembly. Go see the docs of minimap2 for full instructions. Racon Racon is a consensus module for raw de novo DNA assembly of long uncorrected reads. Racon is intended as a standalone consensus module to correct raw contigs generated by rapid assembly methods which do not include a consensus step. The goal of Racon is to generate genomic consensus which is of similar or better quality compared to the output generated by assembly methods which employ both error correction and consensus steps, while providing a speedup of several times compared to those methods. It supports data produced by both Pacific Biosciences and Oxford Nanopore Technologies. Racon can be used as a polishing tool after the assembly with either Illumina data or data produced by third generation of sequencing. The type of data inputed is automatically detected. Racon takes as input only three files: contigs in FASTA/FASTQ format, reads in FASTA/FASTQ format and overlaps/alignments between the reads and the contigs in MHAP/PAF/SAM format. Output is a set of polished contigs in FASTA format printed to stdout. All input files can be compressed with gzip. Racon can also be used as a read error-correction tool. In this scenario, the MHAP/PAF/SAM file needs to contain pairwise overlaps between reads with dual overlaps. A wrapper script is also available to enable easier usage to the end-user for large datasets. It has the same interface as racon but adds two additional features from the outside. Sequences can be subsampled to decrease the total execution time (accuracy might be lower) while target sequences can be split into smaller chunks and run sequentially to decrease memory consumption. Both features can be run at the same time as well. My feelings (about miniasm): very fast comparatively good results The docs are not good enough huge memory consumption (may not suitable for large genome) bugs (at least for miniasm) General usage 1234567891011121314151617181920212223242526272829303132# Install minimap and miniasm (requiring gcc and zlib)git clone https://github.com/lh3/minimap &amp;&amp; (cd minimap &amp;&amp; make)git clone https://github.com/lh3/minimap2 &amp;&amp; (cd minimap2 &amp;&amp; make)git clone https://github.com/lh3/miniasm &amp;&amp; (cd miniasm &amp;&amp; make)# Install Racon (requiring gcc 4.8+ or clang 3.4+, and cmake 3.2+)git clone --recursive https://github.com/isovic/racon.git raconcd raconmkdir buildcd buildcmake -DCMAKE_BUILD_TYPE=Release ..make# All-vs-all PacBio read Overlap with minimapminimap/minimap -Sw5 -L100 -m0 -t 8 reads.fq reads.fq | gzip -1 &gt; reads.paf.gz# or minimap2minimap2/minimap2 -x ava-pb -t 8 reads.fq reads.fq | gzip -1 &gt; reads.paf.gz# Layoutminiasm/miniasm -f reads.fq reads.paf.gz &gt; reads.gfa# Consensus## GFA to fastaawk &apos;$1 ~/S/ &#123;print &quot;&gt;&quot;$2&quot;\n&quot;$3&#125;&apos; reads.gfa &gt; reads.fasta## Correction 1minimap/minimap2 -t 8 reads.fasta reads.fq &gt; reads.gfa1.pafracon -t 8 reads.fq reads.gfa1.paf reads.fasta reads.racon1.fasta## Correction 2 (optional)minimap/minimap2 -t 8 reads.racon1.fasta reads.fq &gt; reads.gfa2.pafracon -t 8 reads.fq reads.gfa2.paf reads.racon1.fasta reads.racon2.fasta In practice OS environment: CentOS6.6 86_64 glibc-2.12. QSUB grid system. 15 Fat nodes (2TB RAM, 40 CPU) and 10 Blade nodes (156G RAM, 24 CPU). minimap2 version: 2.8-r672 miniasm version: 0.2-r168-dirty Racon version: 0.5.0 run 1, with about 50X data commands: 1234567# run1, 171224 third data$TOOLDIR/minimap2-2.8_x64-linux/minimap2 -t $PPN -x ava-pb $DATADIR/171224.fasta $DATADIR/171224.fasta | gzip -1 &gt; reads.paf.gz/home/zhangll/software/minimap/miniasm/miniasm -f $DATADIR/171224.fasta reads.paf.gz &gt; reads.gfa# raconawk '$1 ~/S/ &#123;print "&gt;"$2"\n"$3&#125;' reads.gfa &gt; reads.fasta$TOOLDIR/minimap2-2.8_x64-linux/minimap2 -t $PPN reads.fasta $DATADIR/171224.fasta | /home/zhangll/software/racon/bin/racon -t $PPN $DATADIR/171224.fastq - reads.gfa racon1.fasta stats: 123456789101112Size_includeN 1475310871Size_withoutN 1475310871Seq_Num 20186Mean_Size 73085Median_Size 52525Longest_Seq 1183822Shortest_Seq 689GC_Content 31.66N50 97955L50 4346N90 33406Gap 0.0 run2, with about 70X data Didn’t run Racon. commands: 123# run2, all thid data$TOOLDIR/minimap2-2.8_x64-linux/minimap2 -t $PPN -x ava-pb $DATADIR/third_all.fasta $DATADIR/third_all.fasta | gzip -1 &gt; reads.paf.gz/home/zhangll/software/minimap/miniasm/miniasm -f $DATADIR/third_all.fasta reads.paf.gz &gt; reads.gfa stats: 123456789101112Size_includeN 1592482491Size_withoutN 1592482491Seq_Num 23120Mean_Size 68879Median_Size 49376Longest_Seq 1002424Shortest_Seq 689GC_Content 33.1N50 92250L50 5029N90 31666Gap 0.0 The assembling size were larger than the estimated genome size (~850M) in both runs. But this pipeline is very fast. Vaser R, Sovic I, Nagarajan N, Sikic M. Fast and accurate de novo genome assembly from long uncorrected reads. Genome Research. 2017 Jan 18:gr.214270.116. doi:10.1101/gr.214270.116 ↩︎ Li H. Minimap2: versatile pairwise alignment for nucleotide sequences. arXiv:1708.01492 [q-bio]. 2017 Aug 4 [accessed 2018 Jan 10]. http://arxiv.org/abs/1708.01492 ↩︎ Li H. Minimap and miniasm: fast mapping and de novo assembly for noisy long sequences. Bioinformatics. 2016;32(14):2103–2110. doi:10.1093/bioinformatics/btw152 ↩︎]]></content>
      <categories>
        <category>genome assembly</category>
        <category>TGS pipeline</category>
      </categories>
      <tags>
        <tag>bio-tools</tag>
        <tag>genome assembly pipeline</tag>
        <tag>TGS genome assembly</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Genome Assembly Pipeline: OPERA-LG]]></title>
    <url>%2Fblog%2F2018%2F03%2FGenome-assembly-pipeline-OPERA-LG%2F</url>
    <content type="text"><![CDATA[Genome assembly pipeline: OPERA-LG tags: bio-tools, genome assembly pipeline, hybrid genome assembly, scaffloding category: genome assembly, hyrid pipeline Intro From The OPERA wiki OPERA (Optimal Paired-End Read Assembler) is a sequence assembly program (http://en.wikipedia.org/wiki/Sequence_assembly). It uses information from paired-end/mate-pair/long reads to order and orient the intermediate contigs/scaffolds assembled in a genome assembly project, in a process known as Scaffolding. OPERA is based on an exact algorithm that is guaranteed to minimize the discordance of scaffolds with the information provided by the paired-end/mate-pair/long reads (for further details see Gao et al, 2011). Note that since the original publication, we have made significant changes to OPERA (v1.0 onwards) including refinements to its basic algorithm (to reduce local errors, improve efficiency etc.) and incorporated features that are important for scaffolding large genomes (multi-library support, better repeat-handling etc.), in addition to other scalability and usability improvements (bam and gzip support, smaller memory footprint). We therefore encourage you to download and use our latest version: OPERA-LG. In our benchmarks, it has significantly improved corrected N50 and reduced the number of scaffolding errors. Furthermore, our latest release contains the wrapper script OPERA-long-read that enables scaffolding with long-reads from third-generation sequencing technologies (PacBio or Oxford Nanopore). The manuscript describing the new features and algorithms is available at Genome Biology. We look forward to getting your feedback to improve it further. Its paper Gao S, Bertrand D, Chia BKH, Nagarajan N. OPERA-LG: efficient and exact scaffolding of large, repeat-rich eukaryotic genomes with performance guarantees. Genome Biology. 2016;17:102. doi:10.1186/s13059-016-0951-y My feelings: too many dependencies not so easy to use have bugs support re-scaffolding can’t use NGS reads and long-reads simultaneously. In practice See The OPERA wiki for full docs. Scripts used: 1234567891011121314151617181920212223242526272829303132#!/bin/sh#PBS -N OPERA-LG#PBS -j eo#PBS -q Test#PBS -l nodes=1:ppn=8#PBS -d /DenovoSeq/OPERA-LG#PBS -Vecho Start time is `date +%Y/%m/%d--%H:%M`# scaffold with short reads## preprocess reads#for sample in 270B 500B 800B 3k_1 5k-1 5k-2 10k; do#perl /software/OPERA-LG_v2.0.6/bin/preprocess_reads.pl --contig /DenovoSeq/MEGAHIT/megahit_out.no270/final.contigs.fa --illumina-read1 /DenovoSeq/trimmomatic/$&#123;sample&#125;_R_1P.fastq --illumina-read2 /DenovoSeq/trimmomatic/$&#123;sample&#125;_R_2P.fastq --out $&#123;sample&#125;.map --tool-dir /software/bwa-0.7.15 --samtools-dir /software/samtools-0.1.19#done## with all libraries/software/OPERA-LG_v2.0.6/bin/OPERA-LG /DenovoSeq/MEGAHIT/megahit_out.no270/final.contigs.fa 270B.map,500B.map,800B.map,3k_1.map,5k-1.map,5k-2.map,10k.map ./opera /software/samtools-0.1.19## without 270 library/software/OPERA-LG_v2.0.6/bin/OPERA-LG /DenovoSeq/MEGAHIT/megahit_out.no270/final.contigs.fa 500B.map,800B.map,3k_1.map,5k-1.map,5k-2.map,10k.map ./opera.no270 /software/samtools-0.1.19# This is the first run of OPERA-LG, with 270 library, and megahit's contigsperl /software/OPERA-LG_v2.0.6/bin/OPERA-long-read.pl --contig-file /DenovoSeq/MEGAHIT/megahit_out/final.contigs.fa --illumina-read1 10k_R1.fasta --illumina-read2 10k_R2.fasta --long-read-file av_20k.fasta --output-prefix 10k.lr --output-directory ./ --num-of-processors 40 --blasr /src/wgs-8.3rc2/Linux-amd64/bin --short-read-tooldir /software/bwa-0.7.15 --opera /software/OPERA-LG_v2.0.6/bin# This the second run of OPERA-LG, re-scaffold the results of SOAP-fusion. ins_270 libraryperl /software/OPERA-LG_v2.0.6/bin/OPERA-long-read.pl --contig-file /DenovoSeq/MEGAHIT/small_insert.no270/SOAP-fusion/k41.scafSeq --illumina-read1 /DenovoSeq/trimmomatic/270B_R_1P.fasta --illumina-read2 /DenovoSeq/trimmomatic/270B_R_2P.fasta --long-read-file /DenovoSeq/Third_rawData/av_20k.fasta --output-prefix 270B.lr --output-directory ./270B --num-of-processors 10 --blasr /software/src/wgs-8.3rc2/Linux-amd64/bin --short-read-tooldir /software/bwa-0.7.15 --opera /software/OPERA-LG_v2.0.6/bin --samtools-dir /software/samtools-0.1.19/# This is the third run of OPERA-LG, re-scaffold the results of SOAP-fusion. ins_500 libraryperl /software/OPERA-LG_v2.0.6/bin/OPERA-long-read.pl --contig-file /DenovoSeq/MEGAHIT/small_insert.no270/SOAP-fusion/k41.scafSeq --illumina-read1 /DenovoSeq/trimmomatic/500B_R_1P.fasta --illumina-read2 /DenovoSeq/trimmomatic/500B_R_2P.fasta --long-read-file /DenovoSeq/Third_rawData/av_20k.fasta --output-prefix 500B.lr --output-directory ./500B --num-of-processors 10 --blasr /software/src/wgs-8.3rc2/Linux-amd64/bin --short-read-tooldir /software/bwa-0.7.15 --opera /software/OPERA-LG_v2.0.6/bin --samtools-dir /software/samtools-0.1.19/echo Finish time is `date +%Y/%m/%d--%H:%M` The stats I got: OPERA with all libraries 1234567891011Size_includeN: 679262960Size_withoutN: 679262960Seq_Num: 782765Mean_Size: 867Median_Size: 429Longest_Seq: 61533Shortest_Seq: 200GC_Content: 32.31N50: 1529N90: 348Gap: 0.0 without ins_270 library: 1234567891011Size_includeN: 679262960Size_withoutN: 679262960Seq_Num: 782765Mean_Size: 867Median_Size: 429Longest_Seq: 61533Shortest_Seq: 200GC_Content: 32.31N50: 1529N90: 348Gap: 0.0 OPERA-LG First run with long-reads, with 270 library, and megahit’s contigs: 1234567891011Size_includeN 767662393Size_withoutN 767662393Seq_Num 989298Mean_Size 775Median_Size 428Longest_Seq 80889Shortest_Seq 200GC_Content 32.64N50 1115N90 340Gap 0.0 Second run, re-scaffold the results of SOAP-fusion. ins_270 library 1234567891011Size_includeN: 765246417Size_withoutN: 574675978Seq_Num: 377325Mean_Size: 2028Median_Size: 393Longest_Seq: 436254Shortest_Seq: 200GC_Content: 31.42N50: 33478N90: 439Gap: 24.9 Third run, re-scaffold the results of SOAP-fusion. ins_500 library 1234567891011Size_includeN: 765246417Size_withoutN: 574675978Seq_Num: 377325Mean_Size: 2028Median_Size: 393Longest_Seq: 436254Shortest_Seq: 200GC_Content: 31.42N50: 33478N90: 439Gap: 24.9 What did this software do? The scaffold N50 of SOAPdenovo-fusion is 33478 … What a waste of time! This note can serve as a reference in case I will have to use it again…]]></content>
      <categories>
        <category>genome assembly</category>
        <category>TGS pipeline</category>
      </categories>
      <tags>
        <tag>bio-tools</tag>
        <tag>genome assembly pipeline</tag>
        <tag>scaffolding</tag>
        <tag>hybrid genome assembly</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Genome Assembly Pipeline: BESST]]></title>
    <url>%2Fblog%2F2018%2F03%2FGenome-assembly-pipeline-BESST%2F</url>
    <content type="text"><![CDATA[Intro From the introduction of BESST git repo: BESST is a package for scaffolding genomic assemblies. It paper Sahlin K, Chikhi R, Arvestad L. Assembly scaffolding with PE-contaminated mate-pair libraries. Bioinformatics. 2016;32(13):1925–1932. doi:10.1093/bioinformatics/btw064 My feeling: too many steps awkward only support NGS reads not so good results (at least in my case) In practice BESST git repo has full docs. Scripts used: 123456789101112131415161718192021222324252627282930#!/bin/sh#PBS -N BESST#PBS -j eo#PBS -q Test#PBS -l nodes=1:ppn=20#PBS -Vecho Start time is `date +%Y/%m/%d--%H:%M`# align the PE/MP reads to contigs with BWA MEM#for sample in 270B 500B 800B 5k-1 10k; do#/software/bwa-0.7.15/bwa mem -t 40 /DenovoSeq/MEGAHIT/megahit_out/final.contigs.fa /DenovoSeq/raw_data/$&#123;sample&#125;_R1.fastq /DenovoSeq/raw_data/$&#123;sample&#125;_R2.fastq | samtools view -uS - | samtools sort -@ 8 -m 4G - -T sam_sort_tmp -o ./bwaout/$&#123;sample&#125;.sorted.bam#samtools index ./bwaout/$&#123;sample&#125;.sorted.bam#done# Damn, I forgot why I use repair.sh.for sample in 3k_1 5k-2; do/software/bbmap/repair.sh in1=/DenovoSeq/raw_data/$&#123;sample&#125;_R1.fastq in2=/DenovoSeq/raw_data/$&#123;sample&#125;_R2.fastq out1=/DenovoSeq/raw_data/$&#123;sample&#125;_R1.fixed.fastq out2=/DenovoSeq/raw_data/$&#123;sample&#125;_R2.fixed.fastq/software/bwa-0.7.15/bwa mem -t 40 /DenovoSeq/MEGAHIT/megahit_out/final.contigs.fa /DenovoSeq/raw_data/$&#123;sample&#125;_R1.fixed.fastq /DenovoSeq/raw_data/$&#123;sample&#125;_R2.fixed.fastq | samtools view -uS - | samtools sort -@ 8 -m 4G - -T sam_sort_tmp -o ./bwaout/$&#123;sample&#125;.sorted.bamsamtools index ./bwaout/$&#123;sample&#125;.sorted.bamdone# scaffold the contigs of MEGAHITexport PATH=/software/Python.2.7.13/bin:$PATH/software/BESST/runBESST -plots -q -c /DenovoSeq/MEGAHIT/megahit_out/final.contigs.fa -f ./bwaout/270B.sorted.bam ./bwaout/500B.sorted.bam ./bwaout/800B.sorted.bam ./bwaout/3k_1.sorted.bam ./bwaout/5k-1.sorted.bam ./bwaout/5k-2.sorted.bam ./bwaout/10k.sorted.bam -orientation fr fr fr rf rf rf rf/software/BESST/runBESST -plots -q -c /DenovoSeq/MEGAHIT/megahit_out.no270/final.contigs.fa -f ./bwaout/500B.sorted.bam ./bwaout/800B.sorted.bam ./bwaout/3k_1.sorted.bam ./bwaout/5k-1.sorted.bam ./bwaout/5k-2.sorted.bam ./bwaout/10k.sorted.bam -orientation fr fr rf rf rf rf -o ./no270echo Finish time is `date +%Y/%m/%d--%H:%M` Tested with or without ins_270 library. And the stats I got: with ins_270 library: 1234567891011Size_includeN: 778878802Size_withoutN: 755401923Seq_Num: 811939Mean_Size: 959Median_Size: 393Longest_Seq: 561369Shortest_Seq: 200GC_Content: 32.65N50: 2715N90: 342Gap: 3.01 without ins_270 library: 1234567891011Size_includeN: 654690437Size_withoutN: 626185453Seq_Num: 618555Mean_Size: 1058Median_Size: 438Longest_Seq: 327376Shortest_Seq: 200GC_Content: 32.31N50: 2277N90: 365Gap: 4.35 Though not been fully tested, using BESST got worse results than SOAPdenovo. Then I gave up this tool. This note can serve as a reference in case I will have to use it again…]]></content>
      <categories>
        <category>genome assembly</category>
        <category>NGS pipeline</category>
      </categories>
      <tags>
        <tag>bio-tools</tag>
        <tag>genome assembly pipeline</tag>
        <tag>NGS genome assembly</tag>
        <tag>scaffolding</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Genome Assembly Pipeline: LINKS]]></title>
    <url>%2Fblog%2F2018%2F03%2FGenome-assembly-pipeline-LINKS%2F</url>
    <content type="text"><![CDATA[Intro From its Git Repo LINKS is a scalable genomics application for scaffolding or re-scaffolding genome assembly drafts with long reads, such as those produced by Oxford Nanopore Technologies Ltd and Pacific Biosciences. It provides a generic alignment-free framework for scaffolding and can work on any sequences. It is versatile and supports not only long sequences as a source of long-range information, but also MPET pairs and linked-reads, such as those from the 10X Genomics GemCode and Chromium platform, via ARCS (http://www.bcgsc.ca/platform/bioinfo/software/arcs). Fill gaps in LINKS-derived scaffolds using Sealer (http://www.bcgsc.ca/platform/bioinfo/software/sealer). Its paper: Warren RL, Yang C, Vandervalk BP, Behsaz B, Lagman A, Jones SJM, Birol I. LINKS: Scalable, alignment-free scaffolding of draft genomes with long reads. GigaScience. 2015;4:35. doi:10.1186/s13742-015-0076-3 My feelings: easy to use support scaffolding and re-scaffolding fast comparatively good results (at least in my case, compared to BESST and OPERA-LG) huge RAM consumption General usage Install Dowload the latest version from its release page. 12345678# decompresstar -zxvf links_v1-8-6.tar.gz; cd links_v1.8.6# build BloomFilter PERL modulecd lib/bloomfilter/swigswig -Wall -c++ -perl5 BloomFilter.ig++ -c BloomFilter_wrap.cxx -I/usr/lib64/perl5/CORE -fPIC -Dbool=char -O3g++ -Wall -shared BloomFilter_wrap.o -o BloomFilter.so -O3 Parameter setting Here gives a general description of the way LINKS works. Setting the parameters is crucial for scaffolding. Below I summarize some tips or practices when setting these parameters. scaffolding control -a: the maximum ratio between the best two contig pairs for a given seed/contig being extended (default: 0.3). -l: the minimum number of links (read pairs) a valid contig pair MUST have to be considered (default: 5). For example, contig A shares 4 links with B and 2 links with C, in this orientation. contig rA (reverse) also shares 3 links with D. When it’s time to extend contig A (with the options -l and -a set to 2 and 0.7, respectively), both contig pairs AB and AC are considered. Since C (second-best) has 2 links and B (best) has 4 (2/4) = 0.5 below the maximum ratio of 0.7, A will be linked with B in the scaffold and C will be kept for another extension. If AC had 3 links the resulting ratio (0.75), above the user-defined maximum 0.7 would have caused the extension to terminate at A, with both B and C considered for a different scaffold. A maximum links ratio of 1 (not recommended) means that the best two candidate contig pairs have the same number of links – LINKS will accept the first one since both have a valid gap/overlap. When a scaffold extension is terminated on one side, the scaffold is extended on the “left”, by looking for contig pairs that involve the reverse of the seed (in this example, rAD). With AB and AC having 4 and 2 links, respectively and rAD being the only pair on the left, the final scaffolds outputted by LINKS would be: rD-A-B and C. k-mer length: -k -k: k-mer value (default 15). LINKS is a k-mer scaffolder, and the -k parameter controls the k-mer length. Exploration of vast kmer space is expected to yield better scaffolding results. You may increase -k to 21 while working with pacbio reads. I also recommend correcting ONT reads if you can, it will allow you to choose higher k values and increase the specificity. The sweet spot will be somewhere k 15-19 for 2Gb genome (assuming raw ONT reads). k-mer pairs extraction: -d, -e and -t. And -t and -d are important for memory usage. -e: error (%) allowed on -d distance e.g. -e 0.1 == distance +/- 10% (default: 0.1) In theory, the -e parameter will play an important role limiting linkages outside of the target range -d (+/-) -e %. This is especially true when using raw MPET for scaffolding, to limit spurious linkages by contaminating PETs. -d: distance between the 5’-end of each pairs (default: 4000) -t: sliding window when extracting k-mer pairs from long reads (default: 2) Because you want want to start with a low -d for scaffolding, you have to estimate how many minimum links (-l) would fit in a -d window +/- error -e given sliding window -t. For instance, it may not make sense to use -t 200, -d 500 at low coverages BUT if you have at least 10-fold coverage it might since, in principle, you should be able to derive sufficient k-mer pairs within same locus if there’s no bias in genome sequencing. On the data side of things, reducing the coverage (using less long reads), and limiting to only the highest quality reads would help decrease RAM usage. WARNING: Specifying many distances will require large amount of RAM, especially with low -t values. As -d increases, -t must decrease (otherwise you’ll end up with too few pairs for scaffolding over larger kmer distances). An example The power of LINKS is in scaffolding using various distance constraints, iteratively. Running links multiple times (when working with large long read dataset / genomes are very big) 123456789nohup ./runIterativeLINKS.sh 17 beluga.fa 5 0.3 &amp;./LINKS -f $2 -s ont.fof -b links1 -d 1000 -t 10 -k $1 -l $3 -a $4./LINKS -f links1.scaffolds.fa -s ont.fof -b links2 -d 2500 -t 5 -k $1 -l $3 -a $4 -o 1 -r links1.bloom./LINKS -f links2.scaffolds.fa -s ont.fof -b links3 -d 5000 -t 5 -k $1 -l $3 -a $4 -o 2 -r links1.bloom./LINKS -f links3.scaffolds.fa -s ont.fof -b links4 -d 7500 -t 4 -k $1 -l $3 -a $4 -o 3 -r links1.bloom./LINKS -f links4.scaffolds.fa -s ont.fof -b links5 -d 10000 -t 4 -k $1 -l $3 -a $4 -o 4 -r links1.bloom./LINKS -f links5.scaffolds.fa -s ont.fof -b links6 -d 12500 -t 3 -k $1 -l $3 -a $4 -o 5 -r links1.bloom./LINKS -f links6.scaffolds.fa -s ont.fof -b links7 -d 15000 -t 3 -k $1 -l $3 -a $4 -o 6 -r links1.bloom./LINKS -f links7.scaffolds.fa -s ont.fof -b links8 -d 30000 -t 2 -k $1 -l $3 -a $4 -o 7 -r links1.bloom Running links iteratively in a single command (works best when genomes are small/RAM not limiting) 1./LINKS -f $2 -s ont.fof -b links1 -d 1000,5000,7500,10000,12500,15000,30000 -t 10,5,5,4,4,3,3,2 -k $1 -l $3 -a $4 the value of -t will determine the #kmer pairs extracted and incidentally the RAM used. this will vary from one dataset to the next. The example was from: working with 2 Gb genome; stuck on bloom filter being built. It seems LINKS needs huge RAM, and you may also want to try lrscaf, another long reads scaffolding tool. In practice background An insect The species: high heterogeneity, high AT, high repetition. Genome size: male 790M, female 830M. data The Illumina data I used: Source Insert size (bp) Avg. read size (bp) Raw bases (G) Raw reads (M) Sequencing depth AV1, M 270 150 44.1 293.6 55.5 AV2, F 500 150 24.4 162.8 29.4 AV2, F 800 150 15.8 105.4 19.0 AV2, F 3k 114 10.4 91.8 12.5 AV2, F 5k 150 29.8 198.7 35.9 AV2, F 5k 114 11.5 101.2 13.8 AV2, F 10k 150 17.5 116.8 21.1 Total - - 153.5 1070.3 187.3 And the PacBio data: Source Raw bases (G) Raw reads (M) Sequencing depth Avg.read size (bp) N50 (bp) N90 (bp) Note AV3, F 15.2 20.1 18.31 7550 10046 4558 20170111 AV4, F 45.2 4.6 54.46 9798 17348 5702 20171224 Total 60.4 24.7 72.77 9115 14630 5310 - Because we didn’t receive the data at the same time (first all Illumina data, then 20X PacBio data, finally another 50X PacBio data), we tried many different assembly strategies: Illumina-dominant, and PacBio-dominant. We found ‘PacBio-dominant pipelines’ produced significantly good results, so we gradually gave up other pipelines and focused on exploring PacBio assemblers. This note is one of the attempts of ‘Illumina-dominant pipelines’, and I ran LINKS as a scaffoling tool, after runing MEGAHIT-SOAPdenovo successfully. I’ve tried so many with MEGAHIT, and I ran LINKS after every attempt. See Genome Assembly Pipeline: MEGAHIT &amp; SOAPdenovo-fusion. I put two representative results here as a memo. PS: I encountered a problem and solved with the help of the author: LINKS termineted with no error message when I used hybrid reads to scaffold. The -t and -d are important for RAM consumption. I’ve tried -t with 5, 10, 20, and -t 20 works. -d didn’t matter in my case, so I just set lots of -d. Scripts I used and the stats I got: 12345678910111213LINKS_HOME=/software/links_v1.8.5WORK_DIR=/DenovoSeqecho Start time is `date +%Y/%m/%d--%H:%M`# re-scafflod the output of MEGAHIT small insert size and SOAP-fusion. use about 50X data#$LINKS_HOME/LINKS -k 21 -f $WORK_DIR/MEGAHIT/small_insert.no270/SOAP-fusion/k41.scafSeq -s Pacbio.fof -b reLINKS2 -t 20 -d 4000,5000,6000,8000,10000,15000,20000# re-scaffold the output of MEGAHIT small insert size and SOAP-fusion, use about 70X data$LINKS_HOME/LINKS -k 21 -f $WORK_DIR/MEGAHIT/small_insert.no270/SOAP-fusion/k41.scafSeq -s Pacbio_171231.fof -b reLINKS_171231 -t 20 -d 4000,5000,6000,8000,10000,15000,20000echo Finish time is `date +%Y/%m/%d--%H:%M` And the input data: 1234567## first batch PB data, about 20X$ cat Pacbio.fof/DenovoSeq/Third_rawData/av_20k.fasta## all PB data, about 70X$ cat Pacbio_171231.fof/DenovoSeq/Third_rawData/third_all.fasta The stats I got: re-scaffloding the output of MEGAHIT small insert size and SOAP-fusion, use ~20X PB data: MEGAHIT SOAP-fusion LINKS Size_includeN 582099585 765246417 776147223 Size_withoutN 582099585 574675978 574675978 Seq_Num 575808 377325 370033 Mean_Size 1010 2028 2097 Median_Size 433 393 389 Longest_Seq 55064 436254 621272 Shortest_Seq 200 200 200 GC_Content 31.43 31.42 31.42 N50 2158 33478 42519 N90 358 439 444 Gap 0 24.9 25.96 re-scaffolding the output of MEGAHIT small insert size and SOAP-fusion, use ~70X PB data. MEGAHIT SOAP-fusion LINKS Size_includeN 582099585 765246417 798803587 Size_withoutN 582099585 574675978 574675978 Seq_Num 575808 377325 359653 Mean_Size 1010 2028 2221 Median_Size 433 393 384 Longest_Seq 55064 436254 732454 Shortest_Seq 200 200 200 GC_Content 31.43 31.42 31.42 N50 2158 33478 68195 N90 358 439 453 Gap 0 24.9 28.06 The last run took about 27 hours. As can be seen, adding 50X PacBio data did not help a lot. After this, we started to use ‘PacBio-dominant pipelines’ and have tried many different assemblers. But, LINKS is a good scaffolding tool if you assemble the genome with illumina data and want to further scaffold with &lt;20X long-reads. This note can serve as a reference in case I will have to use it again. Change log 20180307: createt the note. 20180709: update the ‘background information’. 20180817: change the ‘General usage’ part, add contents about ‘parameter setting’.]]></content>
      <categories>
        <category>genome assembly</category>
        <category>hybrid pipeline</category>
      </categories>
      <tags>
        <tag>bio-tools</tag>
        <tag>genome assembly pipeline</tag>
        <tag>scaffolding</tag>
        <tag>hybrid genome assembly</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Genome Assembly Pipeline: MEGAHIT & SOAPdenovo-fusion]]></title>
    <url>%2Fblog%2F2018%2F03%2FGenome-assembly-pipeline-MEGAHIT-SOAPdenovo-fusion%2F</url>
    <content type="text"><![CDATA[MEGAHIT can be used to assemble contigs, and SOAPdenovo-fusion can be used for scaffolding. Since they were developed by the same team, I just put them together. This note is more about MEGAHIT and its performance, because you can choose not to use SOAPdevovo-fusion. SOAPdenovo-fusion had comparatively good performance in my case, so why not give it a try? Intro From MEGAHIT git repo MEGAHIT is a single node assembler for large and complex metagenomics NGS reads, such as soil. It makes use of succinct de Bruijn graph (SdBG) to achieve low memory assembly. MEGAHIT can optionally utilize a CUDA-enabled GPU to accelerate its SdBG contstruction. The GPU-accelerated version of MEGAHIT has been tested on NVIDIA GTX680 (4G memory) and Tesla K40c (12G memory) with CUDA 5.5, 6.0 and 6.5. MEGAHIT v1.0 or greater also supports IBM Power PC and has been tested on IBM POWER8. Its paper Li D, Liu C-M, Luo R, Sadakane K, Lam T-W. MEGAHIT: an ultra-fast single-node solution for large and complex metagenomics assembly via succinct de Bruijn graph. Bioinformatics. 2015;31(10):1674–1676. doi:10.1093/bioinformatics/btv033 My feelings: very easy to use fast enough better than SOAPdenovo2 no need to designate k-mer General usage See MEGAHIT wiki for full docs. Assembly Tips An example of real assembly In practice - MEGAHIT An insect The species: high heterogeneity, high AT, high repetition. Genome size: male 790M, female 830M. data The Illumina data I used: Source Insert size (bp) Avg. read size (bp) Raw bases (G) Raw reads (M) Sequencing depth AV1, M 270 150 44.1 293.6 55.5 AV2, F 500 150 24.4 162.8 29.4 AV2, F 800 150 15.8 105.4 19.0 AV2, F 3k 114 10.4 91.8 12.5 AV2, F 5k 150 29.8 198.7 35.9 AV2, F 5k 114 11.5 101.2 13.8 AV2, F 10k 150 17.5 116.8 21.1 Total - - 153.5 1070.3 187.3 I’ve tried MEGAHIT with raw/trimmed data, with/without ins_270 library, with all (PE and MPE)/PE libraries, and here are the scripts I used and stats received. The reason why I tried with/without ins_270 library was because it’s from a male but other libraries were from females. The reason why I tried with all (PE and MPE)/PE libraries was because I ran MEGAHIT with all data I had, and then the author recommended only to use PE libraries. See the discussions with the authors. Segmentation fault with scaff step when use different MEGAHIT’s output How to use SOAPdenovo-fusion scaffold the output of MEGAHIT? run1, all raw data 1/software/megahit_v1.1.1_LINUX_CPUONLY_x86_64-bin/megahit -t 20 --no-mercy -1 /DenovoSeq/raw_data/270B_R1.fastq,/DenovoSeq/raw_data/500B_R1.fastq,/DenovoSeq/raw_data/800B_R1.fastq,/DenovoSeq/raw_data/3k_1_R1.fastq,/DenovoSeq/raw_data/5k-1_R1.fastq,/DenovoSeq/raw_data/5k-2_R1.fastq,/DenovoSeq/raw_data/10k_R1.fastq -2 /DenovoSeq/raw_data/270B_R2.fastq,/DenovoSeq/raw_data/500B_R2.fastq,/DenovoSeq/raw_data/800B_R2.fastq,/DenovoSeq/raw_data/3k_1_R2.fastq,/DenovoSeq/raw_data/5k-1_R2.fastq,/DenovoSeq/raw_data/5k-2_R2.fastq,/DenovoSeq/raw_data/10k_R2.fastq -o megahit_out1 and the stats: 12345678910Size_includeN: 894816039Size_withoutN: 894816039Seq_Num: 1292253Mean_Size: 692Median_Size: 418Longest_Seq: 42542Shortest_Seq: 200GC_Content: 32.9N50: 834N90: 333 run2, all trimmed data (by Trimmomatic) 1/software/megahit_v1.1.1_LINUX_CPUONLY_x86_64-bin/megahit -t 38 --no-mercy -1 /DenovoSeq/trimmomatic/270B_R_1P.fastq,/DenovoSeq/trimmomatic/500B_R_1P.fastq,/DenovoSeq/trimmomatic/800B_R_1P.fastq,/DenovoSeq/trimmomatic/3k_1_R_1P.fastq,/DenovoSeq/trimmomatic/5k-1_R_1P.fastq,/DenovoSeq/trimmomatic/5k-2_R_1P.fastq,/DenovoSeq/trimmomatic/10k_R_1P.fastq -2 /DenovoSeq/trimmomatic/270B_R_2P.fastq,/DenovoSeq/trimmomatic/500B_R_2P.fastq,/DenovoSeq/trimmomatic/800B_R_2P.fastq,/DenovoSeq/trimmomatic/3k_1_R_2P.fastq,/DenovoSeq/trimmomatic/5k-1_R_2P.fastq,/DenovoSeq/trimmomatic/5k-2_R_2P.fastq,/DenovoSeq/trimmomatic/10k_R_2P.fastq and the stats: 12345678910Size_includeN: 767662393Size_withoutN: 767662393Seq_Num: 989298Mean_Size: 775Median_Size: 428Longest_Seq: 80889Shortest_Seq: 200GC_Content: 32.64N50: 1115N90: 340 run3, all raw data, without ins_270 1/software/megahit_v1.1.1_LINUX_CPUONLY_x86_64-bin/megahit -t 20 --no-mercy -1 /DenovoSeq/raw_data/500B_R1.fastq,/DenovoSeq/raw_data/800B_R1.fastq,/DenovoSeq/raw_data/3k_1_R1.fastq,/DenovoSeq/raw_data/5k-1_R1.fastq,/DenovoSeq/raw_data/5k-2_R1.fastq,/DenovoSeq/raw_data/10k_R1.fastq -2 /DenovoSeq/raw_data/500B_R2.fastq,/DenovoSeq/raw_data/800B_R2.fastq,/DenovoSeq/raw_data/3k_1_R2.fastq,/DenovoSeq/raw_data/5k-1_R2.fastq,/DenovoSeq/raw_data/5k-2_R2.fastq,/DenovoSeq/raw_data/10k_R2.fastq -o megahit_out.no2701 and the stats: 12345678910Size_includeN: 800012379Size_withoutN: 800012379Seq_Num: 1045419Mean_Size: 765Median_Size: 422Longest_Seq: 52915Shortest_Seq: 200GC_Content: 32.64N50: 1074N90: 340 run4, all trimmed data, without ins_270 12345678910Size_includeN: 679262960Size_withoutN: 679262960Seq_Num: 782765Mean_Size: 867Median_Size: 429Longest_Seq: 61533Shortest_Seq: 200GC_Content: 32.31N50: 1529N90: 348 run5, use only with all PE libraries 1/software/megahit_v1.1.1_LINUX_CPUONLY_x86_64-bin/megahit -t 40 --no-mercy -1 /DenovoSeq/trimmomatic/270B_R_1P.fastq,/DenovoSeq/trimmomatic/500B_R_1P.fastq,/DenovoSeq/trimmomatic/800B_R_1P.fastq -2 /DenovoSeq/trimmomatic/270B_R_2P.fastq,/DenovoSeq/trimmomatic/500B_R_2P.fastq,/DenovoSeq/trimmomatic/800B_R_2P.fastq -o small_insert and the stats: 12345678910Size_includeN: 672319141Size_withoutN: 672319141Seq_Num: 777747Mean_Size: 864Median_Size: 428Longest_Seq: 71939Shortest_Seq: 200GC_Content: 31.93N50: 1505N90: 346 run6, use only with all PE libraries but ins_270 1/software/megahit_v1.1.1_LINUX_CPUONLY_x86_64-bin/megahit -t 40 --no-mercy -1 /DenovoSeq/trimmomatic/500B_R_1P.fastq,/DenovoSeq/trimmomatic/800B_R_1P.fastq -2 /DenovoSeq/trimmomatic/500B_R_2P.fastq,/DenovoSeq/trimmomatic/800B_R_2P.fastq -o small_insert.no270 and the stats: 12345678910Size_includeN: 582099585Size_withoutN: 582099585Seq_Num: 575808Mean_Size: 1010Median_Size: 433Longest_Seq: 55064Shortest_Seq: 200GC_Content: 31.43N50: 2158N90: 358 conclusions Though not been fully tested, I can draw some simple conclusions trimmed data generates better results than raw data. (but the way trimming data will influce the results) using only PE libraries generates better results than using all libraries (PE, MPE) In practice - SOAPdenovo-fusion I’ve asked the author that How to use SOAPdenovo-fusion scaffold the output of MEGAHIT?. I first tried SOAPdenovo-fusion with/without ins_270 library, and found not using ins_270 library got better results (tested with k-mer=63). Then I tested different kmer: 37, 41, 43, 45, 55, 61, 63, 71, 75 and found that kmer=41 got best results. I’ve also tried with/without -F parameter, but I didn’t understand the diference completely. I just put the config, scripts and stats here when using kmer = 41. config 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475#maximal read lengthmax_rd_len=151[LIB]avg_ins=500reverse_seq=0asm_flags=2#in which order the reads are used while scaffoldingrank=1# cutoff of pair number for a reliable connection (at least 3 for short insert size)pair_num_cutoff=3#minimum aligned length to contigs for a reliable read location (at least 32 for short insert size)map_len=32#a pair of fastq file, read 1 file should always be followed by read 2 fileq1=/DenovoSeq/trimmomatic/500B_R_1P.fastqq2=/DenovoSeq/trimmomatic/500B_R_2P.fastq[LIB]#average insert sizeavg_ins=800#if sequence needs to be reversedreverse_seq=0#in which part(s) the reads are usedasm_flags=2#in which order the reads are used while scaffoldingrank=3# cutoff of pair number for a reliable connection (at least 3 for short insert size)pair_num_cutoff=3#minimum aligned length to contigs for a reliable read location (at least 32 for short insert size)map_len=32#a pair of fastq file, read 1 file should always be followed by read 2 fileq1=/DenovoSeq/trimmomatic/800B_R_1P.fastqq2=/DenovoSeq/trimmomatic/800B_R_2P.fastq[LIB]avg_ins=3000reverse_seq=1asm_flags=2rank=3# cutoff of pair number for a reliable connection (at least 5 for large insert size)pair_num_cutoff=4#minimum aligned length to contigs for a reliable read location (at least 35 for large insert size)map_len=35q1=/DenovoSeq/trimmomatic/3k_1_R_1P.fastqq2=/DenovoSeq/trimmomatic/3k_1_R_2P.fastq[LIB]avg_ins=5000reverse_seq=1asm_flags=2rank=4# cutoff of pair number for a reliable connection (at least 5 for large insert size)pair_num_cutoff=5#minimum aligned length to contigs for a reliable read location (at least 35 for large insert size)map_len=35q1=/DenovoSeq/trimmomatic/5k-1_R_1P.fastqq2=/DenovoSeq/trimmomatic/5k-1_R_2P.fastq[LIB]avg_ins=5000reverse_seq=1asm_flags=2rank=4# cutoff of pair number for a reliable connection (at least 5 for large insert size)pair_num_cutoff=5#minimum aligned length to contigs for a reliable read location (at least 35 for large insert size)map_len=35q1=/DenovoSeq/trimmomatic/5k-2_R_1P.fastqq2=/DenovoSeq/trimmomatic/5k-2_R_2P.fastq[LIB]avg_ins=10000reverse_seq=1asm_flags=2rank=5# cutoff of pair number for a reliable connection (at least 5 for large insert size)pair_num_cutoff=5#minimum aligned length to contigs for a reliable read location (at least 35 for large insert size)map_len=35q1=/DenovoSeq/trimmomatic/10k_R_1P.fastqq2=/DenovoSeq/trimmomatic/10k_R_2P.fastq run1, without -F 123/software/SOAPdenovo2-r241/SOAPdenovo-fusion -D -s config -p 40 -K 41 -g k41 -c ../final.contigs.fa/software/SOAPdenovo2-r241/SOAPdenovo-127mer map -s config -p 40 -g k41/software/SOAPdenovo2-r241/SOAPdenovo-127mer scaff -p 40 -g k41 stats: 1234567891011Size_includeN: 765246417Size_withoutN: 574675978Seq_Num: 377325Mean_Size: 2028Median_Size: 393Longest_Seq: 436254Shortest_Seq: 200GC_Content: 31.42N50: 33478N90: 439Gap: 24.9 run2, with -F 123/software/SOAPdenovo2-r241/SOAPdenovo-fusion -D -s config -p 40 -K 41 -g k41_1 -c ../final.contigs.fa/software/SOAPdenovo2-r241/SOAPdenovo-127mer map -s config -p 40 -g k41_1/software/SOAPdenovo2-r241/SOAPdenovo-127mer scaff -p 40 -g k41_1 -F stats: 1234567891011Size_includeN: 764869730Size_withoutN: 579451220Seq_Num: 377325Mean_Size: 2027Median_Size: 393Longest_Seq: 436104Shortest_Seq: 200GC_Content: 31.45N50: 33463N90: 439Gap: 24.24 It seems that -F parameter didn’t help much (the %gap). This note can be a reference in case I will have to use it again. Change log 20180308: create the note.]]></content>
      <categories>
        <category>genome assembly</category>
        <category>NGS pipeline</category>
      </categories>
      <tags>
        <tag>bio-tools</tag>
        <tag>genome assembly pipeline</tag>
        <tag>NGS genome assembly</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Upstream Analysis of TCR/BCR Repertoires in RNA-seq Data]]></title>
    <url>%2Fblog%2F2018%2F03%2FUpstream-analysis-of-TCR-BCR-repertoires-in-RNA-seq-data%2F</url>
    <content type="text"><![CDATA[Bacground: because the transcriptome data I recently worked with is highly related with immune. So I want to dig out something about immune. When I viewed RNAseq analysis notes from Tommy Tang, I found ImReP. It’s a tool designed for profiling TCR/BCR repertoire in regular RNA-seq data. That was something I nerver heard before. Then I found more tools through the paper of ImRep [1]. They are: MiXCR [2] TRUST [3] TraCeR [4] V'DJer [5] IgBlast-based pipeline [6] iSSAKE [7] And the paper of ImRep introduce them as follows: TRUST and TraCeR do not support the analysis of BCR sequences and were excluded from the comparison based for the IGH data. iSSAKE is no longer supported and was not recommended for use. Unfortunately, we obtained empty output after running V’DJer, and increasing coverage in the simulated data did not solve the problem. Alternative approaches, such as IMSEQ, cannot be applied directly to RNA-Seq reads because they were originally designed for targeted sequencing of B or T cell receptor loci. Thus, to independently assess and compare accuracy with ImReP, we only ran IMSEQ with the simulated reads derived from BCR or TCR transcripts (Figure S1). Scripts and commands to run all tools used in this study are provided in the Extended Experimental Procedures and are available online at https://github.com/smangul1/Profiling-adaptive-immune-repertoires-across-multiple-humantissues-by-RNA-Sequencing. ImReP consistently outperformed existing methods on IGH data in both recall and precision rates for the majority of simulated parameters. ImReP and MiXCR show similar performance on TCRA data and outperform other methods. Notably, ImReP was the only method with acceptable performance on IGH data at 50bp read length, reconstructing with a higher precision rate significantly more CDR3 clonotypes than other methods. Because I only have regular RNA-seq data (non-enriched and/or randomly-shred ©DNA libraries), ImRep and MiXCR were the only two software I want to try. MiXCR Intro MiXCR is a universal framework that processes big immunome data from raw sequences to quantitated clonotypes. MiXCR efficiently handles paired- and single-end reads, considers sequence quality, corrects PCR errors and identifies germline hypermutations. The software supports both partial- and full-length profiling and employs all available RNA or DNA information, including sequences upstream of V and downstream of J gene segments. (https://mixcr.readthedocs.io/en/latest/) MiXCR has very nice docs: https://mixcr.readthedocs.io/en/latest. See them for full instructions. Typical MiXCR workflow consists of three main processing steps: align: align sequencing reads to reference V, D, J and C genes of T- or B- cell receptors assemble: assemble clonotypes using alignments obtained on previous step (in order to extract specific gene regions e.g. CDR3) export: export alignment (exportAlignments) or clones (exportClones) to human-readable text file Enriched RepSeq Data Here is a very simple usage example that will extract repertoire data (in the form of clonotypes list) from raw sequencing data of enriched RepSeq library: 123mixcr align -r log.txt input_R1.fastq.gz input_R2.fastq.gz alignments.vdjcamixcr assemble -r log.txt alignments.vdjca clones.clnsmixcr exportClones clones.clns clones.txt this will produce a tab-delimited list of clones (clones.txt) assembled by their CDR3 sequences with extensive information on their abundances, V, D and J genes, mutations in germline regions, topology of VDJ junction etc. Repertoire extraction from RNA-Seq MiXCR is equally effective in extraction of repertoire information from non-enriched data, like RNA-Seq or WGS. This example illustrates usage for RNA-Seq: 1234mixcr align -p rna-seq -r log.txt input_R1.fastq.gz input_R2.fastq.gz alignments.vdjcamixcr assemblePartial alignments.vdjca alignment_contigs.vdjcamixcr assemble -r log.txt alignment_contigs.vdjca clones.clnsmixcr exportClones clones.clns clones.txt Install download the latest stable MiXCR build from release page 123456unzip mixcr-2.1.9.zip &amp;&amp; cd mixcr-2.1.9$ $ ./mixcr -hUsage: mixcr [options] [command] [command options] Options: -h, --help Displays this help message. Run Pipelines from https://github.com/milaboratory/mixcr and http://mixcr.readthedocs.io/en/latest/rnaseq.html are not exactly the same. I used the latter. 1234567891011path2MiXCR=$TOOLDIR/mixcr-2.1.9/mixcrMiXCR_output=$&#123;WORKDIR&#125;/MiXCR/human/$&#123;sample&#125;PPN=20$path2MiXCR align -t $PPN -r $MiXCR_output/log.txt -p rna-seq -s hsa -OallowPartialAlignments=true $WORKDIR/clean_fastq/human/$&#123;sample&#125;_R1.fastq.gz $WORKDIR/clean_fastq/human/$&#123;sample&#125;_R2.fastq.gz $MiXCR_output/alignments.vdjca$path2MiXCR assemblePartial -r $MiXCR_output/log.txt $MiXCR_output/alignments.vdjca $MiXCR_output/alignments_rescued_1.vdjca$path2MiXCR assemblePartial -r $MiXCR_output/log.txt $MiXCR_output/alignments_rescued_1.vdjca $MiXCR_output/alignments_rescued_2.vdjca$path2MiXCR extendAlignments -r $MiXCR_output/log.txt $MiXCR_output/alignments_rescued_2.vdjca $MiXCR_output/alignments_rescued_2_extended.vdjca$path2MiXCR assemble -r $MiXCR_output/log.txt -t $PPN $MiXCR_output/alignments_rescued_2_extended.vdjca $MiXCR_output/clones.clns$path2MiXCR exportClones $MiXCR_output/clones.clns $MiXCR_output/clones.txt And the output looks like this: 1234$ head -n 3 clones.txtcloneId cloneCount cloneFraction clonalSequence clonalSequenceQuality allVHitsWithScore allDHitsWithScore allJHitsWithScore allCHitsWithScore allVAlignments allDAlignments allJAlignments allCAlignments nSeqFR1 minQualFR1 nSeqCDR1 minQualCDR1 nSeqFR2 minQualFR2 nSeqCDR2 minQualCDR2 nSeqFR3 minQualFR3 nSeqCDR3 minQualCDR3 nSeqFR4 minQualFR4 aaSeqFR1 aaSeqCDR1 aaSeqFR2 aaSeqCDR2 aaSeqFR3 aaSeqCDR3 aaSeqFR4 refPoints0 130 0.008044056679660912 TGCTGCTCATATGCAGGCAGCTACACTTGGGTGTTC FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF IGLV2-11*00(498.3) IGLJ3*00(155.3) IGLC2*00(628.6),IGLC3*00(628.3),IGLC7*00(558) 324|352|374|0|28||140.0 20|30|58|26|36||50.0 ;; TGCTGCTCATATGCAGGCAGCTACACTTGGGTGTTC 37 CCSYAGSYTWVF :::::::::0:-2:28:::::26:0:36:::1 62 0.003836396262607512 TGCAGCTCATATACAAGCAGCAGCACTTTCGTCTTC FFFFFFFFFFFFFFFFFFFFFFFFFNNNNNNNNNNN IGLV2-14*00(599.8) IGLJ1*00(118.3) IGLC1*00(446.2) 324|355|374|0|31|SC351T|139.0 24|30|58|30|36||30.0 TGCAGCTCATATACAAGCAGCAGCACTTTCGTCTTC37 CSSYTSSSTFVF :::::::::0:1:31:::::30:-4:36::: No ideas what to do next… I will try some post-analysis tools to explore the clonetypes. ImRep Intro ImReP is a method for rapid and accurate profiling of the adaptive immune repertoires from regular RNA-Seq data. Install 12345678910111213git clone https://github.com/mandricigor/imrep.git &amp;&amp; cd imrep./install.sh$ python imrep.py -husage: python2 imrep.py [-h] [--fastq] [--bam] [--chrFormat2] [--hg38] [-a ALLREADS] [--digGold] [-s SPECIES] [-o OVERLAPLEN] [--noOverlapStep] [--extendedOutput] [-c CHAINS] [--noCast] [-f FILTERTHRESHOLD] [--minOverlap1 MINOVERLAP1] [--minOverlap2 MINOVERLAP2] [--misMatch1 MISMATCH1] [--misMatch2 MISMATCH2] reads_file output_clones Run Then I was caught in an embarrassing situation. ImRep now is designed to handle two cases: When you have saved mapped and unmapped reads in one BAM file, ImRep can accept one BAM as input. Given the bam file with mapped and unmapped reads, you can run ImReP using this command. python imrep.py --bam example/toyExample.bam example/toyExample.cdr3 When you forgot to save unmapped reads, ImRep can accept BAM file with mapped reads and all raw FASTQ files as input. Forgot to save unmapped reads, we got you covered. Use --digGold and -a options. For example: python imrep.py --digGold -a example/toyExample_allReads.fastq example/toyExample_onlyMapped.bam example/toyExample.cdr3 I’ve aligned the FASTQ files to genome with STAR, and saved unmapped reads in FASTQ format (using --outReadsUnmapped Fastx). And the author said: Some mapping tools produce partially-mapped reads (i.e. STAR). In case read is mapped to BCR or TCR genes and is partially mapped to V or J gene, such read may be used to assemble full-length CDR3 sequences. Considering only unmapped reads will result in missing such reads. So the first case doesn’t suit me, I have to follow the second. And the questions are: Can I feed ImRep with the BAM and unmapped reads? not all raw reads. It seems ImRep only accepts one single FATSQ as input, should I cat two FASTQ of pair-end data? Or it just works for single-end data? I reported a issue to the author: unmapped reads in fastq/fasta format and pair-end data. And he suggested: Please merge PE into one file. Also to use --digGold, you need to provide original reads, not the unmapped reads. Please let me know how it goes. If this doesn’t’ work for you, we can implement the option to allow to supply bam with mapped and FASTQ with unmapped (this is on our TODO list anyway). Thanks, Serghei I should run STAR with --outSamUnmapped Within option hereafter. And I’ll not plan to re-align the reads for now. Moreover, using MiXCR, I can use post-analysis tools such as VDJtools easily. Change notes 20180324: create the note. Mangul S, Mandric I, Yang HT, Strauli N, Montoya D, Rotman J, Wey WVD, Ronas JR, Statz B, Zelikovsky A, et al. Profiling adaptive immune repertoires across multiple human tissues by RNA Sequencing. bioRxiv. 2017 Mar 25:089235. doi:10.1101/089235 ↩︎ Bolotin DA, Poslavsky S, Mitrophanov I, Shugay M, Mamedov IZ, Putintseva EV, Chudakov DM. MiXCR: software for comprehensive adaptive immunity profiling. Nature Methods. 2015;12(5):380–381. doi:10.1038/nmeth.3364 ↩︎ Li B, Li T, Wang B, Dou R, Pignon J-C, Choueiri TK, Signoretti S, Liu JS, Liu XS. Ultrasensitive detection of TCR hypervariable region in solid-tissue RNA-seq data. bioRxiv. 2016 Sep 5:073395. doi:10.1101/073395 ↩︎ Stubbington MJT, Lönnberg T, Proserpio V, Clare S, Speak AO, Dougan G, Teichmann SA. T cell fate and clonality inference from single-cell transcriptomes. Nature Methods. 2016;13(4):329–332. doi:10.1038/nmeth.3800 ↩︎ Mose LE, Selitsky SR, Bixby LM, Marron DL, Iglesia MD, Serody JS, Perou CM, Vincent BG, Parker JS. Assembly-based inference of B-cell receptor repertoires from short read RNA sequencing data with V’DJer. Bioinformatics. 2016;32(24):3729–3734. doi:10.1093/bioinformatics/btw526 ↩︎ Strauli NB, Hernandez RD. Statistical inference of a convergent antibody repertoire response to influenza vaccine. Genome Medicine. 2016;8:60. doi:10.1186/s13073-016-0314-z ↩︎ Warren RL, Nelson BH, Holt RA. Profiling model T-cell metagenomes with short reads. Bioinformatics (Oxford, England). 2009;25(4):458–464. doi:10.1093/bioinformatics/btp010 ↩︎]]></content>
      <categories>
        <category>RNA-seq</category>
        <category>immune</category>
        <category>TCR/BCR</category>
      </categories>
      <tags>
        <tag>bio-tools</tag>
        <tag>RNA-seq</tag>
        <tag>immune</tag>
        <tag>TCR/BCR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tools for Analysis TEs in RNA-seq Data]]></title>
    <url>%2Fblog%2F2018%2F03%2FTools-for-analysis-TEs-in-RNA-seq-data%2F</url>
    <content type="text"><![CDATA[I observed large fraction reads from intronic regions in my total RNA-seq data. Considering that many Alu elements are located in introns, I want to check the TEs in RNA-seq data. I will first go through tools avaiable for analysis TEs in RNA-seq data, then use two of them to quantify TEs in test or real data. Tools avaiable Tools for the analysis of TEs (or REs) in RNA-seq data can be divided into three categories based on their function/purpose: Quantification of TEs (also compare the expression of TEs from different conditions) RepEnrich, TEtranscripts, SalmonTE Analyze TE involved transcript (find them, quantification, compare) LIONS, CLIFinder others IM-Fusion I will introduce them by the order of their publication dates. RepEnrich RepEnrich is a method to estimate repetitive element enrichment using high-throughput sequencing data. Its paper: Criscione SW, Zhang Y, Thompson W, Sedivy JM, Neretti N. Transcriptional landscape of repetitive elements in normal and cancer human cells. BMC Genomics. 2014;15:583. doi:10.1186/1471-2164-15-583 RepEnrich uses the repeatmasker files which can be downloaded from repeatmasker.org or UCSC genome table browser. It first sets up the annotations, then maps the FASTQ files to genome using Bowtie1, and finally computes count of TEs. Examples in the docs are very detailed and also provide R codes of using edgeR for differential enrichment analysis. RepEnrich read mapping strategy. Reads are mapped to the genome using the Bowtie1 aligner. Reads mapping uniquely to the genome are assigned to subfamilies of repetitive elements based on their degree of overlap to RepeatMasker annotated genomic instances of each repetitive element subfamily. Reads mapping to multiple locations are separately mapped to repetitive element assemblies – referred to as repetitive element psuedogenomes – built from RepeatMasker annotated genomic instances of repetitive element subfamilies. In the RepEnrich paper, the authors also analyzed data from ChIP-seq, they found: We show that many of the Long Terminal Repeat retrotransposons in humans are transcriptionally active in a cell line-specific manner. Cancer cell lines display increased RNA Polymerase II binding to retrotransposons than cell lines derived from normal tissue. Consistent with increased transcriptional activity of retrotransposons in cancer cells we found significantly higher levels of L1 retrotransposon RNA expression in prostate tumors compared to normal-matched controls. TEToolkit/TEtranscripts TEToolkit is a software package that utilizes both unambiguously (uniquely) and ambiguously (multi-) mapped reads to perform differential enrichment analyses from high throughput sequencing experiments. Currently, most expression analysis software packates are not optimized for handling the complexities involved in quantifying highly repetitive regions of the genome, especially transposable elements (TE), from short sequencing reads. Although transposon elements make up between 20 to 80% of many eukaryotic genomes and contribute significantly to the cellular transcriptome output, the difficulty in quantifying their abundances from high throughput sequencing experiments has led them to be largely ignored in most studies. The TEToolkit provides a noticeable improvement in the recovery of TE transcripts from RNA-Seq experiments and identification of peaks associated with repetitive regions of the genome. Its paper: Jin Y, Tam OH, Paniagua E, Hammell M. TEtranscripts: a package for including transposable elements in differential expression analysis of RNA-seq datasets. Bioinformatics. 2015;31(22):3593–3599. doi:10.1093/bioinformatics/btv422 TEToolkit includes two tools: TEtranscripts quantifies both gene and transposable element (TE) transcript abundances from RNA-Seq experiments, utilizing both uniquely and ambiguously mapped short read sequences. It processes the short reads alignments (BAM files) and proportionally assigns read counts to the corresponding gene or TE based on the user-provided annotation files (GTF files). TEpeaks identifies regions enriched for protein binding or modification to repetitive DNA and RNA sequences. It has been utilized in a variety of high throughput sequencing experiments, such as ChIP-Seq, CLIP-Seq and RIP-Seq. The tool performs peak calling utilizing a method that extends the approach implemented by MACS, utilizing ambiguously mapped reads and bin-correlation normalization to identify narrow enriched repetitive regions typically missed by standard approaches. Differential peak enrichment can also be performed to identify regions of differential protein-association in high throughput sequencing experiments. TEtranscripts is suitable for my purpose. It uses BAM files generated by other software, and the author recommended that users identify the optimal parameters for their particular genome and alignment program in order to get the best results. TEtranscripts flow chart. Reads mapping to TEs are assigned in two different modes: uniq (reads mapping uniquely in the genome), and multi (reads mapping to multiple insertions of TEs). In the multi mode, an iterative algorithm is used to optimally distribute ambiguously mapped reads. Figure was downloaded from here. rprofile/rop rprofile is part of rop, which is designed to find all the origin of RNA-seq reads (mapped/unmapped, human/non-human, etc.). rprofile is disigned to profile repetitive elements of the human genome from RNA-Seq data. Its paper: Mangul S, Yang HT, Strauli N, Gruhl F, Porath H, Hsieh K, Chen L, Daley T, Christenson S, Andersen AW, et al. Comprehensive analysis of RNA-sequencing to find the source of 1 trillion reads across diverse adult human tissues. bioRxiv. 2017 Jun 12:053041. doi:10.1101/053041 It uses BAM file to profile REs in mapped reads. Not accurate, not for quantification, just for wider snapshot of RE fraction in RNA-seq data. The annotation of TEs rprofile used now is just the one generated by TEtranscripts, only hg19 available. But we can create other TE annotations using makeTEgtf.pl script in TETookit. LIONS Introduction from Git repo: LIONS is a bioinformatic analysis pipeline which brings together a few pieces of software and some home-brewed scripts to annotate a paired-end RNAseq library to detect TE-intiated transcripts. Its paper: Babaian A, Lever J, Gagnier L, Mager DL. LIONS: Analysis Suite for Detecting and Quantifying Transposable Element Initiated Transcription from RNA-seq. bioRxiv. 2017 Jun 14:149864. doi:10.1101/149864 The last paragraph of the “Backgroud” part describes it more clear: To quantitatively measure and compare the contribution of TE promoters to normal and cancer transcriptomes we developed a tool that incorporates features of previous methods but significantly builds upon them. We were motivated to use paired-end RNA-seq data alone, a broadly available data- type, to rapidly measure TE-initiations and transcriptome contributions. With a defined set of TE- initiated transcripts in each library, commonalities and differences between sets of data (biological replicates) can be determined. Its workflow: This tool has not been published yet (20180320). There are limited docs in its Git repo and so many dependencies (it bases on the transcriptome assembled by cufflinks, so I doubt its accuracy). IM-Fusion IM-Fusion is a tool for identifying transposon insertion sites in insertional mutagenesis screens using single- and paired-end RNA-sequencing data. It essentially identifies insertion sites from gene-transposon fusions in the RNA-sequencing data, which represent splicing events between the transposon and endogeneous genes. IM-Fusion also identifies candidate genes for a given screen using a statistical test (based on the Poisson distribution) that identifies Commonly Targeted Genes (CTGs) – genes that are more frequently affected by insertions than would be expected by chance. To further narrow down a list of CTGs, which may contain hundreds of genes, IM-Fusion also tests if insertions in a CTG have a significant effect on the expression of the gene, which is a strong indicator of them having an actual biological effect. Its paper: de Ruiter JR, Kas SM, Schut E, Adams DJ, Koudijs MJ, Wessels LFA, Jonkers J. Identifying transposon insertions and their effects from RNA-sequencing data. Nucleic Acids Research. 2017;45(12):7064–7077. doi:10.1093/nar/gkx461 The abstract from its paper: Insertional mutagenesis using engineered transposons is a potent forward genetic screening technique used to identify cancer genes in mouse model systems. In the analysis of these screens, transposon insertion sites are typically identified by targeted DNA-sequencing and subsequently assigned to predicted target genes using heuristics. As such, these approaches provide no direct evidence that inser- tions actually affect their predicted targets or how transcripts of these genes are affected. To address this, we developed IM-Fusion, an approach that identifies insertion sites from gene-transposon fusions in standard single- and paired-end RNA-sequencing data. We demonstrate IM-Fusion on two separate transposon screens of 123 mammary tumors and 20 B-cell acute lymphoblastic leukemias, respectively. We show that IM-Fusion accurately identifies trans- poson insertions and their true target genes. Fur- thermore, by combining the identified insertion sites with expression quantification, we show that we can determine the effect of a transposon insertion on its target gene(s) and prioritize insertions that have a significant effect on expression. We expect that IM-Fusion will significantly enhance the accuracy of cancer gene discovery in forward genetic screens and provide initial insight into the biological effects of insertions on candidate cancer genes. Overview of IM-Fusion: I do not know what is “Insertional mutagenesis using engineered transposons”. So I do not know when to use this tool. It seems that IM-Fusion does not suit me. CLIFinder L1 Chimeric Transcripts (LCTs) are initiated by repeated LINE-1 element antisense promoters and include the L1 5′UTR sequence in antisense orientation followed by the adjacent genomic region. LCTs have been characterized mainly using bioinformatics approaches to query dbEST. To take advantage of NGS data to unravel the transcriptome composition, we developed Chimeric LIne Finder (CLIFinder), a new bioinformatics tool. Using stranded paired-end RNA-seq data, we demonstrated that CLIFinder can identify genome-wide transcribed chimera sequences corresponding to potential LCTs. Moreover, CLIFinder can be adapted to study transcription from other repeat types. CLIFinder v0.4.1 is a Galaxy tool, specifically designed to identify potential LCTs from one or several oriented RNA-seq paired-end reads in the human genome. CLIFinder v0.4.1 is customizable to detect transcripts initiated by different types of repeat elements. Its paper: Pinson M-E, Pogorelcnik R, Court F, Arnaud P, Vaurs-Barriere C. CLIFinder: Identification of LINE-1 Chimeric Transcripts in RNA-seq data. Bioinformatics. 2017 Oct 23 [accessed 2017 Nov 2]. https://academic.oup.com/bioinformatics/article/doi/10.1093/bioinformatics/btx671/4562333. doi:10.1093/bioinformatics/btx671 It is a Galaxy tool, so…(we do not have Galaxy installed on the computer cluster.) SalmonTE SalmonTE is an ultra-Fast and Scalable Quantification Pipeline of Transpose Element (TE) Abundances. Its paper: Jeong H-H, Yalamanchili HK, Guo C, Shulman JM, Liu Z. An ultra-fast and scalable quantification pipeline for transposable elements from next generation sequencing data. In: Biocomputing 2018. WORLD SCIENTIFIC; 2017. p. 168–179. http://www.worldscientific.com/doi/abs/10.1142/9789813235533_0016. doi:10.1142/9789813235533_0016 Below is its working pipeline, clear and simple! Pros. from its paper: In contrast to TEtranscripts, SalmonTE starts with raw RNA-seq files, and does not need any additional pre-processing for a given sequence file. Moreover, TEtranscripts requires a modied GTF files based on RepeatMasker database. SalmonTE only needs the FASTA file of cDNA (complementary DNA) sequences of each TE. SalmonTE is based on Salmon, which is a very fantastic tool for RNA-seq quantification. I will give it a try. Summary TE quantification: RepEnrich: map FASTQ files to the genome using Bowtie1, use repeat annotation from repeatmasker.org or UCSC genome table browser, can also analyze ChIP-seq data. TEtranscripts: map FASTQ files to the genome using any alignment software (but need to tune the parameters), use a custom GTF file for repeat and gene annotation. SalmonTE: no need to align reads to genome, use Salmon for TE quantification, only four species’ reference available now (hs, mm, dm, dr, 20180320), base on annotation of Repbase (a website I do not like…), not easy to create customized reference. TE involved transcript: LIONS: limited docs. CLIFincer: a Galaxy tool Other: IM-Fusion A little thoughts: all above tools for TE quantification may be inaccurate (the intrinsic drawback of short-reads mapping). Even SalmonTE does not need alignment, it may also deviate from reality. These tools maybe more suit for compasion of different conditions. Using TEtranscripts for TE quantification Install Download zip file from tetoolkit release page 12345678910111213141516171819202122232425262728293031323334$ unzip tetoolkit-1.5.0.zip &amp;&amp; cd tetoolkit-1.5.0# install using Python 2.7 from Anaconda, failed, don't know why, maybe something related to PYTHONPATH$ python setup.py installProcessing dependencies for TEToolkit==1.5.0Searching for pysam&gt;=0.8Reading http://pypi.python.org/simple/pysam/Couldn't find index page for 'pysam' (maybe misspelled?)Scanning index of all packages (this may take a while)Reading http://pypi.python.org/simple/No local packages or download links found for pysam&gt;=0.8error: Could not find suitable distribution for Requirement.parse('pysam&gt;=0.8')# install using original Python 2.7, it worked$ /software/Python.2.7.13/bin/python setup.py install$ ~/software/Python.2.7.13/bin/TEtranscripts -husage: TEtranscripts [-h] -t treatment sample [treatment sample ...] -c control sample [control sample ...] --GTF genic-GTF-file --TE TE-GTF-file [--format [input file format]] [--stranded [option]] [--mode [TE counting mode]] [--project [name]] [-p [pvalue]] [-f [foldchange]] [--minread [min_read]] [-n [normalization]] [--sortByPos] [-i [iteration]] [--maxL [maxL]] [--minL [minL]] [-L [fragLength]] [--verbose [verbose]] [--version]Required arguments: -t | --treatment [treatment sample 1 treatment sample 2...] Sample files in group 1 (e.g. treatment/mutant), separated by space -c | --control [control sample 1 control sample 2 ...] Sample files in group 2 (e.g. control/wildtype), separated by space --GTF genic-GTF-file GTF file for gene annotations --TE TE-GTF-file GTF file for transposable element annotationsIdentifying differential transcription of gene and transposable elements. So TEtranscripts is mainly for compare two conditions. Prepare annotations Download RepeatMasker tracks from UCSC Table Browser for pig: susScr11 and rhesus: rheMac8. Then use makeTEgtf.pl from here to make GTF files for TE. 123456789101112131415161718192021$ perl ~/software/tetoolkit-1.5.0/makeTEgtf.pl Usage: makeTEgtf.pl -c [chrom column] -s [start column] -e [stop/end column] -o [strand column] -n [source] -t [TE name column] (-f [TE family column] -C [TE class column] -1) [INFILE] Output is printed to STDOUT$ head -5 susScr11_rmsk#bin swScore milliDiv milliDel milliIns genoName genoStart genoEnd genoLeft strand repName repClass repFamily repStart repEnd repLeft id2 20440 173 38 45 chr1 75496265 75498174 -198832358 - L1B_SS LINE L1 -3164 3808 1876 12 4217 186 71 11 chr1 92274327 92275321 -182055211 - L1MA9 LINE L1 -322 7082 5254 12 762 294 110 60 chr1 117440307 117440687 -156889845 - L2c LINE L2 -511 2908 2508 13 49915 32 7 3 chr1 142603227 142607372 -131723160 - L1_SS LINE L1 -5 6813 2643 2# pig$ perl ~/software/tetoolkit-1.5.0/makeTEgtf.pl -c 6 -s 7 -e 8 -o 10 -t 11 -n susScr11_rmsk -f 13 -C 12 -S 2 susScr11_rmsk &gt; susScr11_rmsk.gtf# monkey$ perl ~/software/tetoolkit-1.5.0/makeTEgtf.pl -c 6 -s 7 -e 8 -o 10 -t 11 -n susScr11_rmsk -f 13 -C 12 -S 2 rheMac8_rmsk &gt; rheMac8_rmsk.gtf Mapping reads to genome using STAR There are detailed discussions about the parameters: Question about the alignments preparation Question reagarding a parameter in STAR I have over 70 samples and have aligned reads to the genomes using STAR before. I do not want to align again… The author provided some test files at this location. I will download BAM files of human for test. 123456# GTFwget http://labshare.cshl.edu/shares/mhammelllab/www-data/TEToolkit/test_data/testdata_GTF/hg19_rmsk_TE.gtf.gzwget http://labshare.cshl.edu/shares/mhammelllab/www-data/TEToolkit/test_data/testdata_GTF/hg19_refGene.gtf.gz# BAMwget http://labshare.cshl.edu/shares/mhammelllab/www-data/TEToolkit/test_data/testdata_PE/test_data_PE_control.bamwget http://labshare.cshl.edu/shares/mhammelllab/www-data/TEToolkit/test_data/testdata_PE/test_data_PE_treatment.bam Quantification 1$ ~/software/Python.2.7.13/bin/TEtranscripts --sortByPos --mode multi --TE hg19_rmsk_TE.gtf --GTF hg19_refGene.gtf --project pairedEnd_test -t test_data_PE_treatment.bam -c test_data_PE_control.bam 2&gt; log Then got the following errors: 1234567...INFO @ Wed, 21 Mar 2018 18:47:20: Reading sample files ... Error occured when reading first line of sample file test_data_PE_treatment.bam.Error: 'samtools returned with error 1: stdout=, stderr=[bam_sort] Use -T PREFIX / -o FILE to specify temporary and final output files\nUsage: samtools sort [options...] [in.bam]\nOptions:\n -l INT Set compression level, from 0 (uncompressed) to 9 (best)\n -m INT Set maximum memory per thread; suffix K/M/G recognized [768M]\n -n Sort by read name\n -o FILE Write final output to FILE rather than standard output\n -T PREFIX Write temporary files to PREFIX.nnnn.bam\n --input-fmt-option OPT[=VAL]\n Specify a single input file format option in the form\n of OPTION or OPTION=VALUE\n -O, --output-fmt FORMAT[,OPT[=VAL]]...\n Specify output format (SAM, BAM, CRAM)\n --output-fmt-option OPT[=VAL]\n Specify a single output file format option in the form\n of OPTION or OPTION=VALUE\n --reference FILE\n Reference sequence FASTA FILE [null]\n -@, --threads INT\n Number of additional threads to use [0]\n'[Exception type: SamtoolsError, raised in utils.py:75] Other users also found this bug: Problem with samtools. It was caused by pysam (&gt;0.9) and samtools (&gt;1.3). A solution is sorting the BAM files according to read names rather than by coordinates then feeding to TEtranscripts for now. 1234$ samtools sort -@4 -O BAM -n test_data_PE_treatment.bam -o test_data_PE_treatment.sortedByReadname.bam$ samtools sort -@4 -O BAM -n test_data_PE_control.bam -o test_data_PE_control.sortedByReadname.bam$ ~/software/Python.2.7.13/bin/TEtranscripts --mode multi --TE hg19_rmsk_TE.gtf --GTF hg19_refGene.gtf --project pairedEnd_test -t test_data_PE_treatment.sortedByReadname.bam -c test_data_PE_control.sortedByReadname.bam 2&gt; log There are four outputs of the above command: 1234pairedEnd_test.cntTable # the count table (genes along with TEs)pairedEnd_test_DESeq.R # the R codes for DE analysispairedEnd_test_gene_TE_analysis.txt # analysis result of DEseqpairedEnd_test_sigdiff_gene_TE.txt # significant genes and TEs We can also use the count table for DE analysis. Using SalmonTE for TE quantification Install Install Python3 modules: 1$ pip3 install snakemake docopt pandas Install R packages: 123install.packages(c("tidyverse", "cowplot", "scales", "WriteXLS"))source("https://bioconductor.org/biocLite.R")biocLite(c("DESeq2", "tximport")) Clone the repository 1$ git clone https://github.com/hyunhwaj/SalmonTE Add PATH of SalmonTE to your .bashrc file: 1export PATH=$PATH:/PATH_OF_SALMON_TE/ After sweating my blood (trying different R versions, different gcc versions, using callr 1.0 rather than callr 2.0）, I installed all dependent R packages … I hate installing software. Build index SalmonTE now (20180320) provides pre-built reference for four species: human (hs), Mus musculus (mm), Danio rerio (dr), Fruit fly (dm). It provides a doc about How to build a customized index, but when the species is not included in Repbase, it is not easy to create a index. I asked the author about this: build index from species which is not available in Repbase, and there is no good solutions so far. There is also a thread talking about this: reference. Well, I will just try SalmonTE for human. Quantification Then, I encountered another problem: SalmonTE only recognize .fastq and .fastq.gz extensions, but all my files end with .fq.gz. Even by using ln -s to create files with .fastq.gz extensions, it still went wrong. 1234$ python3 /software/SalmonTE/SalmonTE.py quant --reference=hs clean_fastq/human2018-03-21 14:53:13,061 Starting quantification mode2018-03-21 14:53:13,061 Collecting FASTQ files...2018-03-21 14:53:13,062 Failed to read clean_fastq/human This problem was also reported by other users: Nothing to be done, and it has not been solved. So I have to change the name my files…I have lots files. 12345# output count$ python3 /software/SalmonTE/SalmonTE.py quant --reference=hs --outpath=SalmonTE/human/count --num_threads=16 clean_fastq/human# output TPM$ python3 /software/SalmonTE/SalmonTE.py quant --reference=hs --outpath=SalmonTE/human/TPM --num_threads=16 --exprtype=TPM clean_fastq/human It was very fast. Using 16 threads, 22 samples took about 150 minutes. The output count/TPM are just like what Salmon does. All samples are put into one file named EXPR.csv. We can use this file for downstream analysis. Good. Change notes 20180322: create the note. 20180323: add rprofile.]]></content>
      <categories>
        <category>RNA-seq</category>
        <category>TE</category>
      </categories>
      <tags>
        <tag>bio-tools</tag>
        <tag>RNA-seq</tag>
        <tag>TE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Set up Jupyter Notebook on the Server]]></title>
    <url>%2Fblog%2F2018%2F03%2Fset-up-Jupyter-Notebook-on-the-server%2F</url>
    <content type="text"><![CDATA[Jupyter Notebook can be a good place to save work environment. I want to set up a Jupyter Notebook on the server, then I can visit it through my browser (Win10 x64). I have installed 64-bit Anaconda, and it already includes the Jupyter Notebook. First, generate configuration files 1$ ~/software/anaconda3/bin/jupyter notebook --generate-config Second, generate password 1234567$ python3&gt;&gt;&gt; from IPython.lib import passwd&gt;&gt;&gt; passwd()Enter password: Verify password: &apos;sha1:44b701133f2c:803e8dc59ceafe9a54b4ae1efb79dd9a1ca83192&apos;&gt;&gt;&gt; exit() Third, edit the configuration file 1$ vi ~/.jupyter/jupyter_notebook_config.py modified like this: 123456c.NotebookApp.ip=&apos;*&apos; # the notebook can be visited by any IP addressc.NotebookApp.password = u&apos;sha:ce...&apos; # password generated beforec.NotebookApp.open_browser = Falsec.NotebookApp.port =8889 # any port will be finec.NotebookApp.notebook_dir = u&apos;/home/niuyw/Jupyter_Notebook&apos; # notebook directoryc.IPKernelApp.pylab = &apos;inline&apos; Fourth, start the Jupyter Notebook 1$ ~/software/anaconda3/bin/jupyter notebook Fifth, start SSH in the local machine (I use git bash) 12$ ssh -N -f -L localhost:8888:localhost:8889 niuyw@192.168.71.41niuyw@192.168.71.41's password: Sixth, open the browser http://localhost:8888 to visit the Notebook: like this: Enter the password and enjoy!]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Jupyter</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[构建自己的笔记管理系统]]></title>
    <url>%2Fblog%2F2018%2F02%2F%E6%9E%84%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E7%AC%94%E8%AE%B0%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[下面提到的笔记管理方法，说是笔记管理，也可以勉强说是知识管理。毕竟知识是复杂的，并非所有的知识（或经验）都能方便地整理成能存储在目前的计算机中的格式。所以，还是说笔记管理更合适。笔记整理，实际上是一个分类工作，最本质的难度也在这里。夸张一点说，科学就是关于分类的学科，当你搞清楚了不同乱七八糟的层级关系时，你就理解了它是怎么工作的……嗯，扯远了。 说明一下，本人研究生二年级在读，专业是生物信息学。所以下面的内容可能更适合与我处境类似的朋友，毕竟大家需求千差万别。 作为一只“科研狗”，做笔记当然是很重要的啦。做过笔记的好孩子都知道，整理笔记是一项非常非常花时间，但十分“划算”的工作。俗话说，好记性不如烂笔头。除了在有需要时进行查询，做笔记也是一个“学而时习之”，整理思路，归纳方法的过程。另外，收藏不等于知道（可惜一直记不住）！ 用过不少的笔记管理工具，比如为知笔记、有道云笔记、印象笔记、OneNote、Endnote 等。但一直没有找到一个满意的解决方法，在不断不断……不断地折腾中，最近找到了一套差强人意的方法。整理一下分享给大家，希望能有些帮助。 有哪些笔记需要管理 有哪些笔记需要整理呢？想了一下，格式可谓……十分复杂。 文献：虽然不想看…… 网页：随着自媒体（比如博客、公众号等）的兴起，网页可能已经成为了最大也最复杂的知识来源。 其他：比如自己做的 PPT，网上下载的 PPT，excel，word 等。 文献管理比较简单，找一个心仪的工具来专门管理就好了，反正这个也（几乎）必须由专门软件来管理。 网页比较复杂，所以印象笔记等工具提供的网页剪辑功能才十分诱人，鼠标右键 -&gt; 保存到我的……，别人辛苦整理好的笔记（博客）就到了我的笔记里。虽然我也经常用这种方式，有时候保存书签（网页地址），有时候保存网页内容（防止网页链接死掉，经常发生！）。但是经常觉得别人的理解并非我的理解，里面没有我自己的经验。在工作中，非常典型的场景是，当需要解决一个问题时，我可能查询浏览了数个甚至几十上百个网页，它们都或多或少地帮助了我，但是可能并非所有内容都那么“有用”。所以经常有先保存再整理的冲动，虽然大多数时候也就仅仅只是……冲动，空余时间都喂了狗了…… 其他就更复杂了，许多情况下，文本（纯文本、富文本）并非最好的展示方式，比如你总得做工作汇报和文献讨论吧……这些 PPT 放在哪，怎么管理，当然，不嫌累的话可以把 PPT 导出图片，然后再整理到别的文本中。这只是举个例子，实际情况更加复杂。 对笔记管理方案的需求 我对笔记管理主要有下面这些需求： 轻便。依赖环境越简单越好，工具越轻便越好。 方便同步。我对移动端等的同步需求不大，毕竟在小屏幕上看笔记的需求不大，工作时肯定以个人电脑为主。我指的同步包括但不限于：云同步（同步到公共服务器或私有服务器）、拷贝到其他存储媒介等。 可移植。解决方法要容易移植，比如不用花很多时间就能分别在工作电脑和个人电脑上搭建。笔记要容易移植，每个笔记都是独立的文件，用户可以不通过笔记管理软件而通过资源管理器自由查看更改这些文件。这一点和方便同步其实有些重合。 支持 markdown。markdown 是一种轻量级的标记语言，非常方便书写。 关于为什么对 markdown 有执念，网上已经有很多讨论了。语法非常简单，强烈建议学习一下，连我都会用…… Markdown——入门指南 为什么我们要学习 Markdown 的三个理由 为什么作家应该用 Markdown 保存自己的文稿 怎样使用Markdown 另外，最重要的一点：尽量不依赖商业软件。商业软件当然具有各种优势，比如印象笔记的网页剪辑，为知笔记非常的用户友好，有道云对 markdown 的支持，OneNote 多层级的管理系统及自由书写的快感。但是，依赖商业软件让人非常“不爽”，比如程序是个黑盒子，公司对于产品的态度及支持一直在变化，买了会员后万一公司哪天倒闭了呢，格式不友好（不兼容其他平台，不便导出等）……总之，就是想自己折腾，哈哈。 其他的一些小的需求比如：界面美观，使用简单，开源，支持全文搜索，标签，时间线等。不过这些都不是主要的。 解决思路 先说方法，用过这些工具朋友应该已经知道怎么做了。 Zotero：文献和笔记管理软件 VNote：一个基于 markdown 的本地笔记管理软件。 坚果云：一个非常方便的云同步平台 火狐浏览器 + 一些插件：书签管理 资源管理器：就是……文件夹 markdonw 书写（非必需，但是建议） 其他 用 Zotero 管理文献（和笔记） 许多人（身边的朋友，不在身边的朋友……）都用 Endnote 管理文献，不在此讨论 Endnote 的不好了。毕竟工具不是关键，只要能到罗马，黑猫白猫都行。 Zotero 有哪些优势呢？ 免费（Zotero 同步空间存储收费，但是可以用其他同步方式，比如坚果云） 简单 漂亮 导入文献太方便了，仅仅需要鼠标左键（方便收集文献） 导出文献到 word 方便（方便写论文时插入文献） 可以配合坚果云实现“完美”同步 不仅仅可以管理文献，还有笔记、网页等 支持插件，比如谷歌学术插件，可以很方便地更新文献的引用次数。 群组，可以很方便地把整理好的文献分享给他人 好像还能导入 Endnote 文献，我没用过。 举个例子，下面是我收集的关于从 RNA-seq 数据中探索 splicing 的工具：可以看到左边是目录，右边不仅仅有文献，还有网页，每个条目下又可以建立笔记、链接等。Extra 列是用 Zotero Scholar Citations 插件获取的文献引用次数。 总之一句话来形容喜欢的话，我已经几乎离不开了 Zotero 了。每天工作时最先（手动）打开的两个软件，一个是火狐浏览器，一个是 Zotero。 网上有很多关于 Zotero 的教程了，比如这个系列：Zotero入门六篇。再比如配合坚果云的同步教程：用坚果云作为Zotero云存储的设置方法、如何在Zotero中设置webdav连接到坚果云？。有时候，我们只是不知道用什么合适的关键词去搜索，许多问题早就被解决了。 用 VNote 管理笔记 VNote，这是一个最近（大约半个月前）才发现的良品工具，作者解决的问题其实我也一直想解决，可惜技术上水平不够。能发现这样优秀的工具实在是荣幸。不多说，看作者的介绍，或者下载下来试一下~ VNote 项目主页 VNote: 一个舒适的Markdown笔记软件 靡不有初，鲜克有终——写在VNote半周岁 VNote：一个更懂程序员和 Markdown 的笔记 举个例子，我正在写的这篇笔记就是用 VNote + markdown 写的： 用坚果云同步到云端 使用很简单，不用学，直接用。这是一张来自其帮助中心的截图，可以说非常接近事实： 用火狐浏览器和插件同步书签 正如上面提到的，许多许多的知识来自于网页，有些时候我们不需要保存整个网页的内容，只需要（暂时）保存它们的地址。那么，好用的书签管理肯定最方便了。 我采用的方式是：Firefox + Xmarks Bookmark Sync + Bookmark Dupes 来管理书签： Firefox，亦即火狐娘（有些傲娇，但功能十分强大，故称娘 _） EverSync，同步书签到云端 Bookmark Dupes，整理重复的书签及文件夹 用资源管理器管其他的 这个无需赘言，对于像我这样对电脑有强迫症的人来说，目录最好井井有序，不然肯定会抓狂的（有时也用 everything）。关于整理电脑，网上也有一些建议，无聊时候可以搜搜看，有些还是挺有帮助的。 操作方案 除了书签管理（在火狐浏览器中完成）和文献管理（在 Zotero 中完成），下面是我在电脑上对笔记管理采用的目录结构 12345笔记整理 # 存放笔记的根目录，也是同步的目录├── VNote # VNote 的根目录（笔记整理的主要文件夹，软件自动整理）└── 工作 # 存放工作汇报（主要是 PPT ）的目录└── 文献 # 存放文献讨论的（主要是 PPT ）的目录└── 笔记 # 存放一些其他的笔记（比如偶尔整理的 word，excel 等） 比如这是放文献汇报 PPT 的文件夹，PPT 都以日期命名，方便查找： 以后做笔记都尽量整理到这些目录中，然后只需要备份整个文件夹就可以了（再安装几个反正我也会装的软件……）。拷贝到 U 盘，移动硬盘，或者通过坚果云同步： 写在最后 我的方法不一定适用，只是我自己目前觉得好用的方法。欢迎交流讨论~ 学习是一个积年累月的过程，笔记整理只是其中很小的一个部分。保持无知，不断坚持，共勉。 感谢那些我无法一一列出但十分需要感谢的来自网络的知识的作者，以及一些与我讨论的朋友，谢谢他们的分享！ 更新日志 20180217：创建笔记。 20180507：修改用火狐浏览器同步书签部分。]]></content>
      <categories>
        <category>mixture</category>
      </categories>
      <tags>
        <tag>VNote</tag>
        <tag>Zotero</tag>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hugo 建站记录]]></title>
    <url>%2Fblog%2F2018%2F02%2FHugo-%E5%BB%BA%E7%AB%99%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[Hugo 是由 Go 语言实现的静态网站生成器。简单、易用、高效、易扩展、快速部署。 根据官网的一些文档及网上的教程，尝试用 Hugo 搭建一个静态的博客站点。下面的操作在 win10 64 位家庭版中完成。bash 命令在 git bash 中操作。 安装 在 Hugo Releases 页面下载了最新的 v0.36 win64 位压缩包，解压后将其添加到环境变量中。 生成站点 使用 Hugo 快速生成站点： 123456789101112131415161718$ hugo new site testCongratulations! Your new Hugo site is created in D:\hugo.Just a few more steps and you're ready to go:1. Download a theme into the same-named folder. Choose a theme from https://themes.gohugo.io/, or create your own with the "hugo new theme &lt;THEMENAME&gt;" command.2. Perhaps you want to add some content. You can add single files with "hugo new &lt;SECTIONNAME&gt;\&lt;FILENAME&gt;.&lt;FORMAT&gt;".3. Start the built-in live server via "hugo server".Visit https://gohugo.io/ for quickstart guide and full documentation.$ $ cd test/$ lsarchetypes/ config.toml content/ data/ layouts/ static/ themes/ config.toml是网站的配置文件，包括 baseurl, title, copyright 等等网站参数。 这几个文件夹的作用分别是： archetypes：包括内容类型，在创建新内容时自动生成内容的配置 content：包括网站内容，全部使用markdown格式 data：存放 Hugo 生成网站时的配置文件 layouts：包括了网站的模版，决定内容如何呈现 static：包括了css, js, fonts, media等，决定网站的外观 themes：主题 添加主题 按照一般方法这一步应该添加主题，然后添加文章，然后就可以预览站点了。 这里选用了 Academic 这一主题。这是一个适合做个人简历的主题（学院风格）。按照它的方式进行安装，跳过了初始化网站的过程，直接从 git 上克隆一个版本过来。 12345678910111213$ git clone https://github.com/sourcethemes/academic-kickstart.git hugoCloning into 'hugo'...remote: Counting objects: 77, done.remote: Total 77 (delta 0), reused 0 (delta 0), pack-reused 77Unpacking objects: 100% (77/77), done.$ cd hugo/# 初始化主题$ git submodule update --init --recursiveSubmodule 'themes/academic' (https://github.com/gcushen/hugo-academic.git) registered for path 'themes/academic'Cloning into 'D:/知识库/hugo/themes/academic'...Submodule path 'themes/academic': checked out 'b02f5ecedabe4494dabee522db3ab843472afa41' 配置 修改一下网站的顶层配置文件，进行相应的配置。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287# Configuration of Academic# Documentation: https://sourcethemes.com/academic/# The URL of your website.# End your URL with a `/` trailing slash, e.g. `https://example.com/`.baseurl = "https://yiweiniu.github.io/"# Title of your sitetitle = "Yiwei Niu"# Your copyright notice - appears in site footer.# To display a copyright symbol, type `&amp;copy;`.copyright = "&amp;copy; 2018 Yiwei Niu"theme = "academic"enableEmoji = truefootnotereturnlinkcontents = "&lt;sup&gt;^&lt;/sup&gt;"ignoreFiles = ["\\.Rmd$", "\\.Rmarkdown$", "_files$", "_cache$"]preserveTaxonomyNames = truepaginate = 10# Enable comments by entering your Disqus shortnamedisqusShortname = ""# Enable analytics by entering your Google Analytics tracking IDgoogleAnalytics = ""# Default language to use (if you setup multilingual support)defaultContentLanguage = "en"defaultContentLanguageInSubdir = false[outputs] home = [ "HTML", "CSS", "RSS" ] section = [ "HTML", "RSS" ]# Configure BlackFriday Markdown rendering.# See: https://gohugo.io/readfiles/bfconfig/[blackfriday] hrefTargetBlank = true # `true` opens external links in a new tab. fractions = true # `false` disables smart fractions (e.g. 5/12 formatted as a fraction). smartypants = true # `false` disables all smart punctuation substitutions (e.g. smart quotes, dashes, fractions).[params] # Color theme. # Choose from `default`, `ocean`, `forest`, `coffee`, `dark`, or `1950s`. color_theme = "default" # Font style. # Choose from `default`, `classic`, or `playfair`. font = "default" # Your details. name = "Yiwei Niu" role = "Graduate student" # Organizations/Affiliations. # Separate multiple entries with a comma, using the form: `[ &#123;name="Org1", url=""&#125;, &#123;name="Org2", url=""&#125; ]`. organizations = [ &#123; name = "Institute of Biophysics, Chinese Academy of Sciences", url = "http://www.ibp.cas.cn/" &#125; ] gravatar = false # Get your avatar from Gravatar.com? (true/false) avatar = "portrait.jpg" # Specify an avatar image (in `static/img/` folder) or delete value to disable avatar. email = "xiaohuwangwang@qq.com" address = "15 Datun Road, Chaoyang District, Beijing, 100101, China" office_hours = "" phone = "" skype = "" telegram = "" # Enable Keybase in Contact widget by entering your keybase.io username. keybase = "" # Discussion link (e.g. link to a forum, mailing list, or chat). # Uncomment line below to use. # discussion = &#123; name = "Discuss", url = "https://discourse.gohugo.io" &#125; # Diplay a logo in navigation bar rather than title (optional). # To enable, place an image in `static/img/` and reference its filename below. To disable, set the value to "". logo = "" # Enable/disable map in Contact widget. # To show your address on a map in the contact widget, you need to enter your latitude, longitude and choose # a map provider below. # To use Google Maps, set `map = 1` and enter your API key that can be obtained here: # https://developers.google.com/maps/documentation/javascript/get-api-key # To use OpenStreetMap tiles, set `map = 2`. # To use OpenStreetMap on a high traffic site, set `map = 3` and enter your API key that can be obtained here: # https://www.mapbox.com/studio/account/tokens # To get your coordinates, right-click on Google Maps and choose "What's here?". The coords will show up at the bottom. # # Map provider: # 0: No map # 1: Google Maps # 2: OpenStreetMap (Mapnik) # 3: OpenStreetMap (Mapbox) map = 0 map_api_key = "" latitude = "37.4275" longitude = "-122.1697" zoom = 15 # Date and time format (refer to Go's date format: http://fuckinggodateformat.com ) # Examples: "Mon, Jan 2, 2006" or "2006-01-02" date_format = "Jan 2, 2006" # Examples: "3:04 pm" or "15:04" time_format = "3:04 PM" # Show estimated reading time for posts? reading_time = false # Display comment count? Requires commenting to be enabled. comment_count = true # Display next/previous section pager? section_pager = false # Enable global LaTeX math rendering? # If false, you can enable it locally on a per page basis. math = false # Highlight.js options # highlight # Enable global source code highlighting? If false, you can # override it for a particular page in that page's preamble. # # Example: highlight = true # # highlight_languages # Add support for highlighting additional languages. Support for # languages mentioned here will be included in all pages. You # can also set this variable for a particular page in that # page's preamble. # # Example: highlight_languages = ["go", "lisp", "ocaml"] # # highlight_style # Choose a different CSS style for highlighting source # code. Setting this option in a page's preamble has no # effect. # # Example: highlight_style = "github-gist" # # For the list of supported languages and styles, see: # https://cdnjs.com/libraries/highlight.js/ # # For more info on the highlighting options, see: # https://sourcethemes.com/academic/post/writing-markdown-latex/#highlighting-options highlight = true highlight_languages = [] # highlight_style = "github" # Enable native social sharing buttons? sharing = true # Link custom CSS and JS assets # (relative to /static/css and /static/js respectively) custom_css = [] custom_js = [] # Publication types. # Used to categorize publications. # The index of the publication type in the list is used as its unique numerical identifier. # The numeric ID is used in a publication's frontmatter to categorize it. # The language can be edited below. # For multilingual sites, `publication_types` can be copied to each language section at the end of this file and # translated. publication_types = [ 'Uncategorized', # 0 'Conference paper', # 1 'Journal article', # 2 'Manuscript', # 3 'Report', # 4 'Book', # 5 'Book section' # 6 ] # Configuration of talk pages. [params.talks] # Show talk time? time = true # Configuration of publication pages. [params.publications] # Date format (refer to Go's date format: http://fuckinggodateformat.com ) # Examples: "Mon, Jan 2, 2006" or "2006-01-02" date_format = "January, 2006" # Configuration of project pages. [params.projects] # List publications and talks related to the project? list_children = true # Publication list format. # 0 = Simple # 1 = Detailed # 2 = APA # 3 = MLA publication_format = 2 # Social/Academic Networking # # Icon pack "fa" includes the following social network icons: # # twitter, weibo, linkedin, github, facebook, pinterest, google-plus, # youtube, instagram, soundcloud # # For email icon, use "fa" icon pack, "envelope" icon, and # "mailto:your@email.com" as the link. # # Full list: https://fortawesome.github.io/Font-Awesome/icons/ # # Icon pack "ai" includes the following academic network icons: # # google-scholar, arxiv, orcid, researchgate, mendeley # # Full list: https://jpswalsh.github.io/academicons/ [[params.social]] icon = "envelope" icon_pack = "fa" link = "mailto:xiaohuwangwang@qq.com" [[params.social]] icon = "twitter" icon_pack = "fa" link = "https://twitter.com/ywniu" [[params.social]] icon = "weibo" icon_pack = "fa" link = "//weibo.com/u/3049858680" [[params.social]] icon = "google-scholar" icon_pack = "ai" link = "https://scholar.google.com.hk/citations?user=JMK-4AEAAAAJ&amp;hl=en" [[params.social]] icon = "github" icon_pack = "fa" link = "//github.com/YiweiNiu"# Navigation Links# To link a homepage widget, specify the URL as a hash `#` followed by the filename of the# desired widget in your `content/home/` folder.# The weight parameter defines the order that the links will appear in.[params.menus] # Align the main menu to the right of the page? (true/false) align_right = true[[menu.main]] name = "Home" url = "#about" weight = 1[[menu.main]] name = "Publications" url = "#publications_selected" weight = 2[[menu.main]] name = "Projects" url = "#projects" weight = 3[[menu.main]] name = "Contact" url = "#contact" weight = 4[[menu.main]] name = "Blog" url = "https://YiweiNiu.github.io/blog/" weight = 5# Taxonomies.[taxonomies] tag = "tags" category = "categories" publication_type = "publication_types"# Languages# Create a [languages.X] block for each language you want, where X is the language ID.# Configure the English version of the website.[Languages.en] languageCode = "en-us" 本地预览 1$ hugo server 部署 先在 git 上建立了一个空的 repo，名字为 YiweiNiu.github.io。 在站点根目录执行 Hugo 命令生成最终页面： 1$ hugo --theme=academic --baseUrl="https://yiweiniu.github.io/" （注意，以上命令并不会生成草稿页面，如果未生成任何文章，请去掉文章头部的 draft=true 再重新生成。） 所有静态页面都会生成到 public 目录，将 pubilc 目录里所有文件 push 到刚创建的 Repository 的 master 分支。 123456cd publicgit initgit remote add origin https://github.com/yiweiniu/yiweiniu.github.io.gitgit add -Agit commit -m "first commit"git push -u origin master 浏览器里访问：https://yiweiniu.github.io/ 即可查看。 日常使用 更新 hugo 到 hugo 的 releases 页面下载最新的安装包。 更新主题 12345678# Display available updates to Academic.cd themes/academicgit fetchgit log --pretty=oneline --abbrev-commit --decorate HEAD..origin/mastercd ../../# Update Academic.git submodule update --remote --merge 更新内容 12345678# 生成新的hugocd publicgit add .git commit -m "update"git push -u origin master 参考 Hugo Documentation Hugo中文文档 GotGitHub - 建立主页 Academic theme]]></content>
      <categories>
        <category>mixture</category>
      </categories>
      <tags>
        <tag>Hugo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo 建站记录]]></title>
    <url>%2Fblog%2F2018%2F02%2FHexo-%E5%BB%BA%E7%AB%99%E8%AE%B0%E5%BD%95%2F</url>
    <content type="text"><![CDATA[Hexo 是一个轻便的博客框架。根据官网的一些文档及网上的教程，尝试用 Hexo 搭建一个静态的博客站点。下面的操作在 win10 64 位家庭版中完成。bash 命令在 git bash 中操作。 基础 前提 首先需要安装 git 和 npm，git 已经安装好了。到 npm 官网 下载并安装 npm。 安装 123456789101112131415161718192021222324252627# 安装npm install hexo-cli -g# 查看版本$ hexo -vhexo: 3.5.0hexo-cli: 1.0.4os: Windows_NT 10.0.16299 win32 x64http_parser: 2.7.0node: 9.5.0v8: 6.2.414.46-node.18uv: 1.19.1zlib: 1.2.11ares: 1.13.0modules: 59nghttp2: 1.29.0napi: 2openssl: 1.0.2nicu: 60.1unicode: 10.0cldr: 32.0tz: 2017c# 建站hexo init hexocd hexonpm install 配置站点 打开 _config.yml 文件，修改一些基本配置 1234567891011121314# Sitetitle: Yiwei Niu's blogsubtitle: to share, to learndescription: a beginner, a learnerauthor: Yiwei Niulanguage: entimezone: Asia/Shanghai# URL## If your site is put in a subdirectory, set url as 'http://yoursite.com/child' and root as '/child/'url: https://yiweiniu.github.io/blogroot: /blog/permalink: :year/:month/:title/permalink_defaults: 安装主题 选择了 Next 这一主题： 1git clone https://github.com/theme-next/hexo-theme-next themes/next 将 _config.yml 中 theme 设置为 next。 生成 1hexo g 部署 先在 git 上建立了一个空的 repo，名字为 blog。然后修改 _config.yml 文件，注意这里设置的 branch 名字。第一个 branch 名字 为 gh-pages，这样命名是默认的项目主页。利用新的 hexo-deployer-git 就可以很方便地将网站和源码同时放在 git 上，这样即便更换电脑也可以从源码拉取代码，重新生成网站。参考自: 使用hexo，如果换了电脑怎么更新博客？ 123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: - type: git repo: git@github.com:YiweiNiu/blog.git branch: gh-pages 首先需要配置 ssh 公钥 12# 生成ssh-keygen -t rsa (一路回车) 打开 git 网站，在设置部分添加 ssh keys，然后将 ~/.ssh/id_rsa.pub 中的内容复制到其中。 接下来就可以部署到 git 了。 123npm install hexo-deployer-git --save# 部署hexo d 本地调试 可以在本地调试 1234# 安装npm install hexo-server --save# 运行本地服务器hexo s [-p 4000] 服务器默认运行在 0.0.0.0，您可以覆盖默认的 IP 设置。默认端口为 4000，可以用 -p 设置端口。 1hexo server -i 192.168.1.1 高级 添加「标签」页面 新建「标签」页面，并在菜单中显示「标签」链接。「标签」页面将展示站点的所有标签，若你的所有文章都未包含标签，此页面将是空的。 底下代码是一篇包含标签的文章的例子： 12345title: 标签测试文章tags: - Testing - Another Tag--- 在终端窗口下，定位到 Hexo 站点目录下。使用 hexo new page 新建一个页面，命名为 tags ： 1hexo new page tags 编辑刚新建的页面，将页面的 type 设置为 tags ，主题将自动为这个页面显示标签云。页面内容如下： 123456---title: tagsdate: 2018-02-07 17:14:22type: "tags"comments: false--- 添加「分类」页面 新建「分类」页面，并在菜单中显示「分类」链接。「分类」页面将展示站点的所有分类，若你的所有文章都未包含分类，此页面将是空的。 底下代码是一篇包含分类的文章的例子： 123title: 分类测试文章categories: Testing--- 在终端窗口下，定位到 Hexo 站点目录下。使用 hexo new page 新建一个页面，命名为 categories ： 1hexo new page categories 编辑刚新建的页面，将页面的 type 设置为 categories ，主题将自动为这个页面显示分类。页面内容如下： 123456---title: categoriesdate: 2018-02-07 17:14:06type: "categories"comments: false--- 设置页面文章的篇数 我想在首页及 archive/tag 页面展示不同的文章数，参考这里进行了设置：如何设置页面文章的篇数？ 首先检查这几个插件是否安装上了： 123npm install --save hexo-generator-indexnpm install --save hexo-generator-archivenpm install --save hexo-generator-tag 然后打开站点的 _config.yml 添加下面的内容： 123456789101112index_generator: path: '' per_page: 8 order_by: -datearchive_generator: per_page: 0 yearly: true monthly: falsetag_generator: per_page: 10 资源文件夹 对于那些想要更有规律地提供图片和其他资源以及想要将他们的资源分布在各个文章上的人来说，Hexo也提供了更组织化的方式来管理资源。这个稍微有些复杂但是管理资源非常方便的功能可以通过打开 _config.yml 文件，修改 post_asset_folder 选项设为 true， 12_config.ymlpost_asset_folder: true 当资源文件管理功能打开后，Hexo将会在你每一次通过 hexo new [layout] &lt;title&gt; 命令创建新文章时自动创建一个文件夹。这个资源文件夹将会有与这个 markdown 文件一样的名字。将所有与你的文章有关的资源放在这个关联文件夹中之后，你可以通过相对路径来引用它们，这样你就得到了一个更简单而且方便得多的工作流。 通过常规的 markdown 语法和相对路径来引用图片和其它资源可能会导致它们在存档页或者主页上显示不正确。在 Hexo2 时代，社区创建了很多插件来解决这个问题。但是，随着 Hexo3 的发布，许多新的标签插件被加入到了核心代码中。这使得你可以更简单地在文章中引用你的资源。 1234&#123;% asset_path slug %&#125;&#123;% asset_img slug [title] %&#125;&#123;% asset_link slug [title] %&#125;&#123;% post_link post_name post_name_show %&#125; 比如： 123&#123;% asset_img zotero_rna_1518862657_29629.png zotero_example %&#125;&#123;% post_link Detect-Microbial-Contamination-in-Contigs-by-Kraken "Detect Microbial Contamination in Contigs by Kraken" %&#125; 注意：图片标题不能以下划线起始 _，不知道为什么。 置顶文章 突然发现有时候有博客置顶的功能。参考这篇博客：解决Hexo置顶问题。 12npm uninstall hexo-generator-index --savenpm install hexo-generator-index-pin-top --save 然后在需要置顶的文章的 Front-matter 中加上 top: true 即可。 改变 Markdown 渲染器 由于使用 VNote 做笔记，顺便发布到博客上。VNote 默认的 Markdown 渲染器是 markdown-it，而 Hexo 的渲染器是 GFM。并且 GFM 不支持上标、下标、脚注等。为了二者能保持一致，所以我想更换 Hexo 默认的渲染器。 12npm un hexo-renderer-marked --save # 卸载原有渲染器npm i hexo-renderer-markdown-it-plus --save # 安装新的 编辑站点的 _config.yml，配置插件 12345678910111213141516# hexo-renderer-markdown-it-plus config## Docs: https://github.com/CHENXCHEN/hexo-renderer-markdown-it-plusmarkdown_it_plus: highlight: true html: true xhtmlOut: true breaks: true langPrefix: linkify: true typographer: quotes: “”‘’ pre_class: highlight plugins: - plugin: name: markdown-it-mark enable: true 查看 hexo-renderer-markdown-it-plus 了解更多。 文章加密 当然，这种加密方式非常不安全，只能阻挡普通的浏览者。参考：hexo文章密码访问的&quot;破解&quot;方法。 既然是博客，加密其实也没有多大意义。所以这节内容是为了玩一玩。 安装 hexo-blog-encrypt 插件 1npm install hexo-blog-encrypt --save 启用文章加密功能，修改站点的 _config.yml 文件： 1234# Security##encrypt: enable: true 然后在想要加密的文章头部加上对应字段，如 password, abstract, message 等： 123456789---title: hello worlddate: 2016-03-30 21:18:02tags: - fdsafsdafpassword: Mikeabstract: Welcome to my blog, enter password to read. # 文章摘要message: Welcome to my blog, enter password to read. # 密码提示--- 添加站点地图并提交到谷歌 配置站点地图，主要用于 SEO 优化。主要参考了这篇博客：next添加sitemap 安装 hexo-generator-sitemap 插件 1npm install hexo-generator-sitemap --save 提交 sitemap 到 Google 这块在官方文档里面有提到（官方文档其实很容易上手，跟着官方走还是很容易的，有些地方可能不够详细，但是网上关于next的配置博客也不少，如｜Hexo优化｜如何向google提交sitemap（详细），这里给出傻瓜式详细步骤： 进入 Google Webmaster Central 点击骚红色的”ADD A PROPERTY” 在弹出来的小框中加入你的站点地址 http://yoursite.com ，然后点击”Continue” Tab栏选择”Alternate methods”，选中HTML tag可以看见 &lt;meta name=&quot;google-site-verification&quot; content=&quot;xxxxxxxxxxxxxxxxxx&quot; /&gt; #复制content的值 打开next主题的配置文件_config.yml，找到google_site_verification字段（找不到就新建）： 1234&gt;&gt;# Google Webmaster tools verification setting&gt;&gt;# See: https://www.google.com/webmasters/&gt;&gt;google_site_verification: xxxxxxxxxxxxxxxxxx #4中content的值&gt;&gt; 执行命令重新发布站点 hexo d -g 回到4中的 Google Webmaster Central 页面，点击骚红色的”VERIFY”，done！ 添加蜘蛛协议 robots.txt 是搜索引擎中访问网站的时候要查看的第一个文件。robots.txt 文件告诉蜘蛛程序在服务器上什么文件是可以被查看的。 在站点 source 文件夹下新建 robots.txt 文件，文件内容如下: 12345678910User-agent: *Allow: /Allow: /archives/Allow: /tags/Allow: /categories/Disallow: /js/Disallow: /css/Disallow: /fonts/Sitemap: https://yiweiniu.github.io/blog/sitemap.xml 配置主题 下面的配置基于 Next v6.0.6，不同版本可能会有一定差异。 下面如果需要修改配置文件，都是在 themes/next 下的 _config.yml，这种站点配置和主题配置分离的方式非常灵活方便。 选择 Scheme Scheme 是 NexT 提供的一种特性，借助于 Scheme，NexT 为你提供多种不同的外观。同时，几乎所有的配置都可以 在 Scheme 之间共用。 12345678# Scheme Settings# ---------------------------------------------------------------## Schemes#scheme: Muse#scheme: Mist#scheme: Piscesscheme: Gemini 侧栏菜单 打开 themes/next 下的 _config.yml 文件，搜索 menu 关键字 123456menu: home: / || home categories: /categories/ || th tags: /tags/ || tags archives: /archives/ || archive about: https://yiweiniu.github.io/ || user 添加头像 打开 themes/next 下的 _config.yml 文件，搜索 Sidebar Avatar 关键字，去掉 avatar 前面的# 1234# Sidebar Avatar# in theme directory(source/images): /images/avatar.gif# in site directory(source/uploads): /uploads/avatar.gifavatar: /images/avatar.jpg 侧边栏社交链接 侧栏社交链接的修改包含两个部分，第一是链接，第二是链接图标。 两者配置均在 主题配置文件 中。 链接放置在 social 字段下，一行一个链接。其键值格式是 显示文本: 链接地址。 12345social: E-Mail: mailto:xiaohuwangwang@qq.com || envelope GitHub: https://github.com/YiweiNiu || github Weibo: http://weibo.com/u/3049858680 || weibo Twitter: https://twitter.com/ywniu || twitter 阅读更多 在文章中使用 &lt; !--more--&gt; 手动进行截断。这种方法可以根据文章的内容，自己在合适的位置添加 &lt; !--more--&gt; 标签，使用灵活，也是 Hexo 推荐的方法。 自动形成摘要，在主题配置文件中添加。默认截取的长度为 150 字符，可以根据需要自行设定 12345# Automatically Excerpt. Not recommend.# Please use &lt;!-- more --&gt; in the post to control excerpt accurately.auto_excerpt: enable: true length: 150 显示文章字数统计,阅读时长 在你站点的根目录下 1npm install hexo-symbols-count-time --save 修改站点配置 _config.yml，添加下面的内容： 1234567# word count# Dependencies: https://github.com/theme-next/hexo-symbols-count-timesymbols_count_time: symbols: true time: true total_symbols: false total_time: false 修改主题配置 themes/next 下的 _config.yml，搜索关键字 post_wordcount 12345678# Post wordcount display settings# Dependencies: https://github.com/theme-next/hexo-symbols-count-timesymbols_count_time: separated_meta: false item_text_post: false item_text_total: false awl: 5 wpm: 200 修改网页底部 在图标库中找到你自己喜欢的图标，打开 themes/next 下的 _config.yml，搜索关键字 authoricon，替换图标名。 1234567891011121314151617181920212223242526272829footer: # Specify the date when the site was setup. # If not defined, current year will be used. since: 2017 # Icon between year and copyright info. icon: # Icon name in fontawesome, see: https://fontawesome.com/v4.7.0/icons # `heart` is recommended with animation in red (#ff0000). name: mars # If you want to animate the icon, set it to true. animated: true # Change the color of icon, using Hex Code. color: "#808080" # If not defined, will be used `author` from Hexo main config. copyright: # ------------------------------------------------------------- # Hexo link (Powered by Hexo). powered: false theme: # Theme &amp; scheme info link (Theme - NexT.scheme). enable: false # Version info of NexT after scheme info (vX.X.X). version: false # ------------------------------------------------------------- # Any custom text can be defined here. #custom_text: Hosted by &lt;a target="_blank" href="https://pages.github.com"&gt;GitHub Pages&lt;/a&gt; 开启版权声明 主题配置文件下,搜索关键字 post_copyright，enable 改为 true 1234# Declare license on postspost_copyright: enable: true license: &lt;a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow" target="_blank"&gt;CC BY-NC-SA 4.0&lt;/a&gt; 浏览进度 浏览页面的时候显示当前浏览进度 12# Scroll percent label in b2t button.scrollpercent: true 评论系统 DISQUS 先到 Disqus 网站配置一下，在右上角有一个齿轮的标记 setting，在下拉列表中选择 add disqus to site，需要填写 Site URL，Site Name，Site Shortname，完事后就注册了一个账号。 编辑 主题配置文件， 将 disqus 下的 enable 设定为 true，同时提供您的 shortname。count 用于指定是否显示评论数量。 1234disqus: enable: false shortname: disqus_short_name count: true 内容分享 在网站 AddThis 上注册账号。 可以使用 Google/Facebook/Twitter 账号进行第三方登陆。从菜单获得 AddThis id：More… --&gt; General --&gt; ID。在 主题配置文件 中，把 #Share 下的 #add_this_id 取消注释， 改为 add_this_id: put_your_add_this_id_here。 本地搜索 添加百度/谷歌/本地 自定义站点内容搜索 安装 hexo-generator-searchdb，在站点的根目录下执行以下命令： 1npm install hexo-generator-searchdb --save 编辑 站点配置文件，新增以下内容到任意位置： 12345search: path: search.xml field: post format: html limit: 10000 编辑 主题配置文件，启用本地搜索功能： 123# Local searchlocal_search: enable: true 添加谷歌统计 注册 Google Analytics。 在主题下的 _config.yml 文件中添加： 1google_analytics: UA-[TrackingID]-1 阅读次数统计（LeanCloud) 参考了这篇博客：为NexT主题添加文章阅读量统计功能。对 LeanCloud 进行配置。 1234leancloud_visitors: enable: true app_id: sxYNFK15bXIJqayGqYdiJNsw-gzGzoHsz app_key: inuUVGRl3VjbS7oJLrS8uYMe 修改超链接样式 Next 主题默认的超链接样式是下划线，纯白色，在网页中很不明显。我参考了这篇博客对超链接样式进行了修改： Hexo + Next 的优化 在 /theme/next/source/css/_custom/custom.styl 中修改，添加如下代码： 1234567891011.post-body a &#123; color: #428BCA; text-decoration: none; border-bottom: 1px dashed #6465a5; background-color: transparent; word-wrap: break-word;&#125;.post-body a: hover&#123; color:#f15838; border-bottom: 1px dashed #f28a31;&#125; 其中 a 定义了超链接的基本样式，a:hover 定义了超链接在鼠标悬停时颜色的变化。.post-body 使得改变的样式只作用与文章内部，否则你会发现文章标题下面的分类等等样式都会被改变。 日常使用 创建新文章 12345678# 创建新文章hexo new "xxx xxx xxx"# 测试hexo s# 生成站点并部署hexo d -g 更新 Next 主题 先备份 themes/next 下的 _config.yml。 12cd themes/nextgit pull 再把 _config.yml 拷贝回去。 更新 hexo 及插件 更新 hexo 1npm update -g hexo 更新插件 1npm update 更换电脑 有哪些文件需要同步？ blog scaffolds draft.md page.md post.md source themes next _config.yml 在尝试使用 git 同步博客的原始时，发现会遇到各种问题。觉得不如用坚果云选择性同步这几个文件夹。然后在另一台电脑上搭建博客环境、安装相关插件后。同步这些文件，然后重新生成、部署站点。如果遇到问题，参考这篇笔记。 最后……样式虽然重要，内容才是一切。 更新历史 20180207：创建笔记，记录了建站的过程 20180320：更新了 Next 主题，并修改相应内容 20180322：调整了笔记结构，增加了文章加密，站点地图等部分 20180324：增加了“改变 Markdown 渲染器”，更改了“更换电脑”部分 20180328：由于 hexo-neat 报错，移除了“静态资源压缩”部分 20180330：增加“设置页面文章的篇数”,&quot;文章置顶&quot;部分 20180404：将主题更新到了 6.1.0，更新了“修改网页底部”部分 20180710：添加了“添加谷歌统计”部分 20180815：添加了“修改超链接样式”部分 20181105：移除了“不蒜子”部分，增加了“leancloud”部分 参考 Hexo 文档 Next git repo Next 文档 Hexo-Next-主题优化 Blog更新&amp;配置文件详解 Hexo 使用，博客写作相关问题及解决方法 next添加sitemap 使用hexo，如果换了电脑怎么更新博客？ Hexo 主题 —— NexT 设置 【工具】用hexo搭建博客]]></content>
      <categories>
        <category>mixture</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
</search>
